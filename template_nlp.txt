0	Nov <*> <*> <*> <*> ERROR  lock_repo.go <*>  LockRepo .FindLocks   finding the lock failed. DbFindError  Error while performing the find operation in the database <*> locks   spec.key    $eq   MsgBrokerMemCacheSubcriberSubscriber     caused by  connection   error occured during connection handshake  auth error  sasl conversation error  unable to authenticate using mechanism  <*>    KeyNotFound  Cache Reader No keys found for HMAC that is valid for time    ts  Timestamp <*>  <*>    with id  <*>	985
1	Nov <*> <*> <*> <*> ERROR  lock_repo.go <*>  LockRepo .FindLocks   finding the lock failed. DbFindError  Error while performing the find operation in the database <*> locks   spec.key    $eq   DatabaseBackupScheduler     caused by  connection   error occured during connection handshake  auth error  sasl conversation error  unable to authenticate using mechanism  <*>    KeyNotFound  Cache Reader No keys found for HMAC that is valid for time    ts  Timestamp <*>  <*>    with id  <*>	979
2	data    colName   locks   filter      spec.key      $eq     DatabaseBackupScheduler      	979
3	caused by  connection   error occured during connection handshake  auth error  sasl conversation error  unable to authenticate using mechanism  <*>    KeyNotFound  Cache Reader No keys found for HMAC that is valid for time    ts  Timestamp <*>  <*>    with id  <*>   lockKey DatabaseBackupScheduler spec.key DatabaseBackupScheduler  	979
4	Nov <*> <*> <*> <*> ERROR  <*> <*>  LockSvc <*>   Not able to check the lock exists or not  	2948
5	Nov <*> <*> <*> <*> INFO  scheduler_service.go <*>  SchedulerService .executeTask   schedule task  DatabaseBackupScheduler  is locked by another process at  <*> <*> <*>  	979
6	data    colName   locks   filter      spec.key      $eq     MsgBrokerMemCacheSubcriberSubscriber      	986
7	caused by  connection   error occured during connection handshake  auth error  sasl conversation error  unable to authenticate using mechanism  <*>    KeyNotFound  Cache Reader No keys found for HMAC that is valid for time    ts  Timestamp <*>  <*>    with id  <*>   lockKey MsgBrokerMemCacheSubcriberSubscriber spec.key MsgBrokerMemCacheSubcriberSubscriber  	987
8	Nov <*> <*> <*> <*> INFO  scheduler_service.go <*>  SchedulerService .executeTask   schedule task  MsgBrokerMemCacheSubcriberSubscriber  is locked by another process at  <*> <*> <*>  	989
9	Nov <*> <*> <*> <*> ERROR  lock_repo.go <*>  LockRepo .FindLocks   finding the lock failed. DbFindError  Error while performing the find operation in the database <*> locks   spec.key    $eq   DeleteDatabaseBackupScheduler     caused by  connection   error occured during connection handshake  auth error  sasl conversation error  unable to authenticate using mechanism  <*>    KeyNotFound  Cache Reader No keys found for HMAC that is valid for time    ts  Timestamp <*>  <*>    with id  <*>	984
10	data    colName   locks   filter      spec.key      $eq     DeleteDatabaseBackupScheduler      	983
11	caused by  connection   error occured during connection handshake  auth error  sasl conversation error  unable to authenticate using mechanism  <*>    KeyNotFound  Cache Reader No keys found for HMAC that is valid for time    ts  Timestamp <*>  <*>    with id  <*>   lockKey DeleteDatabaseBackupScheduler spec.key DeleteDatabaseBackupScheduler  	982
12	Nov <*> <*> <*> <*> INFO  scheduler_service.go <*>  SchedulerService .executeTask   schedule task  DeleteDatabaseBackupScheduler  is locked by another process at  <*> <*> <*>  	980
13	<*>  info  Enabled listen on IPv6 in <*>	11
14	     START logs for container spectro-ui of pod <*>     	1
15	<*>  <*> is not empty  will attempt to perform configuration	4
16	<*>  Looking for shell scripts in <*>	6
17	<*>  Launching <*>	33
18	<*>  info  Getting the checksum of <*>	10
19	<*>  Configuration complete  ready for start up	14
20	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>   <*> 	21
21	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>   <*> 	468
22	     START logs for container <*> of pod <*>     	56
23	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*> <*> false   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>   <*> 	23
24	 Config map  <*>  is invalid. Watching again for appropriate <*> 	74
25	Got initial config snapshot	6
26	 Starting init container.. with jet-namespace and watching <*> <*> 	2
27	     END logs for container <*> of pod <*>     	54
28	     START logs for container jet-manager of pod <*>     	11
29	actionTimeOut    <*>	11
30	 Setting hubble ip to <*> and port to <*> 	15
31	 >>>>>>>>>>>> Initializing upgrade scheduler 	11
32	 Initializing hubble client with URI  <*> <*>  	11
33	<*> <*> <*> INFO <*> metrics server is starting to listen   addr     <*>  	11
34	<*> <*> <*> INFO setup starting manager	11
35	<*> <*> <*> INFO controller-runtime.manager starting metrics server   path    <*>  	11
36	attempting to acquire leader lease  <*>	49
37	successfully acquired lease <*>	67
38	<*> <*> <*> DEBUG <*> Normal   object     kind   ConfigMap   namespace   <*>   name   <*>   uid   <*>   apiVersion   <*>   resourceVersion   <*>     reason    LeaderElection    message    <*> became leader  	11
39	 STARTING LEADER OPERATION 	11
40	 >>>>>>>>>> Initializing namespaces watch 	11
41	<*> <*> <*> INFO <*> Starting EventSource   controller    spectroclusteraction    source    kind source  <*>  Kind   	11
42	<*> <*> <*> INFO <*> Starting Controller   controller    spectroclusteraction  	11
43	<*> <*> <*> INFO <*> Starting workers   controller    spectroclusteraction    worker count   <*> 	11
44	 Polling hubble for pending cluster action s  	11
45	Failed Request  POST <*> error  &  <nil>      <*>  is not supported by the TextConsumer  can be resolved by supporting TextUnmarshaler interface	30
46	retrying  <*>   Request  POST <*> Failed Request  POST <*> error  &  <nil>      <*>  is not supported by the TextConsumer  can be resolved by supporting TextUnmarshaler interface	80
47	 Failed to get auth token for service jet. &  <nil>      <*>  is not supported by the TextConsumer  can be resolved by supporting TextUnmarshaler interface 	23
48	     END logs for container upgrade-ipam of pod <*>     	6
49	     START logs for container upgrade-ipam of pod <*>     	1
50	migrating from host-local to calico-ipam...	2
51	checking host-local IPAM data dir dir existence...	3
52	host-local IPAM data dir dir not found  no migration necessary  successfully exiting...	4
53	migration from host-local to calico-ipam complete node  <*> 	5
54	     START logs for container install-cni of pod <*>     	7
55	 Running as a Kubernetes pod  source  install.go <*> 	8
56	 Installed <*> 	96
57	 Wrote Calico CNI binaries to <*> n 	11
58	 CNI plugin version  <*> n 	11
59	 <*> is not writeable  skipping 	11
60	 Using CNI config template from CNI_NETWORK_CONFIG environment variable.  source  install.go <*> 	11
61	 Created <*> 	11
62	 	264
63	 name    <*>  	11
64	 cniVersion    <*>  	11
65	 plugins    	11
66	 type    calico  	11
67	 log_level    info  	11
68	 log_file_path    <*>  	11
69	 datastore_type    kubernetes  	11
70	 nodename    <*>  	11
71	 mtu   <*> 	11
72	 ipam    	11
73	 type    host-local  	11
74	 subnet    usePodCidr 	11
75	 Failed to get auth token &  <nil>      <*>  is not supported by the TextConsumer  can be resolved by supporting TextUnmarshaler interface 	21
76	  	44
77	 policy    	11
78	 type    k8s 	11
79	 kubernetes    	11
80	 kubeconfig    <*> 	11
81	 type    <*>  	11
82	 snat   true 	11
83	 capabilities     portMappings   true 	11
84	 type    bandwidth  	11
85	 capabilities     bandwidth   true 	11
86	 Done configuring CNI.  Sleep  false 	11
87	     END logs for container install-cni of pod <*>     	11
88	Early log level set to info	11
89	Using NODENAME environment for node name	22
90	Determined node name  <*>	22
91	Checking datastore connection	11
92	Datastore connection verified	11
93	Datastore is ready	11
94	Error getting resource Key GlobalFelixConfig name CalicoVersion  Name  calicoversion  Resource  GlobalFelixConfigs  error the server could not find the requested resource  get <*> calicoversion 	11
95	Using autodetected IPv4 address on interface <*>  <*>	11
96	Node IPv4 changed  will check for conflicts	11
97	No AS number configured on node resource  using global value	11
98	Setting NetworkUnavailable to False	11
99	found <*> <*> in the kubeadm config map	11
100	Loaded configuration from environment config &config.Config LogLevel  info   WorkloadEndpointWorkers <*>  ProfileWorkers <*>  PolicyWorkers <*>  NodeWorkers <*>  Kubeconfig     DatastoreType  kubernetes  	2
101	Neither <*> nor <*> was specified.  Using the inClusterConfig.  This might not work.	316
102	Ensuring Calico datastore is initialized	4
103	Getting initial config snapshot from datastore	5
104	Start called	18
105	Starting status report routine	8
106	Starting controller ControllerType  Node 	9
107	Starting Node controller	10
108	Sending status update Status <*>	32
109	Node controller syncer status updated  <*>	18
110	Starting main event processing loop	22
111	Full resync is required ListRoot  <*> 	55
112	Sending synced update ListRoot  <*> 	49
113	Sending status update Status resync	22
114	Node controller syncer status updated  resync	11
115	Received InSync event from one of the watcher caches	47
116	All watchers have sync d data <*> sending data and final sync	12
117	Main client watcher loop	6
118	successfully synced all hostendpoints	5
119	Node controller is now running	4
120	Synchronizing IPAM data	3
121	Node and IPAM data is in sync	2
122	Starting <*>  <*>	28
123	Flag <*> has been deprecated  see <*> instead.	2
124	Generated self-signed cert in-memory	7
125	Version  <*>	14
126	Starting request-header  <*>	27
127	Serving securely on <*> <*>	18
128	Starting <*>	74
129	 Event occurred  object  <*>  kind  Endpoints  apiVersion  <*>  type  Normal  reason  LeaderElection  message  <*> became leader 	11
130	 Event occurred  object  <*>  kind  Lease  apiVersion  <*>  type  Normal  reason  LeaderElection  message  <*> became leader 	11
131	Waiting for caches to sync for tokens	11
132	Caches are synced for tokens	11
133	Started  job 	11
134	Starting job controller	11
135	Waiting for caches to sync for job	11
136	Started  replicaset 	11
137	Starting replicaset controller	11
138	Waiting for caches to sync for ReplicaSet	11
139	Started  bootstrapsigner 	11
140	Waiting for caches to sync for bootstrap_signer	11
141	Started  <*> 	33
142	Starting PV protection controller	11
143	Waiting for caches to sync for PV protection	11
144	Starting <*> controller	33
145	Waiting for caches to sync for <*>	88
146	Started  podgc 	11
147	Starting GC controller	11
148	Waiting for caches to sync for GC	11
149	Starting garbage collector controller	11
150	Waiting for caches to sync for garbage collector	11
151	GraphBuilder running	11
152	Started  garbagecollector 	11
153	Started  csrapproving 	11
154	Starting certificate controller  csrapproving 	11
155	Started  csrcleaner 	11
156	Starting CSR cleaner controller	11
157	Started  ttl 	11
158	Starting TTL controller	11
159	Waiting for caches to sync for TTL	11
160	Started  persistentvolume-expander 	11
161	<*> is set  but no cloud provider specified. Will not configure cloud provider routes.	11
162	Skipping  route 	11
163	Starting expand controller	11
164	Waiting for caches to sync for expand	11
165	Started  clusterrole-aggregation 	11
166	Starting ClusterRoleAggregator	11
167	Waiting for caches to sync for ClusterRoleAggregator	11
168	Starting PVC protection controller	11
169	Waiting for caches to sync for PVC protection	11
170	Started  endpointslice 	11
171	Starting endpoint slice controller	11
172	Waiting for caches to sync for endpoint_slice	11
173	Started  namespace 	11
174	Starting namespace controller	11
175	Waiting for caches to sync for namespace	11
176	     END logs for container spectro-ui of pod <*>     	1
177	Started  disruption 	11
178	Starting disruption controller	11
179	Waiting for caches to sync for disruption	11
180	Started  statefulset 	11
181	Starting stateful set controller	11
182	Waiting for caches to sync for stateful set	11
183	Starting <*>  <*>  <*>	55
184	Starting certificate controller  <*> 	44
185	Started  csrsigning 	11
186	Sending events to api server.	55
187	No Secondary Service CIDR provided. Skipping filtering out secondary service addresses.	11
188	Started  nodeipam 	11
189	Skipping  <*> 	33
190	Starting ipam controller	11
191	Waiting for caches to sync for node	11
192	Started  endpoint 	11
193	Starting endpoint controller	11
194	Waiting for caches to sync for endpoint	11
195	QuotaMonitor created object count evaluator for <*>	165
196	QuotaMonitor created object count evaluator for podtemplates	11
197	QuotaMonitor created object count evaluator for ingresses.extensions	11
198	QuotaMonitor created object count evaluator for endpoints	11
199	QuotaMonitor created object count evaluator for jobs.batch	11
200	QuotaMonitor created object count evaluator for limitranges	11
201	QuotaMonitor created object count evaluator for cronjobs.batch	11
202	QuotaMonitor created object count evaluator for serviceaccounts	11
203	Started  resourcequota 	11
204	Starting resource quota controller	11
205	Waiting for caches to sync for resource quota	22
206	QuotaMonitor running	11
207	Started  horizontalpodautoscaling 	11
208	Starting HPA controller	11
209	Waiting for caches to sync for HPA	11
210	Started  cronjob 	11
211	Starting CronJob Manager	11
212	Sending events to api server	11
213	failed to start cloud node lifecycle controller  no cloud provider provided	11
214	Started  persistentvolume-binder 	11
215	Skipping  ephemeral-volume 	11
216	Starting persistent volume controller	11
217	<*> <*> <*> <*> I | etcdmain  Go <*>  linux/amd64	6
218	     START logs for container etcd of pod <*>     	1
219	 WARNING  Deprecated  <*> capnslog  flag is set  use  <*> zap  flag instead	10
220	<*> <*> <*> <*> I | etcdmain  etcd Version  <*>	3
221	<*> <*> <*> <*> I | etcdmain  Git SHA  <*>	4
222	<*> <*> <*> <*> I | etcdmain  Go Version  <*>	5
223	<*> <*> <*> <*> I | etcdmain  setting maximum number of CPUs to 4  total number of available CPUs is 4	7
224	<*> <*> <*> <*> I | embed  peerTLS  cert   <*>  key   <*>  <*>   <*>  <*>   true  crl-file  	9
225	<*> <*> <*> <*> I | embed  name   <*>	10
226	<*> <*> <*> <*> I | embed  data dir   <*>	11
227	<*> <*> <*> <*> I | embed  member dir   <*>	11
228	<*> <*> <*> <*> I | embed  heartbeat   <*>	11
229	<*> <*> <*> <*> I | embed  election   <*>	11
230	<*> <*> <*> <*> I | embed  snapshot count   <*>	11
231	Successfully retrieved node IP  <*>	2
232	<*> node IP is an IPv4 address  <*>   assume IPv4 operation	3
233	Unknown proxy mode     assuming iptables proxy	4
234	Using iptables Proxier.	5
235	Set sysctl  <*>  to <*>	14
236	Setting nf_conntrack_max to <*>	7
237	Set sysctl  net/netfilter/nf_conntrack_tcp_timeout_established  to <*>	7
238	Starting service config controller	7
239	Starting endpoint slice config controller	6
240	Waiting for caches to sync for service config	5
241	Waiting for caches to sync for endpoint slice config	4
242	Caches are synced for service config	3
243	Caches are synced for endpoint slice config	2
244	<*> <*> <*> <*> I | embed  advertise client URLs   https <*> <*>	11
245	<*> <*> <*> <*> I | etcdserver  starting member <*> in cluster <*>	11
246	<*> <*> <*> <*> INFO  <*> switched to configuration voters   	11
247	<*> <*> <*> <*> INFO  <*> became <*> at term 0	11
248	<*> <*> <*> <*> INFO  newRaft <*>  peers      term  0  commit  0  applied  0  lastindex  0  lastterm  0 	11
249	<*> <*> <*> <*> INFO  <*> became <*> at term <*>	22
250	<*> <*> <*> <*> INFO  <*> switched to configuration voters  <*> 	22
251	<*> <*> <*> <*> W | auth  simple token is not cryptographically signed	11
252	<*> <*> <*> <*> I | etcdserver  starting server...  version  <*>  cluster version  to_be_decided 	11
253	<*> <*> <*> <*> I | etcdserver  <*> as single-node  fast-forwarding <*> ticks  election ticks <*> 	11
254	<*> <*> <*> <*> I | etcdserver/membership  added member <*>  https <*> <*>  to cluster <*>	11
255	Loaded <*> validating admission controller s  successfully in the following order  LimitRanger ServiceAccount Priority PersistentVolumeClaimResize RuntimeClass CertificateApproval CertificateSigning CertificateSubjectRestriction ValidatingAdmissionWebhook ResourceQuota.	25
256	Flag <*> has been deprecated  This flag will be removed in a future version.	2
257	external host was not specified  using <*>	3
258	Loaded <*> mutating admission controller s  successfully in the following order  NamespaceLifecycle PodPreset LimitRanger ServiceAccount NodeRestriction TaintNodesByCondition Priority DefaultTolerationSeconds DefaultStorageClass StorageObjectInUseProtection RuntimeClass DefaultIngressClass MutatingAdmissionWebhook.	23
259	parsed scheme   endpoint 	830
260	ccResolverWrapper  sending new addresses to cc    https <*> <*>  <nil> 0 <nil>  	826
261	parsed scheme   <*> 	22
262	found v6  in the kubeadm config map	11
263	<*> is true  defaulted  through environment variable	11
264	Ensure default IPv4 pool is created. IPIP mode  Always  VXLAN mode  Never	11
265	Attempt to  List  using kubernetes backend is not supported.	11
266	Created default IPv4 pool  <*>  with NAT outgoing true. IPIP mode  Always  VXLAN mode  Never	11
267	<*> is false through environment variable	11
268	Using node name  <*>	11
269	<*> <*> <*> <*> I | embed  ClientTLS  cert   <*>  key   <*>  <*>   <*>  <*>   true  crl-file  	11
270	<*> <*> <*> <*> I | embed  listening for peers on <*> <*>	11
271	<*> <*> <*> <*> I | embed  listening for metrics on http <*> <*>	11
272	<*> <*> <*> <*> INFO  <*> is starting a new election at term <*>	11
273	<*> <*> <*> <*> INFO  <*> received MsgVoteResp from <*> at term <*>	11
274	<*> <*> <*> <*> INFO  <*> became leader at term <*>	11
275	<*> <*> <*> <*> INFO  raft.node  <*> <*> leader <*> at term <*>	11
276	<*> <*> <*> <*> I | etcdserver  published  Name <*> ClientURLs  https <*> <*>   to cluster <*>	11
277	<*> <*> <*> <*> I | embed  ready to serve client requests	22
278	<*> <*> <*> <*> I | etcdserver  setting up the initial cluster version to <*>	11
279	<*> <*> <*> <*> N | etcdserver/membership  set the initial cluster version to <*>	11
280	<*> <*> <*> <*> I | etcdserver/api  enabled capabilities for version <*>	11
281	<*> <*> <*> <*> I | embed  serving client requests on <*> <*>	22
282	<*> <*> <*> <*> W | etcdserver  read-only range request  key   <*> controller <*>     with result  range_response_count 0 size 4  took too long  <*>  to execute	11
283	<*> <*> <*> <*> I | etcdserver/api/etcdhttp  <*> OK  status code <*> 	1635
284	<*> <*> <*> <*> W | etcdserver  read-only range request  key   <*>     with result  range_response_count <*> size <*>  took too long  <*>  to execute	22
285	Valid token audiences 	2
286	Generating self signed cert as no cert is provided	3
287	Starting TCP socket on <*> <*>	4
288	Listening securely on <*> <*>	5
289	Continuing without authentication configuration. This may treat all requests as anonymous.	6
290	Registering SelectorSpread plugin	22
291	     START logs for container manager of pod <*>     	8
292	 metrics server is starting to listen    addr    <*> 	19
293	 settin webhook 	9
294	Waiting for caches to sync for persistent volume	11
295	Started  endpointslicemirroring 	11
296	Starting EndpointSliceMirroring controller	11
297	Waiting for caches to sync for endpoint_slice_mirroring	11
298	Started  serviceaccount 	11
299	Starting service account controller	11
300	Waiting for caches to sync for service account	11
301	Started  daemonset 	11
302	Starting daemon sets controller	11
303	Waiting for caches to sync for daemon sets	11
304	Started  deployment 	11
305	Starting deployment controller	11
306	Waiting for caches to sync for deployment	11
307	<*> <*> <*> <*> deployment  system  is created	48
308	     START logs for container init of pod <*>     	10
309	 Starting init container... 	20
310	<*> <*> <*> <*> waiting for deployment  system  to be created	24
311	<*> <*> <*> <*> connecting k8s with	235
312	<*> <*> <*> <*> number of replicas for deployment  system     <*> 	56
313	<*> <*> <*> <*> searching deployment s  in <*> Namespace	124
314	<*> <*> <*> <*> total number of replicas  <*>     number of ready replicas  0  out of  <*> 	180
315	<*> <*> <*> <*> Watching deployment with fields  metadata.name system  in <*> Namespace	80
316	     END logs for container init of pod <*>     	108
317	     START logs for container prerequisites of pod <*>     	99
318	     END logs for container prerequisites of pod <*>     	99
319	     START logs for container hashboard of pod <*>     	11
320	Oct <*> <*> <*> <*> INFO  system_config_service.go <*>  SystemConfigService .InitTimeseries   Timeseries Sys Config    Meta  ObjectId <*> TypeMeta  Kind timeseries  Metadata  Uid <*> Name timeseries Annotations map scopeVisibility <*>  Labels map   Tags    CreationTimestamp <*> <*> <*> <*> <*> UTC IsDeleted false DeletionTimestamp <*> <*> <*> <*> <*> UTC  AclMeta  OwnerUid sysadmin TenantUid  ProjectUid  Scope system Visibility <*>   Spec  Metrics  BatchInterval <*> ArchivalInterval <*> RetentionPeriod <*> Enabled false  PodMetrics  BatchInterval <*> ArchivalInterval <*> RetentionPeriod <*> Enabled false    	102
321	Oct <*> <*> <*> <*> WARNING  system_config_service.go <*>  SystemConfigService .InitCertificates   default server certificates not intialized 	92
322	Oct <*> <*> <*> <*> INFO  system_config_service.go <*>  SystemConfigService <*>   skipping init for spectro mgmt 	144
323	Oct <*> <*> <*> <*> ERROR  email_client.go <*>   dial email failed with the error  dial tcp  0  connect  connection refused 	89
324	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  <*>  subscribers for  MemCacheSubcriber  	88
325	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  MemCacheSubcriber  <*>  <*>  with registry ref  <*>  	88
326	Oct <*> <*> <*> <*> INFO  <*> <*>  MsgBrokerClient <*>   Listening on <*> 	411
327	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribe   successfully subscribed to <*> 	407
328	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .SubscribeWithRegistry   subscribed to nats registry   <*> <*> subscriberType MemCacheSubcriber  	88
329	ccResolverWrapper  sending update to cc     https <*> <*>  <nil> 0 <nil>   <nil> <nil> 	22
330	ClientConn switching balancer to  pick_first 	22
331	Using reconciler  lease	11
332	<*> <*> <*> <*> deployment  nats  is created	6
333	 registering webhook    path   <*> 	54
334	 Registering a mutating webhook    GVK    Group   <*>   Version   <*>   Kind   PodPreset    path   <*> 	11
335	Error looking up in-cluster authentication configuration  Get  https <*> <*>   dial tcp <*> <*>  connect  connection refused	5
336	To require authentication configuration lookup to succeed  set <*> false	7
337	Starting <*>  <*>  <*>  <*>	10
338	caused by  mongo  no documents in result 	192
339	     START logs for container mgmt of pod <*>     	1
340	<*> <*> <*> <*> configured elder config is empty  expects unencrypted secrets	2
341	trueOct <*> <*> <*> <*> INFO  mgmt_service.go <*>  MgmtService .updateSpectroMgmtUpgradeStatus   Updating spectro mgmt upgrade status 	3
342	     START logs for container event of pod <*>     	11
343	<*> <*> <*> W  ASIO      main  No TransportLayer configured during NetworkInterface startup	28
344	     START logs for container mongo of pod <*>     	1
345	about to fork child process  waiting until server is ready for connections.	2
346	forked process  <*>	3
347	<*> <*> <*> I  CONTROL   main        SERVER RESTARTED      	15
348	<*> <*> <*> I  CONTROL   main  Automatically disabling TLS <*>  to force-enable TLS <*> specify <*>  none 	27
349	<*> <*> <*> I  CONTROL   initandlisten  MongoDB starting   pid <*> port <*> dbpath <*> <*> host <*>	18
350	<*> <*> <*> I  CONTROL   initandlisten  db version <*>	19
351	<*> <*> <*> I  CONTROL   initandlisten  git version  <*>	20
352	<*> <*> <*> I  CONTROL   initandlisten  OpenSSL version  OpenSSL <*>  <*> Sep <*>	21
353	<*> <*> <*> I  CONTROL   initandlisten  allocator  tcmalloc	22
354	<*> <*> <*> I  CONTROL   initandlisten  modules  none	22
355	<*> <*> <*> I  CONTROL   initandlisten  build environment 	22
356	<*> <*> <*> I  CONTROL   initandlisten      distmod  <*>	22
357	<*> <*> <*> <*> I | mvcc  store.index  compact <*>	38
358	<*> <*> <*> <*> I | mvcc  finished scheduled compaction at <*>  took <*> 	37
359	<*> <*> <*> <*> waiting for deployment  nats  to be created	3
360	<*> <*> <*> <*> number of replicas for deployment  nats     <*> 	7
361	     START logs for container init-nats of pod <*>     	10
362	 installer mode found  <*>  	44
363	 request parameters main.hostReq isVip false  urlVariable   NATS_URL    targetNamespace   <*>    namespace   <*>    targetConfigMap   <*>    serviceType       labels   <*> <*>    podSubstr   <*>    serviceName   <*>    portName   nats    kubeConfigPath       imageName       isHostNetwork false  cli   <*>  nil   installerMode   <*>    	11
364	     START logs for container auth of pod <*>     	11
365	<*> <*> <*> <*> deployment  configserver  is created	6
366	<*> <*> <*> <*> waiting for deployment  configserver  to be created	3
367	<*> <*> <*> <*> number of replicas for deployment  configserver     <*> 	7
368	<*> <*> <*> <*> Watching deployment with fields  metadata.name configserver  in <*> Namespace	10
369	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerMemCacheSubcriberSubscriber with an interval of <*>  	88
370	Oct <*> <*> <*> <*> WARNING  <*> <*>  NatsRegistryRepo <*>   nats registry already exists with same subscriber subject  <*> and type  MemCacheSubcriber  	88
371	Oct <*> <*> <*> <*> WARNING  <*> <*>  NatsRegistryRepo <*>   nats registry already exists with same subscriber subject  <*> and type  SystemSubscriber  	87
372	Oct <*> <*> <*> <*> WARNING  <*> <*>  NatsRegistryRepo <*>   nats registry already exists with same subscriber subject  <*> and type  MigrationSubscriber  	84
373	Oct <*> <*> <*> <*> ERROR  mgmt_repo.go <*>  MgmtConfigRepo <*>   Unable to get spectro mgmt doc due to ResourceNotFound  Resource not found for the type  systems 	60
374	data    params   $and      kind   $eq spectroMgmt       metadata.name   $eq spectroMgmt         resType   systems  	121
375	Oct <*> <*> <*> <*> ERROR  mgmt_service.go <*>  MgmtService .updateSpectroMgmtUpgradeStatus   Unable to get the spectro mgmt due to ResourceNotFound  Resource not found for the type  systems 	61
376	Oct <*> <*> <*> <*> INFO  mgmt_backup_service.go <*>  BackUpService .getBackupFromConfigMap   no config map present for migration status 	11
377	 >>>>>>> Listing pod with labels in <*> namespace 	44
378	 >>>>>>> Found pod <*> in <*> namespace 	44
379	 hostNetwork is true for the pod 	44
380	 >>>>>>> Listing nodes 	44
381	 Updating HostIp  with value <*>  	44
382	 >>>>>>> Getting pod with name  <*>  in <*> namespace 	44
383	 Updating HostPort  with value <*>  	44
384	 Updating Ip  with value <*>  	44
385	Calico node started successfully	11
386	bird  Unable to open configuration file <*>  No such file or directory	22
387	Waiting for caches to sync for <*>  <*>  <*>  <*>	11
388	     START logs for container cert-manager of pod <*>     	2
389	starting <*> <*>  revision <*> 	2
390	     START logs for container user of pod <*>     	11
391	Starting provisioner controller <*>	6
392	 Found cluster id as  <*>   cluster name as  <*>   tenant id as     project uid as    and is system  true  	6
393	 Starting Orchestration Executor 	2
394	 Starting cluster management agent... 	3
395	 Looking for  <*>  config map in namespace <*> 	5
396	 Initializing hubble client with URI  <*> <*>   cluster <*>	7
397	Failed Request  GET <*> uid  error   & Code Service Unavailable Details <html>	8
398	<head><title>503 Service Temporarily Unavailable</title></head>	20
399	<body>	21
400	<center><h1>503 Service Temporarily <*>	22
401	     START logs for container timeseries of pod <*>     	11
402	register gcp provider <*>	6
403	register aws provider <*>	2
404	register azure provider <*>	3
405	 listening for connections on   address   <*> <*> 	6
406	 Updating Port  with value <*>  	44
407	 Applied config     nodePathMap       node     <*>     paths      <*>       	2
408	     START logs for container nats of pod <*>     	6
409	Repository     https <*>	6
410	<hr><center>nginx</center>	22
411	<*> <*> <*> I  CONTROL   initandlisten      distarch  <*>	22
412	register maas provider <*>	4
413	<*>	15
414	register vsphere provider	5
415	register generic provider	7
416	register openstack provider	8
417	 watch in a single namespace    namespace   <*> 	9
418	 resync period    every  <*>	10
419	     START logs for container system of pod <*>     	11
420	     START logs for container controller of pod <*>     	1
421	NGINX Ingress controller	3
422	Release        <*>	4
423	Build          <*>	5
424	nginx version  <*>	7
425	 Provisioner started 	3
426	     START logs for container nas of pod <*>     	6
427		117
428	 starting controller    <*>   <*>   version   <*> 	2
429	 Deleted ConfigMap with name natsoperatortoken in namespace <*> EXTRA <nil>   file  <*> <*>  func  <*>  ConfigMapOperationTask .Delete 	6
430	 Created ConfigMap with name natsoperatortoken in namespace <*>  file  <*> <*>  func  <*>  ConfigMapOperationTask .Create 	8
431	 Updating url  with value <*> <*>  	44
432	Event <*> Kind  Endpoints   Namespace  <*>   Name  <*>   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   LeaderElection  <*> became leader	7
433	 Normal    message   <*> became leader   object    kind   ConfigMap   namespace   <*>   name   <*>   uid   <*>   apiVersion   <*>   resourceVersion   <*>    reason   LeaderElection 	15
434	 configured acme <*> nameservers   nameservers    <*> <*>  	4
435	 Registering a validating webhook    GVK    Group   <*>   Version   <*>   Kind   PodPreset    path   <*> 	11
436	Started provisioner controller <*>	8
437	 starting leader election 	5
438	</body>	22
439	provision  <*>  class  <*>   started	9
440	 Starting EventSource    controller   apiservice   source    Type    metadata    creationTimestamp  null   spec    service  null  groupPriorityMinimum  0  versionPriority  0   status      	20
441	 new certificate request controller registered    type   acme 	9
442	 config doesn t contain node <*>  use <*> instead 	10
443	 Creating volume <*> at <*> <*> 	10
444	 Watching for Ingress  class  nginx 	11
445	 starting manager 	22
446	 new certificate request controller registered    type   ca 	10
447	<*> <*> <*>  invalid use of field  ping_interval   ping_interval should be converted to a duration	7
448	 Starting EventSource    controller   mutatingwebhookconfiguration   source    Type    metadata    creationTimestamp  null   	43
449	 new certificate request controller registered    type   venafi 	11
450	 <*>  <*> <*> <*> <*>  INF  Starting <*> version <*>	8
451	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  0  subscribers for  EventSubscriber  	18
452	 Starting EventSource    controller   validatingwebhookconfiguration   source    Type    metadata    creationTimestamp  null   	44
453	 <*>  <*> <*> <*> <*>  INF  Git commit  <*> 	9
454	Early screen log level set to info	11
455	<*> <*> <*> <*>   <*> 0m  starting NATS Account server  version <*>	7
456	</html>	22
457	 <*>  <*> <*> <*> <*>  WRN  Plaintext passwords detected  use nkeys or bcrypt.	10
458	<*> <*> <*> I  CONTROL   initandlisten      target_arch  <*>	22
459	<*> <*> <*> <*>   <*> 0m  server time is Thu Oct <*> <*> <*> <*> UTC <*>	8
460	 <*>  <*> <*> <*> <*>  INF  Trusted Operators	11
461	<*> <*> <*> <*>   <*> 0m  loading operator from <*>	9
462	<*> <*> <*> <*>   <*> 0m  loading system account from <*>	10
463	 Updating installerMode  with value <*>  	44
464	<*> <*> <*> <*>   <*> 0m  creating a store at <*>	11
465	 create the helper pod <*> into <*> 	9
466	 starting controller   controller   <*> 	66
467	 starting palette metrics server at  <*> 	11
468	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerEventSubscriberSubscriber with an interval of <*>  	11
469	Ingresses with an empty class will also be processed by this Ingress controller	11
470	Message <*> Service Temporarily Unavailable Ref  	22
471	Felix starting up GOMAXPROCS 4 builddate  <*>  gitcommit  <*> <*> <*>  version  <*> 	11
472	 Starting EventSource    controller   customresourcedefinition   source    Type    metadata    creationTimestamp  null   spec    group      names    plural      kind       scope       status    conditions  null  acceptedNames    plural      kind       storedVersions  null   	22
473	 Registering a mutating webhook    GVK    Group   <*>   Version   <*>   Kind   ClusterPodPreset    path   <*> 	11
474	<*> <*> <*> <*>   <*> 0m  connecting to NATS for notifications	39
475	 <*>  <*> <*> <*> <*>  INF    System      	11
476	Failed to watch  <*>  failed to list  <*>  Get  https <*> <*> 500&resourceVersion 0   dial tcp <*> <*>  connect  connection refused	484
477	<*> <*> <*> <*> waiting for configmap  configserver  to be created	11
478	     END logs for container init-nats of pod <*>     	11
479	<*> <*> <*> I  CONTROL   initandlisten  <*> MB of memory available to the process out of <*> MB total system memory	22
480	Oct <*> <*> <*> <*> INFO  system_service.go <*>  SystemService .CreateDefaultServerMeta   spectro version in env  SERVER_SPECTROVERSION  is <*> 	10
481	Controller will reconcile labels.	11
482	Event <*> Kind  PersistentVolumeClaim   Namespace  <*>   Name  <*>   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   Provisioning  External provisioner is provisioning volume for claim  <*> 	8
483	 starting control loop 	132
484	     START logs for container hutil of pod <*>     	11
485	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job eventsArchive with an interval of <*>  	11
486	retrying  <*>   Request  GET <*> uid  Failed Request  GET <*> uid  error   & Code Service Unavailable Details <html>	11
487	Kubernetes server override env vars. KUBERNETES_SERVICE_HOST  <*>  KUBERNETES_SERVICE_PORT  <*> 	11
488	Loading configuration...	11
489	 <*>  <*> <*> <*> <*>  INF    Operator   spectro 	11
490	<*> <*> <*> <*>   <*> 0m  failed to connect to NATS  nats  no servers available for connection	31
491	Started  nodelifecycle 	11
492	Failed to watch  <*>  failed to list  <*>  Get  https <*> <*> <*> 500&resourceVersion 0   dial tcp <*> <*>  connect  connection refused	88
493	<*> <*> <*> I  CONTROL   initandlisten  options    net    bindIp   <*>   port  <*>  tls    mode   disabled       processManagement    fork  true  pidFilePath   <*>     security    keyFile   <*>     storage    dbPath   <*>     systemLog    destination   file   logAppend  true  path   <*>     	11
494	     START logs for container cluster of pod <*>     	11
495	 Starting EventSource    controller   validatingwebhookconfiguration   source    Type    metadata    creationTimestamp  null   spec    secretName      issuerRef    name        status      	11
496	     START logs for container init-rootdomain of pod <*>     	11
497	Oct <*> <*> <*> <*> ERROR  system_service.go <*>  SystemService <*>   validation failed for default syscconfig DataValidationError  ShouldNotEmpty   accessKey  should be not be empty   secretKey  should be not be empty   accoutId  should be not be empty 	9
498	 Volume <*> has been created on <*> <*> 	7
499	 <*>  <*> <*> <*> <*>  INF    Issued    <*> <*> <*> <*> <*> UTC	10
500	Starting node controller	11
501	 starting metrics server    path   <*> 	22
502	 Registering a validating webhook    GVK    Group   <*>   Version   <*>   Kind   ClusterPodPreset    path   <*> 	11
503	Found felix environment variable   etcdscheme    	11
504	provision  <*>  class  <*>   volume  <*>  provisioned	6
505	 Creating API client  host  https <*> <*> 	11
506	<*> <*> <*> I  STORAGE   initandlisten 	22
507	Oct <*> <*> <*> <*> ERROR  system_service.go <*>  SystemService <*>   validation failed for default syscconfig DataValidationError  ShouldNotEmpty   From  should be not be empty   UserName  should be not be empty   password  should be not be empty   SMTPServer  should be not be empty 	8
508	Waiting for caches to sync for taint	11
509	 request parameters main.hostReq isVip false  urlVariable   ROOT_DOMAIN    targetNamespace   <*>    namespace   <*>    targetConfigMap   <*>    serviceType       labels   <*> <*>    podSubstr   <*>    serviceName   <*>    portName   https    kubeConfigPath       imageName       isHostNetwork false  cli   <*>  nil   installerMode   <*>    	11
510	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerEventSubscribersValidator with an interval of <*>  	11
511	<*> <*> <*> <*> Serving event at https <*>     <*>	10
512	Oct <*> <*> <*> <*> INFO  event_service.go <*>  EventService .deleteEvents   Deleting all the events exceeding the limit <*> per cluster 	9
513	Oct <*> <*> <*> <*> INFO  event_service.go <*>  EventService .deleteEvents   Skipping events deletion as no running clusters found 	8
514	Oct <*> <*> <*> <*> INFO  <*> <*>  MsgBrokerClient <*>   Listening on ally.sysadmin. .event.> 	6
515	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribe   successfully subscribed to ally.sysadmin. .event.> 	5
516	Oct <*> <*> <*> <*> INFO  <*> <*>  peerHandler .startPeerSubscribers   Peer subscribe message received  Publisher hubble Key subscribe Data   ServiceName   ally   SubscriberName   ally   AccountUid   sysadmin   UserUid      SubscriberType   EventSubscriber   WildcardReplacement    <*> <*>   Clone  false  IsGrouped  true  Group      IsNotified  true  IsErr false Timestamp <*>  	4
517	 Starting EventSource   controller   pack   source    Type    metadata    creationTimestamp  null   spec    packConfigSpec    layer       packRef    layer      server      name      tag        status      	11
518	 starting webhook server 	11
519	 Updated current TLS certificate 	11
520	provision  <*>  class  <*>   succeeded	5
521	Trying to <*> persistentvolume  <*> 	4
522	persistentvolume  <*>  saved	3
523	Event <*> Kind  PersistentVolumeClaim   Namespace  <*>   Name  <*>   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   ProvisioningSucceeded  Successfully provisioned volume <*>	2
524	Found felix environment variable   etcdkeyfile    	11
525	Found felix environment variable   wireguardmtu   <*> 	11
526	Found felix environment variable   vxlanmtu   <*> 	11
527	Found felix environment variable   healthenabled   true 	11
528	 Running in Kubernetes cluster  major  <*>  minor  <*>  git  <*>  state  clean  commit  <*>  platform  linux/amd64 	11
529	Oct <*> <*> <*> <*> ERROR  system_service.go <*>  SystemService <*>   validation failed for default syscconfig DataValidationError  ShouldNotEmpty   entityId  should be not be empty   audienceUrl  should be not be empty   acsUrlRoot  should be not be empty 	7
530	     START logs for container cloud of pod <*>     	11
531	<*> <*> <*> <*>   <*> 0m  will try to connect again in <*> milliseconds	30
532	<*> <*> <*> <*> Serving auth at https <*>     <*>	9
533	 <*>  <*> <*> <*> <*>  INF    Expires   <*> <*> <*> <*> <*> UTC	9
534	 Starting EventSource    controller   mutatingwebhookconfiguration   source    Type    metadata    creationTimestamp  null   spec    secretName      issuerRef    name        status      	11
535	Oct <*> <*> <*> <*> ERROR  mgmt_upgrade_service.go <*>  HubbleUpgradeService .GetHubbleUpgrde   Unable to find the <*> configmap  will go ahead with the flow 	11
536	Found felix environment variable   etcdcafile    	11
537	 SSL fake certificate created  file  <*> 	11
538	Throttling request took <*>  request  GET https <*> <*> <*>	11
539	 Starting EventSource   controller   spectrocluster   source    Type    metadata    creationTimestamp  null   spec    clusterProfileTemplate      clusterConfig    machineManagementConfig      machineHealthConfig        status      	11
540	 <*>  <*> <*> <*> <*>  WRN  Trusted Operators should utilize a System Account	8
541	<*> <*> <*> I  STORAGE   initandlisten     WARNING  Using the XFS filesystem is strongly recommended with the WiredTiger storage engine	22
542	<*> <*> <*> I  STORAGE   initandlisten              See http <*>	22
543	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job machineMetricsArchive with an interval of <*>  	11
544	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  0  subscribers for  TimeseriesSubscriber  	22
545	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerTimeseriesSubscriberSubscriber with an interval of <*>  	11
546	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerTimeSeriesSubscribersValidator with an interval of <*>  	11
547	<*> <*> <*> <*> Serving timeseries at https <*>     <*>	11
548	Oct <*> <*> <*> <*> INFO  metric_service.go <*>  MetricService .cleanupMetrics   Cleaning up the machine metrics older than  <*>  days 	11
549	Oct <*> <*> <*> <*> INFO  metric_base.go <*>  MetricBase .DeleteMachineMetrics   Cleaning up the machine metrics older than  <*> <*> <*> <*> <*> UTC  	11
550	     END logs for container event of pod <*>     	1
551	<*> <*> <*> <*>   <*> 0m  https listening on port <*>	11
552	<*> <*> <*> <*>   <*> 0m  <*> is running	11
553	<*> <*> <*> <*>   <*> 0m  configure the <*> with 	11
554	<*> <*> <*> <*>   <*> 0m    resolver  URL https <*> <*> 	11
555	 <*>  <*> <*> <*> <*>  INF  Starting http monitor on <*> <*>	7
556	 Starting Controller   controller   pack 	11
557	<*> <*> <*> I  STORAGE   initandlisten  wiredtiger_open config  create cache_size <*> cache_overflow  file_max 0M  session_max <*> eviction  threads_min 4 threads_max 4  config_base false statistics  fast  log  enabled true archive true path journal compressor snappy  file_manager  close_idle_time <*> close_scan_interval <*> close_handle_minimum <*>  statistics_log  wait 0  verbose  recovery_progress checkpoint_progress  	22
558	 Starting EventSource    controller   apiservice   source    Type    metadata    creationTimestamp  null   spec    secretName      issuerRef    name        status      	11
559	<*> <*> <*> I  STORAGE   initandlisten  WiredTiger message  <*> <*>  <*> <*>   txn-recover  Set global recovery timestamp   0  0 	22
560	Found felix environment variable   etcdendpoints    	11
561	Found felix environment variable   felixhostname   <*> 	11
562	Found felix environment variable   datastoretype   kubernetes 	11
563	Found felix environment variable   etcdcertfile    	11
564	Found felix environment variable   etcdaddr    	11
565	Found felix environment variable   defaultendpointtohostaction   ACCEPT 	11
566	Found felix environment variable   ipinipmtu   <*> 	11
567	Oct <*> <*> <*> <*> INFO  metric_repo.go <*>  TangoMetricRepo .DeleteMetrics   Deleting machine metric doc s  older than  <*> <*> <*>  	11
568	Oct <*> <*> <*> <*> INFO  metric_service.go <*>  MetricService .cleanupMetrics   Cleaning up the pod metrics older than  <*>  days 	11
569	 <*>  <*> <*> <*> <*>  INF  Listening for client connections on <*> <*>	6
570	<*> <*> <*> I  RECOVERY  initandlisten  WiredTiger recoveryTimestamp. Ts  Timestamp 0  0 	22
571	 serving webhook server    host      port  <*>	11
572	Oct <*> <*> <*> <*> ERROR  system_service.go <*>  SystemService <*>   validation failed for default syscconfig DataValidationError  ShouldNotEmpty   imagesHostEndpoint  should be not be empty 	11
573	 Starting EventSource    controller   customresourcedefinition   source    Type    metadata    creationTimestamp  null   spec    secretName      issuerRef    name        status      	11
574	 Starting EventSource    controller   customresourcedefinition   source    Type    metadata    creationTimestamp  null   	22
575	 <*>  <*> <*> <*> <*>  INF  TLS required for client connections	5
576	 <*>  <*> <*> <*> <*>  INF  Server id is <*>	4
577	 <*>  <*> <*> <*> <*>  INF  Server is ready	3
578	 <*>  <*> <*> <*> <*>  INF  Listening for route connections on <*> <*>	2
579	     END logs for container nats of pod <*>     	1
580	<*> <*> <*> <*> waiting for deployment  nas  to be created	44
581	Oct <*> <*> <*> <*> ERROR  err_logger.go <*>   reqId <*>   Unable to read the  Authorization  cookie data   code CookieReadError mtd GET ref <*> uri <*>  	7
582	Oct <*> <*> <*> <*> ERROR  tenant_repo.go <*>  TenantRepo .FindTenantByOrganization   reqId <*>   finding the tenant by organization failed. ResourceNotFound  Resource not found for the type  tenant 	6
583	data    params   spec.orgName     $eq <*>      resType   tenant  	5
584	caused by  mongo  no documents in result   name <*> tenant <*>  	4
585	Oct <*> <*> <*> <*> INFO  metric_base.go <*>  MetricBase .DeletePodMetrics   Cleaning up the pod metrics older than  <*> <*> <*> <*> <*> UTC  	10
586	 Starting workers   controller   pack   worker count  <*>	11
587	 Starting EventSource    controller   apiservice   source    Type    metadata    creationTimestamp  null   	22
588	 Starting EventSource   controller   spectrocluster   source    Type    metadata    creationTimestamp  null   spec    clusterConfig    region      endpointAccess        status      	11
589	<*> <*> <*> <*>   <*> 0m  connected to NATS for account and activation notifications	6
590	Found felix environment variable   logseverityscreen   info 	11
591	     END logs for container init-rootdomain of pod <*>     	11
592	     START logs for container init-appdomain of pod <*>     	11
593	 request parameters main.hostReq isVip false  urlVariable   API_SERVER    targetNamespace   <*>    namespace   <*>    targetConfigMap   <*>    serviceType       labels   <*> <*>    podSubstr   <*>    serviceName   <*>    portName   https    kubeConfigPath       imageName       isHostNetwork false  cli   <*>  nil   installerMode   <*>    	22
594	 Enabling new Ingress features available since Kubernetes <*> 	11
595	Oct <*> <*> <*> <*> INFO  pod_metric_repo.go <*>  TangoPodMetricRepo .DeleteMetrics   Deleting pod metric doc s  older than  <*> <*> <*> <*> <*> UTC  	9
596	No IngressClass resource with name nginx found. Only annotation will be used.	11
597	<*> <*> <*> <*> <*>  invalid Cookie.Domain  <*> <*>   dropping domain attribute	3
598	Oct <*> <*> <*> <*> INFO  metric_base.go <*>  MetricBase .SquashInActivePodMetrics   Squashing all pod metrics that are last updated before  <*> <*> <*>  	8
599	Oct <*> <*> <*> <*> INFO  metric_base.go <*>  MetricBase .SquashInActivePodMetrics   Skipping squashing of pod metrics as no docs found 	7
600	Oct <*> <*> <*> <*> INFO  <*> <*>  MsgBrokerClient <*>   Listening on ally.sysadmin. .timeseries.> 	6
601	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job dataSyncScheduler with an interval of <*>  	9
602	<*> <*> <*> I  STORAGE   initandlisten  No table logging settings modifications are required for existing WiredTiger tables. Logging enabled? <*>	11
603	 Starting EventSource   controller   spectrocluster   source    Type    metadata    creationTimestamp  null   spec    cloudAccountRef  null  clusterConfig    network    networkName      ipPool    nameserver        controlPlaneEndpoint      placement    network    networkName      ipPool    nameserver          machinePoolConfig  null   status    nodeImage       	11
604	 loading tls certificate  path  <*>  key  <*> 	11
605	Started  tokencleaner 	11
606	 Starting certificate watcher 	11
607	Found felix environment variable   ipv6support   false 	11
608	 starting controller   controller   certificates 	11
609	Loading config file  <*>	11
610	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribe   successfully subscribed to ally.sysadmin. .timeseries.> 	5
611	<*> <*> <*> <*> Serving hashboard at https <*>     <*>	4
612	 got pod    namespace   <*> 	341
613	<*> <*> <*> <*>   <*> 0m  updated JWT for account <*> ABCAWDFE3MGJ <*> <*>	5
614	Oct <*> <*> <*> <*> ERROR  hongo_meta_read_repo.go <*>  HongoMetaReadRepo <*>   Unable to get the metadata for the uid  <*>   due to  ResourceNotFound  Metadata Archive    _id    $eq   <*>     is not found	8
615	Starting token cleaner controller	11
616	<*> <*> <*> I  STORAGE   initandlisten  Timestamp monitor starting	22
617	Oct <*> <*> <*> <*> INFO  mgmt_services_factory.go <*>   Initializing Stateful service 	11
618	Merging in config from environment variable  map datastoretype kubernetes defaultendpointtohostaction ACCEPT etcdaddr  etcdcafile  etcdcertfile  etcdendpoints  etcdkeyfile  etcdscheme  felixhostname <*> healthenabled true ipinipmtu <*> ipv6support false logseverityscreen info vxlanmtu <*> wireguardmtu <*> 	11
619	 got podpresets    podpresetlist    	341
620	 Starting NGINX Ingress controller 	11
621	     END logs for container auth of pod <*>     	1
622	Oct <*> <*> <*> <*> INFO  <*> <*>  peerHandler .startPeerSubscribers   Peer subscribe message received  Publisher hubble Key subscribe Data   ServiceName   ally   SubscriberName   ally   AccountUid   sysadmin   UserUid      SubscriberType   TimeseriesSubscriber   WildcardReplacement    .timeseries. <*>   Clone  false  IsGrouped  true  Group      IsNotified  true  IsErr false Timestamp <*>  	4
623	 Starting EventSource   controller   spectrocluster   source    Type    metadata    creationTimestamp  null   spec    clusterConfig    subscriptionId      resourceGroup      location      sshKey      controlPlaneSubnet      workerSubnet        status    vhdImage      images       	11
624	<*> <*> <*> <*>   <*> 0m  updated JWT for account <*> <*> <*> <*>	6
625	data    identifier      _id      $eq     <*>       kind   Metadata Archive    	7
626	<*> <*> <*> I  STORAGE   initandlisten  createCollection  admin.system.version with provided UUID  <*> and options    uuid  UUID  <*>    	11
627	 Starting Controller   controller   spectrocluster 	11
628	Event <*> Kind  ConfigMap   Namespace  <*>   Name  <*>   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   CREATE  ConfigMap <*>	11
629	 got filtered podpresets    filteredpodpreset  null	341
630	Waiting for caches to sync for token_cleaner	11
631	<*> <*> <*> <*>   <*> 0m  updated JWT for account <*> ACOL6MAKHWXV <*> <*>	3
632	     END logs for container nas of pod <*>     	1
633	Ignoring empty configuration parameter. Use value  none  if your intention is to explicitly disable the default value. name  etcdcertfile  source environment variable	11
634	Oct <*> <*> <*> <*> ERROR  planusage_service.go <*>  PlanUsageService .RestoreTenantsDailyResourceUsage   RestoreTenantsDailyResourceUsage  Restoring the usage data for  0  tenants... 	6
635	     END logs for container hashboard of pod <*>     	1
636	Event <*> Kind  ConfigMap   Namespace  <*>   Name  tcp-services   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   CREATE  ConfigMap <*>	11
637	     END logs for container timeseries of pod <*>     	1
638	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  0  subscribers for  MachineSubscriber  	22
639	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerMachineSubscriberSubscriber with an interval of <*>  	11
640	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job spectroClustersHealthUpdate with an interval of <*>  	11
641	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  0  subscribers for  HeartbeatSubscriber  	22
642	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerHeartbeatSubscriberSubscriber with an interval of <*>  	11
643	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job spectroClustersUsageCleanup with an interval of <*>  	11
644	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  0  subscribers for  ClusterActionsSubscriber  	22
645	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerClusterActionsSubscriberSubscriber with an interval of <*>  	11
646	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job spectroClustersStateMonitor with an interval of <*>  	11
647	 Starting Controller    controller   customresourcedefinition 	11
648	 Starting Controller    controller   validatingwebhookconfiguration 	11
649	 Starting workers    controller   validatingwebhookconfiguration   worker count  <*>	11
650	 Extracting CA from Secret resource   resource_kind   ValidatingWebhookConfiguration   resource_name   <*>   resource_namespace      secret   <*> 	11
651	 unable to fetch associated secret   error   Secret   <*>   not found   resource_kind   ValidatingWebhookConfiguration   resource_name   <*>   resource_namespace      secret    Namespace   cert-manager   Name   <*>  	11
652	     END logs for container init-appdomain of pod <*>     	11
653	Caches are synced for token_cleaner	11
654	Oct <*> <*> <*> <*> INFO  mgmt_health_service.go <*>  HealthService .shouldHubbleStop   Current Version   <*> 	11
655	 Starting workers   controller   spectrocluster   worker count  <*>	11
656	 detecting env   spectrocluster    Namespace   <*>   Name   <*>  	132
657	 namespaced env    env  null	341
658	<*> <*> <*> I  INDEX     initandlisten  index build  done building index _id_ on ns admin.system.version	11
659	Oct <*> <*> <*> <*> INFO  role_service.go <*>  RoleService .UpsertRole   Created the system role  Tenant User Admin  	5
660	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job spectroClustersArchive with an interval of <*>  	11
661	 Initializing nats connection  cluster <*>	11
662	Ignoring empty configuration parameter. Use value  none  if your intention is to explicitly disable the default value. name  etcdaddr  source environment variable	11
663	 Adding secret to local store  name  <*> 	11
664	 reconcile packOnly   spectrocluster    Namespace   <*>   Name   <*>  	132
665	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job expiryTokenCleanUpScheduler with an interval of <*>  	11
666	 starting controller   controller   webhook-bootstrap 	11
667	 could not find any ca data in data source for target   resource_kind   ValidatingWebhookConfiguration   resource_name   <*>   resource_namespace    	11
668	 namespaced volumes    volumes  null	681
669	 Successfully fetched Nats configuration  <*>   cluster <*>	11
670	Failed to start service controller  WARNING  no cloud provider provided  services of type LoadBalancer will fail	11
671	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job deleteErrLogsScheduler with an interval of <*>  	11
672	Event <*> Kind  Ingress   Namespace  <*>   Name  <*>   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   Sync  Scheduled for sync	198
673	 reconcilePacks   spectrocluster    Namespace   <*>   Name   <*>  	132
674	 downloading pack   spectrocluster    Namespace   <*>   Name   <*>    pack   <*> 	11
675	 create success   spectrocluster    Namespace   <*>   Name   <*>    <*>   Pack   name   <*> 	11
676	 pack downloaded    spectrocluster    Namespace   <*>   Name   <*>    <*>   <*> 	11
677	 cannot detect environment from namespace  trying cluster env   atop    Namespace   <*>   Name   <*>  	121
678	 pack waiting for condition  ReadyForInstall    atop    Namespace   <*>   Name   <*>    pack   <*> 	11
679	 reconcilePacks done   spectrocluster    Namespace   <*>   Name   <*>  	132
680	 reconcileInstallPriorityForPacks   spectrocluster    Namespace   <*>   Name   <*>  	131
681	 marking addon packs ready for install   spectrocluster    Namespace   <*>   Name   <*>    cluster   <*>   installPriority  0	130
682	 marked addon packs ready for install  requeue after one minute   spectrocluster    Namespace   <*>   Name   <*>    cluster   <*>   installPriority  0	11
683	 reconcileLoadBalancerService   spectrocluster    Namespace   <*>   Name   <*>  	127
684	 reconcile charts   atop    Namespace   <*>   Name   <*>  	110
685	 reconcileLoadBalancerService Done   spectrocluster    Namespace   <*>   Name   <*>  	126
686	 reconcileAPIEndpoint   spectrocluster    Namespace   <*>   Name   <*>  	125
687	 palette lite using api endpoint from <*> configmap   spectrocluster    Namespace   <*>   Name   <*>  	124
688	 Update api endpoints   spectrocluster    Namespace   <*>   Name   <*>    info     host   <*>   port  <*>  	22
689	 reconcileAPIEndpoint Done   spectrocluster    Namespace   <*>   Name   <*>  	123
690	 looking for releases    filter   <*>   namespace   default 	220
691	 waiting for addon packs to be ready   spectrocluster    Namespace   <*>   Name   <*>    cluster   <*>   installPriority  0	22
692	Ignoring empty configuration parameter. Use value  none  if your intention is to explicitly disable the default value. name  etcdkeyfile  source environment variable	11
693	Ignoring empty configuration parameter. Use value  none  if your intention is to explicitly disable the default value. name  etcdendpoints  source environment variable	11
694	<*> <*> <*> I  SHARDING  initandlisten  Marking collection admin.system.version as collection version  <unsharded>	22
695	<*> <*> <*> I  COMMAND   initandlisten  setting featureCompatibilityVersion to <*>	11
696	<*> <*> <*> I  SHARDING  initandlisten  Marking collection <*> as collection version  <unsharded>	77
697	<*> <*> <*> I  STORAGE   initandlisten  Flow Control is enabled on this deployment.	22
698	<*> <*> <*> I  STORAGE   initandlisten  createCollection  local.startup_log with generated UUID  <*> and options    capped  true  size  <*>  	11
699	<*> <*> <*> I  INDEX     initandlisten  index build  done building index _id_ on ns local.startup_log	11
700	<*> <*> <*> I  SHARDING  initandlisten  Marking collection local.startup_log as collection version  <unsharded>	22
701	<*> <*> <*> I  FTDC      initandlisten  Initializing full-time diagnostic data capture with directory  <*> 	22
702	<*> <*> <*> I  SHARDING  LogicalSessionCacheReap  Marking collection config.system.sessions as collection version  <unsharded>	22
703	<*> <*> <*> I  CONTROL   LogicalSessionCacheReap  Sessions collection is not set up  waiting until next sessions reap interval  config.system.sessions does not exist	11
704	<*> <*> <*> I  STORAGE   LogicalSessionCacheRefresh  createCollection  config.system.sessions with provided UUID  <*> and options    uuid  UUID  <*>    	11
705	<*> <*> <*> I  NETWORK   listener  Listening on <*>	44
706	<*> <*> <*> I  NETWORK   listener  waiting for connections on port <*>	22
707	child process started successfully  parent exiting	11
708	<*> <*> <*> I  INDEX     LogicalSessionCacheRefresh  index build  done building index _id_ on ns config.system.sessions	11
709	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> <*>  <*> connection now open 	22
710	<*> <*> <*> I  SHARDING  conn1  Marking collection admin.system.users as collection version  <unsharded>	22
711	<*> <*> <*> I  ACCESS    conn1  note  no users configured in admin.system.users  allowing localhost access	11
712	<*> <*> <*> I  NETWORK   conn1  received client metadata from <*> <*> conn1    application    name   MongoDB Shell     driver    name   MongoDB Internal Client   version   <*>     os    type   Linux   name   Ubuntu   architecture   <*>   version   <*>     	11
713	<*> <*> <*> I  NETWORK   conn1  end connection <*> <*>  0 connections now open 	11
714	<*> <*> <*> I  INDEX     LogicalSessionCacheRefresh  index build  starting on config.system.sessions properties    v  <*>  key    lastUse  <*>    name   lsidTTLIndex   ns   config.system.sessions   expireAfterSeconds  <*>   using method  Hybrid	11
715	Skipping  service 	11
716	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job spectroClustersMachinesMonitor with an interval of <*>  	11
717	Oct <*> <*> <*> <*> INFO  <*> <*>  LockHandler .UnlockExpiryLocks   unlocking starts in a interval of <*> for expired locks 	11
718	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job FileCleanUpScheduler with an interval of <*>  	11
719	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job notificationsArchive with an interval of <*>  	11
720	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job notificationEventsArchive with an interval of <*>  	11
721	 Successfully Reconciled    controller   validatingwebhookconfiguration   request    Namespace      Name   <*>  	11
722	 Successfully fetched Nats credentials  <*>   cluster <*>	11
723	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribeByType   Subscribing  0  subscribers for  <*>  	22
724	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job <*> with an interval of <*>  	11
725	<*> <*> <*> I  INDEX     LogicalSessionCacheRefresh  build may temporarily use up to <*> megabytes of RAM	11
726	 Initializing Nats connection with url  tls <*> <*> <*>   cluster <*>	11
727	 starting controller   controller   challenges 	11
728	<*> <*> <*> I  INDEX     LogicalSessionCacheRefresh  index build  collection scan done. scanned 0 total records in 0 seconds	11
729	<*> <*> <*> <*> not able to find statefulSet  mgmt   in namespace  <*>  due to  <*>  mgmt  not found 	11
730	<*> <*> <*> <*> not able to find statefulset  mgmt   in namespace  <*>  due to  <*>  mgmt  not found 	11
731	Oct <*> <*> <*> <*> WARNING  mgmt_services_factory.go <*>   statefulset mgmt doesnot exists 	11
732	Oct <*> <*> <*> <*> ERROR  mgmt_health_service.go <*>  HealthService .shouldHubbleStop   error finding the configmap <*>  due to configmaps  <*>  not found. stopping the hubble services 	11
733	Panic recovered   panicked while watching packsync.	22
734	PANIC  runtime error  invalid memory address or nil pointer dereference	22
735	Stack Trace 	22
736	<*> <*> <*> <*> Serving hutil at https <*>     <*>	10
737	Oct <*> <*> <*> <*> INFO  notification_service.go <*>  NotificationService .archiveNotifications   Archiving the notifications older than <*> days 	9
738	Oct <*> <*> <*> <*> INFO  notification_repo.go <*>  NotificationRepo .ArchiveNotifications   0 notification s  is archived 	8
739	Oct <*> <*> <*> <*> INFO  notification_service.go <*>  NotificationService <*>   Archiving the notification events older than <*> days 	7
740	Oct <*> <*> <*> <*> INFO  errlog_repo.go <*>  errLogRepo .GetErrLogsGreaterThanAMonth   Finding the errLogs greater than three months 	6
741	Oct <*> <*> <*> <*> INFO  notificationevent_repo.go <*>  NotificationEventRepo <*>   0 notification event s  is archived 	5
742	Oct <*> <*> <*> <*> INFO  file_service.go <*>  FileService .findAllFiles   Finding the files   domainFilters & <nil> <nil> <nil> <nil> <nil> <nil>    	4
743	     END logs for container hutil of pod <*>     	1
744	Ignoring empty configuration parameter. Use value  none  if your intention is to explicitly disable the default value. name  etcdcafile  source environment variable	11
745	Oct <*> <*> <*> <*> INFO  role_service.go <*>  RoleService .UpsertRole   Created the system role  Tenant Team Admin  	4
746	 Initialized Nats connection..   cluster <*>	11
747	<*> <*> <*> I  INDEX     LogicalSessionCacheRefresh  index build  inserted 0 keys from external sorter into index in 0 seconds	11
748	 Starting Controller    controller   apiservice 	11
749	 Starting workers    controller   apiservice   worker count  <*>	11
750	 Successfully Reconciled    controller   apiservice   request    Namespace      Name   <*>  	120
751	Ignoring empty configuration parameter. Use value  none  if your intention is to explicitly disable the default value. name  etcdscheme  source environment variable	11
752	Parsing value for WireguardMTU  <*>  from environment variable 	44
753	Parsed value for WireguardMTU  <*>  from environment variable 	44
754	Parsing value for FelixHostname  <*>  from environment variable 	44
755	Started  attachdetach 	11
756	 got clusterpodpresets    clusterpodpresetlist    	339
757	<*> <*> <*> <*> no error handler specified with the subscriber. going with default error handler	11
758	Starting attach detach controller	11
759	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job spectroMgmtReconcileMigration with an interval of <*>  	11
760	 starting controller   controller   orders 	11
761	<*> <*> <*> I  INDEX     LogicalSessionCacheRefresh  index build  done building index lsidTTLIndex on ns config.system.sessions	11
762	Parsed value for FelixHostname  <*>  from environment variable 	44
763	 got filtered clusterpodpreset    filteredclusterpodpreset  null	338
764	Waiting for caches to sync for attach detach	11
765	Oct <*> <*> <*> <*> INFO  role_service.go <*>  RoleService .UpsertRole   Created the system role  Tenant Role Admin  	3
766	<*> <*> <*> <*> Listening on  <*> 	11
767	runtime.errorString runtime error  invalid memory address or nil pointer dereference	22
768	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #2  <*> connection now open 	11
769	Event <*> Kind  Ingress   Namespace  <*>   Name  optic-ingress-resource   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   Sync  Scheduled for sync	22
770	 cluster env    env  null	337
771	Parsing value for VXLANMTU  <*>  from environment variable 	44
772	Oct <*> <*> <*> <*> INFO  <*> <*>  PackSyncServiceHandler <*>   Syncing packs from the registry  <*>  	11
773	Caches are synced for ClusterRoleAggregator	11
774	 Subscribed to msg channel <*>  cluster <*>	11
775	<*> <*>  <*> 	154
776	 new certificate request controller registered    type   selfsigned 	11
777	<*> <*> <*> I  NETWORK   conn2  received client metadata from <*> <*> conn2    application    name   MongoDB Shell     driver    name   MongoDB Internal Client   version   <*>     os    type   Linux   name   Ubuntu   architecture   <*>   version   <*>     	11
778	 cluster volumes    volumes  null	671
779	Oct <*> <*> <*> <*> INFO  role_service.go <*>  RoleService .UpsertRole   Created the system role  Tenant Cluster Profile Admin  	2
780	Parsed value for VXLANMTU  <*>  from environment variable 	44
781	 Applying rbac rules  cluster <*>	11
782	<*> <*> <*> I  STORAGE   conn2  createCollection  admin.system.users with generated UUID  <*> and options    	11
783	Parsing value for HealthEnabled  true  from environment variable 	44
784	Oct <*> <*> <*> <*> INFO  role_service.go <*>  RoleService .UpsertRole   Created the system role  Tenant Project Admin  	1
785	Caches are synced for bootstrap_signer	11
786	Parsed value for HealthEnabled  true  from environment variable 	44
787	 merged env    merged env  null	334
788	<*> <*> <*> I  SHARDING  ftdc  Marking collection <*> as collection version  <unsharded>	22
789	 >>>>>>> Scheduling log monitor service  cluster <*>	11
790	 new certificate request controller registered    type   vault 	11
791	Oct <*> <*> <*> <*> INFO  spectrocluster_monitor.go <*>  SpectroClusterMonitor .execute   executing cluster monitor task  ReconcileBrokerSubscriptionMonitor  	11
792	Parsing value for DefaultEndpointToHostAction  ACCEPT  from environment variable 	44
793	Oct <*> <*> <*> <*> INFO  spectrocluster_monitor.go <*>  SpectroClusterMonitor .execute   executing cluster monitor task  ReconcileOsPatchMonitor  	11
794	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroClusterService .monitorMachines   Started monitoring the spectro cluster machines 	11
795	Oct <*> <*> <*> <*> INFO  spectrocluster_monitor.go <*>  SpectroClusterMonitor .execute   executing cluster monitor task  ReconcileDeletingClusterMonitor  	11
796	     START logs for container configserver of pod <*>     	11
797	 >>>>>>> Scheduling polling of event s  after every <*> seconds  cluster <*>	11
798	 Starting NGINX process 	11
799	Oct <*> <*> <*> <*> INFO  spectrocluster_monitor.go <*>  SpectroClusterMonitor .execute   executing cluster monitor task  ReconcileImageResolutionMonitor  	11
800	 merged volumes    merged volumes  null	333
801	Caches are synced for endpoint_slice_mirroring	11
802	 found release    releases    	220
803	<*> <*> <*> I  INDEX     conn2  index build  done building index _id_ on ns admin.system.users	11
804	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job syncCloudInstanceTypesData with an interval of <*>  	11
805	Parsed value for DefaultEndpointToHostAction  ACCEPT  from environment variable 	44
806	attempting to acquire leader lease <*>	11
807	 merged volume mounts    merged volume mounts  null	332
808	Oct <*> <*> <*> <*> INFO  spectrocluster_monitor.go <*>  SpectroClusterMonitor .execute   executing cluster monitor task  ReconcileIpAllocationMonitor  	11
809	Oct <*> <*> <*> <*> INFO  version.go <*>   current app version is <*> 	39
810	 Starting Controller    controller   mutatingwebhookconfiguration 	11
811	<*> <*> <*> I  INDEX     conn2  index build  done building index <*> on ns admin.system.users	11
812	 pack has no charts   atop    Namespace   <*>   Name   <*>  	110
813	Parsing value for LogSeverityScreen  info  from environment variable 	44
814	Oct <*> <*> <*> <*> INFO  cluster_compliancescan_scheduler_service.go <*>  ClusterComplianceScanService .InvokeComplianceScanTask   Compliance scan scheduler 	11
815	edit failed with   Operation cannot be fulfilled on <*>  edit   the object has been modified  please apply your changes to the latest version and try again	11
816	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job syncCloudStorageTypesData with an interval of <*>  	11
817	 Starting validation webhook  address   <*>  certPath  <*>  keyPath  <*> 	11
818	Parsed value for LogSeverityScreen  INFO  from environment variable 	44
819	 starting controller   controller   clusterissuers 	11
820	<*> <*> <*> I  COMMAND   conn2  command admin.system.version appName   MongoDB Shell  command  createUser   createUser   root   pwd   xxx   roles      role   root   db   admin       digestPassword  true  writeConcern    w   majority   wtimeout  <*>    lsid    id  UUID  <*>      $db   admin    numYields 0 reslen <*> locks   ParallelBatchWriterMode    acquireCount    r  4      ReplicationStateTransition    acquireCount    w  <*>      Global    acquireCount    r  <*>  w  4      Database    acquireCount    r  <*>  W  4      Collection    acquireCount    r  <*>  w  <*>  W  <*>      Mutex    acquireCount    r  <*>       flowControl   acquireCount  4  timeAcquiringMicros  <*>   storage    protocol op_msg <*>	11
821	 Successfully audited Acquired all resources to Orchestrate SpectroCluster Acquired all resources Normal  cluster <*>	11
822	Oct <*> <*> <*> <*> ERROR  version.go <*>   Unable to find the last app version as no configmap <*> found  due to configmaps  <*>  not found.  	6
823	Oct <*> <*> <*> <*> INFO  init.go <*>   Function  ClusterBackupMigrationUpdate  with applicableVersion  <*>  do not qualify for execution  	5
824	 reconcile manifests   atop    Namespace   <*>   Name   <*>  	110
825	Service  <*>  does not have any active Endpoint.	2156
826	 starting controller   controller   issuers 	11
827	 syncing item   key   <*> 	1586
828	 finished processing work item   key   <*> 	1553
829	 ca secret does not yet exist   error   secret   <*>   not found   resource_kind   Secret   resource_name   <*>   resource_namespace   cert-manager 	11
830	 Starting workers    controller   mutatingwebhookconfiguration   worker count  <*>	11
831	Parsing value for DatastoreType  kubernetes  from environment variable 	44
832	 re-queuing item  due to error processing   error   secret   <*>   not found   key   <*> 	11
833	Oct <*> <*> <*> <*> INFO  cluster_logfetcher_service.go <*>  ClusterLogFetcherService .DeleteLogFetcher   Delete log fetcher scheduler 	11
834	<*> <*> <*> <*> Serving cloud at https <*>     <*>	11
835	Successfully added user   	44
836	admin failed with   Operation cannot be fulfilled on <*>  admin   the object has been modified  please apply your changes to the latest version and try again	11
837	 Scheduling in lite cluster...  cluster <*>	11
838	 generating new private key   resource_kind   Secret   resource_name   <*>   resource_namespace   cert-manager 	22
839	<*> <*> <*> <*> I | etcdserver  start to snapshot  applied  <*>  lastsnap  0 	10
840	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroClusterService .monitorMachines   Completed monitoring the spectro cluster machines 	11
841	Parsed value for DatastoreType  kubernetes  from environment variable 	44
842	 create attach manifest files 	220
843	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAwsInstanceTypeMeta   Storing aws instance type meta for region <*> 	297
844	 user     root  	11
845	 issuing webhook certificate   resource_kind   Secret   resource_name   <*>   resource_namespace   cert-manager 	22
846	Caches are synced for endpoint	11
847	 <*>    items       n  cluster <*>	10
848	Parsing value for Ipv6Support  false  from environment variable 	44
849	 kube apply success   atop    Namespace   <*>   Name   <*>    file   <*> 	110
850	<*> <*> <*> <*> I | etcdserver  saved snapshot at index <*>	9
851	 roles     	44
852	Oct <*> <*> <*> <*> INFO  spectroclusterusage_service.go <*>  SpectroClusterUsageService .cleanupClustersUsage   Cleaning up the usage data of clusters deleted before  <*>  days 	11
853	 Extracting CA from Secret resource   resource_kind   MutatingWebhookConfiguration   resource_name   <*>   resource_namespace      secret   <*> 	9
854	Parsed value for Ipv6Support  false  from environment variable 	44
855	 reconcile service status   atop    Namespace   <*>   Name   <*>  	110
856	 Successfully audited Watching in lite cluster  Normal  cluster <*>	9
857	<*> <*> <*> <*> I | etcdserver  compacted raft log at <*>	8
858	Caches are synced for PV protection	11
859	Oct <*> <*> <*> <*> INFO  spectroclusterusage_service.go <*>  SpectroClusterUsageService .cleanupClustersUsage   Skipping clusters usage clean up task as no deleted clusters found 	11
860	Parsing value for IpInIpMtu  <*>  from environment variable 	44
861	 Managed <*> is not installed  cluster <*>	8
862	 role     root  	11
863	Caches are synced for <*>	33
864	 Un-managed metrics server test... Getting metrics for node <*>  cluster <*>	7
865	 ca certificate already up to date   resource_kind   Secret   resource_name   <*>   resource_namespace   cert-manager 	533
866	Parsed value for IpInIpMtu  <*>  from environment variable 	44
867	Oct <*> <*> <*> <*> INFO  packsync_service.go <*>  PackSyncService <*>   Started fetching the synced packs for the registry   Spectro Registry   <*>   <*> <*> <*>   	11
868	<*> <*> <*> <*> configmap  configserver  is created	11
869	 db     admin 	11
870	 Failed to get metrics for node <*> No un-managed <*> is found  cluster <*>	6
871	 reconciled chart service status for pack    pack   <*> 	110
872	Merging in config from config file  map LogFilePath None LogSeverityFile None LogSeveritySys None MetadataAddr None 	11
873	 unable to fetch associated secret   error   Secret   <*>   not found   resource_kind   MutatingWebhookConfiguration   resource_name   <*>   resource_namespace      secret    Namespace   cert-manager   Name   <*>  	6
874	<*> <*> <*> <*> searching configmaps s  in <*> Namespace	11
875	 Couldn t find any un-managed <*> installed  cluster <*>	5
876	Oct <*> <*> <*> <*> INFO  packsync_service.go <*>  PackSyncService <*>   Found total  0  synced packs for the registry   Spectro Registry   <*>   <*> <*> <*>   	11
877	 could not find any ca data in data source for target   resource_kind   MutatingWebhookConfiguration   resource_name   <*>   resource_namespace    	5
878	Oct <*> <*> <*> <*> INFO  pack_service.go <*>  PackService <*>   Started updating the packs for the registry   Spectro Registry   <*>   <*> <*> <*>  	11
879	<*> <*> <*> <*> deployment  nas  is created	44
880	 fetch process output object complete 	110
881	Caches are synced for disruption	11
882	 not able to get the config map <*> creatig a new config map 	11
883	 <*> n# Source  <*> napiVersion  <*> nkind  ServiceAccount nmetadata  n  name  <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm <*> n# Source  <*> nkind  ClusterRole napiVersion  <*> nmetadata  n  name  system <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm n    <*>    true   n    <*>    true   n    <*>    true   nrules  n  <*> apiGroups     <*>    n    resources     pods     nodes    n    verbs     get      list      watch    <*> n# Source  <*> napiVersion  <*> nkind  ClusterRole nmetadata  n  name  system <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm nrules  n  <*> apiGroups  n    <*>      n    resources  n      <*> pods n      <*> nodes n      <*> <*> n      <*> namespaces n    verbs  n      <*> get n      <*> list n      <*> watch <*> n# Source  <*> napiVersion  <*> nkind  ClusterRoleBinding nmetadata  n  name  <*> system <*> n  namespace  <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm nroleRef  n  apiGroup  <*> n  kind  ClusterRole n  name  system <*> nsubjects  n  <*> kind  ServiceAccount n    name  <*> n    namespace  default <*> n# Source  <*> napiVersion  <*> nkind  ClusterRoleBinding nmetadata  n  name  system <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm nroleRef  n  apiGroup  <*> n  kind  ClusterRole n  name  system <*> nsubjects  n  <*> kind  ServiceAccount n    name  <*> n    namespace  default <*> n# Source  <*> napiVersion  <*> nkind  RoleBinding nmetadata  n  name  <*> n  namespace  <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm nroleRef  n  apiGroup  <*> n  kind  Role n  name  <*> nsubjects  n  <*> kind  ServiceAccount n    name  <*> n    namespace  default <*> n# Source  <*> napiVersion  <*> nkind  Service nmetadata  n  name  <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm n  annotations  n       nspec  n  ports  n    <*> port  <*> n      protocol  TCP n      targetPort  https n  selector  n    app  <*> n    release  <*> n  type  ClusterIP <*> n# Source  <*> napiVersion  <*> nkind  Deployment nmetadata  n  name  <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm nspec  n  selector  n    matchLabels  n      app  <*> n      release  <*> n  replicas  <*> n  template  n    metadata  n      labels  n        app  <*> n        release  <*> n    spec  n      serviceAccountName  <*> n      containers  n        <*> name  <*> n          image    <*> <*>   n          imagePullPolicy  IfNotPresent n          command  n            <*> <*> n            <*> <*> <*> n            <*> <*> n            <*> <*> n            <*> <*> InternalIP n            <*> <*> <*> n          ports  n          <*> containerPort  <*> n            name  https n          livenessProbe  n            httpGet  n              path  <*> n              port  https n              scheme  HTTPS n            initialDelaySeconds  <*> n          readinessProbe  n            httpGet  n              path  <*> n              port  https n              scheme  HTTPS n            initialDelaySeconds  <*> n          resources  n               n          securityContext  n            allowPrivilegeEscalation  false n            capabilities  n              drop  n              <*> all n            readOnlyRootFilesystem  true n            runAsGroup  <*> n            runAsNonRoot  true n            runAsUser  <*> n          volumeMounts  n          <*> name  tmp n            mountPath  <*> n      nodeSelector  n           n      affinity  n           n      tolerations  n        <*> effect  NoSchedule n          key  <*> n        <*> key  CriticalAddonsOnly n          operator  Exists n      volumes  n      <*> name  tmp n        emptyDir     <*> n# Source  <*> napiVersion  <*> nkind  APIService nmetadata  n  name  <*> n  labels  n    app  <*> n    chart  <*> n    release  <*> n    heritage  Helm nspec  n  service  n    name  <*> n    namespace  default n  group  <*> n  version  <*> n  insecureSkipTLSVerify  true n  groupPriorityMinimum  <*> n  versionPriority  <*> n n  cluster <*>	4
884	     END logs for container etcd of pod <*>     	1
885	Skipping confd config file.	11
886	Service  <*>  does not have any active Endpoint for TCP port <*>	154
887	 Configuration changes detected  backend reload required 	22
888	 Backend successfully reloaded 	22
889	 Initial sync  sleeping for <*> second 	11
890	Oct <*> <*> <*> <*> INFO  pack_service.go <*>  PackService <*>   Updated the packs for the registry   Spectro Registry   <*>   <*> <*> <*>  	11
891	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService .InitSelfCluster   installer mode   <*> 	11
892	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService .InitSelfCluster   installer cloud   vsphere 	11
893	Event <*> Kind  Pod   Namespace  <*>   Name  <*>   UID  <*>   APIVersion  <*>   ResourceVersion  <*>   FieldPath       type   Normal  reason   RELOAD  NGINX reload triggered due to a change in configuration	22
894	 Successfully Reconciled    controller   mutatingwebhookconfiguration   request    Namespace      Name   <*>  	3
895	<*> <*> <*> <*> Watching configMap with fields  metadata.name configserver  in <*> Namespace	11
896	 process status output complete 	110
897	 Deleting api service <*>  cluster <*>	3
898	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService .CreateQuickStartCluster   started linking  vsphere  quick start cluster 	11
899	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService .CreateQuickStartCluster   Fetching the  vsphere  installer cluster profile 	11
900	Oct <*> <*> <*> <*> INFO  clusterprofile_service.go <*>  ClusterProfileService .createInstallerProfileIfNotExists   Creating the vsphere installer cluster profile  <*>  	11
901	Oct <*> <*> <*> <*> INFO  roar_service.go <*>  RoarService .getProfile   File path name for cloud vsphere is <*> 	11
902	Oct <*> <*> <*> <*> INFO  roar_service.go <*>  RoarService .getProfile   Profile file path name for cloud vsphere is <*> 	11
903	Oct <*> <*> <*> <*> INFO  clusterprofile_service.go <*>  ClusterProfileService <*>   <*> installer profile created successfully  %!s MISSING   	11
904	Oct <*> <*> <*> <*> INFO  clusterprofile_service.go <*>  ClusterProfileService .createInstallerProfile   Publishing the cluster profile  <*>  	11
905	Oct <*> <*> <*> <*> INFO  clusterprofile_service.go <*>  ClusterProfileService <*>   Skipping publish of notification events as no pack updates for the profile  <*>  	11
906	Oct <*> <*> <*> <*> INFO  spectrocluster_vsphere.go <*>  VsphereCluster .CreateDefaultCloudConfig   Creating the Vsphere default cloud config 	11
907	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAwsInstanceTypeMeta   Updating aws instance type meta for region <*> 	143
908	 serving certificate already up to date   resource_kind   Secret   resource_name   <*>   resource_namespace   cert-manager 	529
909	Setting lastTransitionTime for Issuer  <*>  condition  Ready  to <*> <*> <*> <*> <*> UTC m <*>	11
910	Setting lastTransitionTime for Certificate  <*>  condition  Ready  to <*> <*> <*> <*> <*> UTC m <*>	22
911	 no existing CertificateRequest resource exists  creating new request...   related_resource_kind   Secret   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	11
912	 created certificate request   related_resource_kind   Secret   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*>   request_name   <*> 	11
913	Oct <*> <*> <*> <*> INFO  spectrocluster_vsphere.go <*>  VsphereCluster .CreateDefaultCloudConfig   Vsphere default cloud config <*> created successfully 	11
914	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService <*>   creating the quick start cluster 	11
915	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService <*>   creating the cluster namespace   <*> 	11
916	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService <*>   applying the manifests in cluster namespace   <*> 	11
917	 reconciled manifest service status for pack    pack   <*> 	110
918	 self signed certificate issued   resource_kind   CertificateRequest   resource_name   <*>   resource_namespace   <*> 	11
919	Setting lastTransitionTime for CertificateRequest  <*>  condition  Ready  to <*> <*> <*> <*> <*> UTC m <*>	11
920	 re-queuing item  due to error processing   error   Operation cannot be fulfilled on <*>   <*>    the object has been modified  please apply your changes to the latest version and try again   key   <*> 	11
921	 validating existing CSR data   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	33
922	 CertificateRequest is not in a final state  waiting until CertificateRequest is complete   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*>   state    	22
923	 pack readiness status    pack   <*>   status  true	110
924	Oct <*> <*> <*> <*> INFO  installer_manifest.go <*>  InstallerService .generateAllyManifest   generating ally-lite manifest for the cluster  <*> <*>  	11
925	<*> <*> <*> <*> number of replicas for deployment  nas     <*> 	44
926	Oct <*> <*> <*> <*> INFO  mgmt_services_factory.go <*>   Stateful services initialized successfully 	11
927	Oct <*> <*> <*> <*> INFO  mgmt_service.go <*>  MgmtService .updateSpectroMgmtUpgradeStatus   Updating spectro mgmt upgrade status 	57
928	 Deleted api service <*>  cluster <*>	2
929	 Deleting role binding <*>  cluster <*>	1
930	 New leader <*>  identity  <*> 	11
931	Caches are synced for HPA	10
932	Caches are synced for deployment	9
933	Caches are synced for job	8
934	 Event occurred  object  <*>  kind  Deployment  apiVersion  <*>  type  Normal  reason  <*>  message  Scaled up replica set <*> to <*> 	7
935	Failed to update statusUpdateNeeded field in actual state of world  Failed to set statusUpdateNeeded to needed true  because nodeName  <*>  does not exist	6
936	Caches are synced for GC	5
937	Caches are synced for ReplicaSet	4
938	 Event occurred  object  <*>  kind  ReplicaSet  apiVersion  <*>  type  Normal  reason  SuccessfulCreate  message  Created pod  <*> 	3
939	Caches are synced for endpoint_slice	2
940	Caches are synced for TTL	1
941	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region eastasia 	11
942	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region southeastasia 	11
943	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region centralus 	11
944	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region eastus 	11
945	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region eastus2 	11
946	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region westus 	11
947	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region northcentralus 	11
948	<*> <*> <*> E  <*>         main  Error saving history file  FileOpenFailed  Unable to open   file <*>  Read-only file system	22
949	<*> <*> <*> I  NETWORK   conn2  end connection <*> <*>  0 connections now open 	11
950	<*>  running <*>	11
951	MongoDB shell version <*>	11
952	connecting to  mongodb <*> <*> disabled&gssapiServiceName mongodb	11
953	Oct <*> <*> <*> <*> INFO  installer_manifest.go <*>  InstallerService .getHubbleEndpoint   hubble endpoint   <*> <*> 	11
954	 POD is not ready  pod  <*>  node  <*> 	11
955	Oct <*> <*> <*> <*> INFO  installer_manifest.go <*>  InstallerService .applyManifests   applying  ally-lite <*>  manifest in cluster namespace  <*>  	11
956	Oct <*> <*> <*> <*> INFO  installer_manifest.go <*>  InstallerService .applyManifests   applying  <*> <*>  manifest in cluster namespace  <*>  	11
957	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region southcentralus 	11
958	Parsing value for MetadataAddr  None  from config file 	33
959	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService <*>   creating the spectro mgmt doc 	11
960	Value set to  none   replacing with zero-value    .	132
961	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region northeurope 	11
962	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #3  <*> connection now open 	11
963	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService .cleanup   deleting the namespace  <*>  	11
964	Parsed value for MetadataAddr    from config file 	33
965	<*> <*> <*> I  NETWORK   conn3  received client metadata from <*> <*> conn3    application    name   MongoDB Shell     driver    name   MongoDB Internal Client   version   <*>     os    type   Linux   name   Ubuntu   architecture   <*>   version   <*>     	11
966	<*> <*> <*> <*> Serving mgmt at https <*>     <*>	11
967	Parsing value for LogFilePath  None  from config file 	33
968	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region westeurope 	11
969	<*> <*> <*> <*> removing the finalizers from  <*>  of crd type  packs 	11
970	<*> <*> <*> <*> removing the finalizers from  <*>  of crd type  spectroclusters 	11
971	Oct <*> <*> <*> <*> INFO  <*> <*>  SpectroMgmtService .CreateQuickStartCluster   quick start cluster created successfully   <*> 	11
972	Oct <*> <*> <*> <*> INFO  spectrcluster_enterprise_link.go <*>  SpectroMgmtService .InitEnterpriseLinking   Installer Mode   <*> 	11
973	Oct <*> <*> <*> <*> INFO  spectrcluster_enterprise_link.go <*>  SpectroMgmtService .InitEnterpriseLinking   Installer Cloud   vsphere 	11
974	Oct <*> <*> <*> <*> INFO  spectrcluster_enterprise_link.go <*>  SpectroMgmtService .InitEnterpriseLinking   Skipping enterprise cluster linking in the installer mode  <*>  	11
975	Oct <*> <*> <*> <*> INFO  <*> <*>   Creating cluster default health status for  <*>  	11
976	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription .subscribeMetrics   subscribing cluster  <*>  metrics 	11
977	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription <*>   Subscribing timeseries with nats for the tenant  sysadmin  and cluster  <*>  	11
978	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription .subscribeHeartbeat   subscribing cluster  <*>  heartbeat 	11
979	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription .subscribeClusterActions   subscribing cluster  <*>  actions 	11
980	 reconcileInstallPriorityForPacks done   spectrocluster    Namespace   <*>   Name   <*>  	96
981	 reconcile packOnly done   spectrocluster    Namespace   <*>   Name   <*>  	95
982	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription .subscribeEvent   subscribing cluster  <*>  events 	11
983	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription <*>   Subscribing event with nats for the tenant  sysadmin  and cluster  <*>  	11
984	Oct <*> <*> <*> <*> INFO  spectrocluster_subscription.go <*>  SpectroClusterSubscription .subscribeMachine   subscribing cluster  <*>  machines 	11
985	Oct <*> <*> <*> <*> INFO  <*> <*>   Creating cluster default cost for  <*>  	11
986	Oct <*> <*> <*> <*> INFO  <*> <*>  Nats .pushToNas   Succeeded in updating the account to <*> https <*> <*> with token length <*> 	33
987	Oct <*> <*> <*> <*> INFO  registry_service.go <*>  RegistryService <*>   syncing system registries 	11
988	Oct <*> <*> <*> <*> INFO  <*> <*>  MsgBrokerClient <*>   Listening on ally.sysadmin. <*> 	22
989	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribe   successfully subscribed to ally.sysadmin. <*> 	22
990	Oct <*> <*> <*> <*> INFO  <*> <*>  peerHandler .startPeerSubscribers   Peer subscribe message received  Publisher hubble Key subscribe Data   ServiceName   ally   SubscriberName   ally   AccountUid   sysadmin   UserUid      SubscriberType   ClusterActionsSubscriber   WildcardReplacement    <*> <*>   Clone  false  IsGrouped  true  Group      IsNotified  true  IsErr false Timestamp <*>  	11
991	<*> <*> <*> I  ACCESS    conn3  Successfully authenticated as principal root on admin from client <*> <*>	22
992	Implicit session  session    id    UUID  <*>    	11
993	MongoDB server version  <*>	11
994	switched to db hubbledb	11
995	 CertificateRequest is in a Ready state  issuing certificate...   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	11
996	 decoding certificate data   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	11
997	 checking stored private key is valid for stored <*> certificate on CertificateRequest   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	11
998	 checking if certificate stored on CertificateRequest is up to date   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	11
999	 CertificateRequest contains a valid certificate for issuance. Issuing certificate...   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	11
1000	 user     hubble  	33
1001	Parsed value for LogFilePath    from config file 	33
1002	Parsing value for LogSeverityFile  None  from config file 	33
1003	Parsed value for LogSeverityFile    from config file 	33
1004	Parsing value for LogSeveritySys  None  from config file 	33
1005	 role     readWrite  	33
1006	 db     hubbledb 	11
1007	switched to db hubble_archivedb	11
1008	 certificate scheduled for renewal   duration_until_renewal   <*>   related_resource_kind   Secret   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	33
1009	 certificate does not require re-issuance. certificate renewal scheduled near expiry time.   related_resource_kind   CertificateRequest   related_resource_name   <*>   related_resource_namespace   <*>   resource_kind   Certificate   resource_name   <*>   resource_namespace   <*> 	33
1010	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region <*> 	11
1011	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region eastasia 	11
1012	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region eastasia 	11
1013	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region japaneast 	11
1014	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region brazilsouth 	11
1015	 db     hubble_archivedb 	11
1016	switched to db hubble_timeseriesdb	11
1017	 db     hubble_timeseriesdb 	11
1018	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region southeastasia 	22
1019	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region australiaeast 	11
1020	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region centralus 	22
1021	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region australiasoutheast 	11
1022	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region eastus 	22
1023	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region southindia 	11
1024	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region eastus2 	11
1025	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region eastus2 	11
1026	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region centralindia 	11
1027	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region westus 	11
1028	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region westus 	11
1029	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region westindia 	11
1030	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region northcentralus 	11
1031	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region northcentralus 	11
1032	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region canadacentral 	11
1033	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region southcentralus 	22
1034	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region canadaeast 	11
1035	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region northeurope 	22
1036	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region uksouth 	11
1037	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region westeurope 	11
1038	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region westeurope 	11
1039	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region ukwest 	11
1040	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region <*> 	11
1041	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region <*> 	11
1042	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region westcentralus 	11
1043	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region japaneast 	22
1044	Parsed value for LogSeveritySys    from config file 	33
1045	Health enabled.  Starting server. host  localhost  port <*>	11
1046	<*> disabled  disabling node poll  if KDD is in use .	11
1047	Connecting to datastore datastore  kubernetes 	11
1048	Created datastore client	11
1049	Loaded ready flag kind  ClusterInformation  name  default  ready true	11
1050	No config of this type kind  FelixConfiguration  name  <*> 	11
1051	Skipping API <*> because it has no resources.	88
1052	Found status change for Certificate  <*>  condition  Ready    False  <*>  True   setting lastTransitionTime to <*> <*> <*> <*> <*> UTC m <*>	11
1053	 not syncing ingress resource as it does not contain a   <*>   or   <*>   annotation   resource_kind   Ingress   resource_name   <*>   resource_namespace   <*> 	198
1054	Oct <*> <*> <*> <*> INFO  mgmt_health_service.go <*>   Suspending all the hubble services 	11
1055	Oct <*> <*> <*> <*> ERROR  mgmt_monitor_services.go <*>  MonitorService <*>   Unable to initialise the nats token due to ResourceNotFound  Resource not found for the type  systems 	72
1056	data    params   metadata.name     $eq DefaultSysAdmin      resType   systems  	71
1057	Oct <*> <*> <*> <*> INFO  <*> <*>  peerHandler .startPeerSubscribers   Peer subscribe message received  Publisher hubble Key subscribe Data   ServiceName   ally   SubscriberName   ally   AccountUid   sysadmin   UserUid      SubscriberType   MachineSubscriber   WildcardReplacement    <*> <*>   Clone  false  IsGrouped  true  Group      IsNotified  true  IsErr false Timestamp <*>  	11
1058	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region westus2 	11
1059	Using pod cidr	11
1060	Oct <*> <*> <*> <*> INFO  registry_service.go <*>  RegistryService <*>   creating the helm system registry  Bitnami  	11
1061	Oct <*> <*> <*> <*> INFO  <*> <*>  PackSyncServiceHandler <*>   Syncing the helm charts from the  registry  <*>  	11
1062	Oct <*> <*> <*> <*> INFO  pack_service.go <*>  PackService .CreateManifestPack   Creating the default spectro byo manifest pack 	11
1063	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job scheduleReleaseVersionsSync with an interval of <*>  	11
1064	Oct <*> <*> <*> <*> INFO  packsync_service.go <*>  PackSyncService <*>   Started fetching the synced packs meta for the registry   Bitnami   <*>   <*> <*> <*>   	11
1065	Oct <*> <*> <*> <*> INFO  packsync_service.go <*>  PackSyncService <*>   Found total  0  synced packs meta for the registry   Bitnami   <*>   <*> <*> <*>   	11
1066	Oct <*> <*> <*> <*> INFO  <*> <*>  <*> .GetHelmCharts   Fetching helm charts metadata   https <*> 	11
1067	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job MsgBrokerSpectroClusterSubscribersValidator with an interval of <*>  	11
1068	Oct <*> <*> <*> <*> INFO  scheduler_handler.go <*>   Scheduled job processSpectroClusterNotificationEvents with an interval of <*>  	11
1069	<*> <*> <*> <*> Serving spectrocluster at https <*>     <*>	11
1070	Current CIDRS   <*> 	11
1071	Oct <*> <*> <*> <*> INFO  spectrocluster_mgmt_service.go <*>  SpectroMgmtService .SyncReleaseVersions   syncing spectro release versions from repo 	11
1072	Oct <*> <*> <*> <*> INFO  spectrocluster_mgmt_service.go <*>  SpectroMgmtService .getAppVersions   Installed Spectro version   <*> 	11
1073	Oct <*> <*> <*> <*> INFO  spectrocluster_mgmt_service.go <*>  SpectroMgmtService .getAppVersions   Available Spectro release versions      	11
1074	Oct <*> <*> <*> <*> INFO  <*> <*>  <*> .downloadHelmChart   downloading helm chart   https <*> 	4431
1075	Oct <*> <*> <*> <*> INFO  <*> <*>  MsgBrokerClient <*>   Listening on ally.sysadmin. .heartbeat.> 	11
1076	Oct <*> <*> <*> <*> INFO  nats_subscription.go <*>  Nats .subscribe   successfully subscribed to ally.sysadmin. .heartbeat.> 	11
1077	Oct <*> <*> <*> <*> INFO  <*> <*>  peerHandler .startPeerSubscribers   Peer subscribe message received  Publisher hubble Key subscribe Data   ServiceName   ally   SubscriberName   ally   AccountUid   sysadmin   UserUid      SubscriberType   HeartbeatSubscriber   WildcardReplacement    .heartbeat. <*>   Clone  false  IsGrouped  true  Group      IsNotified  true  IsErr false Timestamp <*>  	11
1078	Starting serving-cert  <*>  <*>	11
1079	Serving securely on      <*>	11
1080	Starting autoregister controller	11
1081	Waiting for caches to sync for autoregister controller	11
1082	Starting OpenAPI AggregationController	11
1083	Starting AvailableConditionController	11
1084	Waiting for caches to sync for AvailableConditionController controller	11
1085	Starting APIServiceRegistrationController	11
1086	Waiting for caches to sync for APIServiceRegistrationController controller	11
1087	Starting OpenAPI controller	11
1088	Starting NamingConditionController	11
1089	Starting EstablishingController	11
1090	Starting KubernetesAPIApprovalPolicyConformantConditionController	11
1091	Unable to remove old endpoints from kubernetes service  StorageError  key not found  Code  <*>  Key  <*>  ResourceVersion  0  AdditionalErrorMsg 	11
1092	Caches are synced for autoregister controller	11
1093	Caches are synced for AvailableConditionController controller	11
1094	Caches are synced for APIServiceRegistrationController controller	11
1095	Old CIDRS    	11
1096	Merging in config from datastore  global   map CalicoVersion <*> ClusterGUID <*> ClusterType k8s bgp kubeadm kdd IpInIpEnabled true LogSeverityScreen Info ReportingIntervalSecs 0 	11
1097	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region brazilsouth 	22
1098	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region koreacentral 	11
1099	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region australiaeast 	22
1100	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region koreasouth 	11
1101	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region australiasoutheast 	22
1102	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region francecentral 	11
1103	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region southindia 	22
1104	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region francesouth 	11
1105	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region centralindia 	11
1106	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region centralindia 	11
1107	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region australiacentral 	11
1108	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region westindia 	22
1109	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region australiacentral2 	11
1110	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region canadacentral 	22
1111	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region uaecentral 	11
1112	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region canadaeast 	11
1113	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region canadaeast 	11
1114	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region uaenorth 	11
1115	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region southafricanorth 	11
1116	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region uksouth 	22
1117	<*> <*> <*> <*> deleted pod <*>	110
1118	Oct <*> <*> <*> <*> INFO  mgmt_health_service.go <*>  HealthService <*>   change only effective for MODIFIED event. found event ADDED 	11
1119	bye	11
1120	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region southafricawest 	11
1121	<*> <*> <*> I  NETWORK   conn3  end connection <*> <*>  0 connections now open 	11
1122	OpenAPI AggregationController  action for item   Nothing  removed from the queue .	11
1123	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region ukwest 	11
1124	OpenAPI AggregationController  action for item <*>  Nothing  removed from the queue .	11
1125	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region ukwest 	11
1126	created PriorityClass <*> with value <*>	22
1127	Updated with new cluster IP CIDRs    	11
1128	 not syncing ingress resource as it does not contain a   <*>   or   <*>   annotation   resource_kind   Ingress   resource_name   optic-ingress-resource   resource_namespace   <*> 	22
1129	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region westcentralus 	22
1130	Updated with new external IP CIDRs    	11
1131	killing process with pid  <*>	11
1132	all system priority classes are created successfully or already exist.	11
1133	Source SourceRouteGenerator readiness changed  ready true	11
1134	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region switzerlandnorth 	11
1135	quota admission added evaluator for  <*>	88
1136	<*> <*> <*> I  CONTROL   signalProcessingThread  got signal <*>  Terminated   will terminate after current cmd ends	11
1137	Parsing value for ClusterType  k8s bgp kubeadm kdd  from datastore  global  	22
1138	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region switzerlandwest 	11
1139	<*> <*> <*> I  REPL      signalProcessingThread  Stepping down the <*> for shutdown  waitTime  <*>	11
1140	<*> <*> <*> <*> unable to find container with name prerequisites in statuses   	143
1141	<*> <*> <*> I  SHARDING  signalProcessingThread  Shutting down the WaitForMajorityService	11
1142	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region westus2 	11
1143	<*> <*> <*> I  CONTROL   signalProcessingThread  Shutting down the LogicalSessionCache	11
1144	Resetting endpoints for master service  kubernetes  to  <*> 	11
1145	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region westus2 	11
1146	<*> <*> <*> I  NETWORK   signalProcessingThread  shutdown  going to close listening sockets...	11
1147	<*> <*> <*> I  NETWORK   listener  removing socket file  <*>	11
1148	<*> <*> <*> I  NETWORK   signalProcessingThread  Shutting down the global connection pool	11
1149	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down the FlowControlTicketholder	11
1150	<*> <*> <*> I  <*>         signalProcessingThread  Stopping further Flow Control ticket acquisitions.	11
1151	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down the PeriodicThreadToAbortExpiredTransactions	11
1152	quota admission added evaluator for  endpoints	11
1153	quota admission added evaluator for  serviceaccounts	11
1154	Oct <*> <*> <*> <*> INFO  mgmt_health_service.go <*>  HealthService <*>   services are not yet suspended re-checking for the state. 	22
1155	Oct <*> <*> <*> <*> INFO  retry.go <*>   retrying  <*>   retrying nas account init  	51
1156	Oct <*> <*> <*> <*> INFO  retry.go <*>   retrying  <*>   Checking spectro mgmt upgrade status  	47
1157	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down the PeriodicThreadToDecreaseSnapshotHistoryIfNotNeeded	11
1158	<*> <*> <*> I  REPL      signalProcessingThread  Shutting down the <*>	11
1159	<*> <*> <*> I  SHARDING  signalProcessingThread  Shutting down the ShardingInitializationMongoD	11
1160	<*> <*> <*> I  REPL      signalProcessingThread  Enqueuing the ReplicationStateTransitionLock for shutdown	11
1161	<*> <*> <*> I  <*>         signalProcessingThread  Killing all operations for shutdown	11
1162	<*> <*> <*> I  COMMAND   signalProcessingThread  Shutting down all open transactions	11
1163	<*> <*> <*> I  REPL      signalProcessingThread  Acquiring the ReplicationStateTransitionLock for shutdown	11
1164	<*> <*> <*> I  INDEX     signalProcessingThread  Shutting down the IndexBuildsCoordinator	11
1165	<*> <*> <*> I  NETWORK   signalProcessingThread  Shutting down the ReplicaSetMonitor	11
1166	<*> <*> <*> I  CONTROL   signalProcessingThread  Shutting down free monitoring	22
1167	<*> <*> <*> I  FTDC      signalProcessingThread  Shutting down full-time data capture	11
1168	<*> <*> <*> I  FTDC      signalProcessingThread  Shutting down full-time diagnostic data capture	11
1169	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down the HealthLog	11
1170	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down the storage engine	11
1171	<*> <*> <*> I  STORAGE   signalProcessingThread  Deregistering all the collections	11
1172	<*> <*> <*> I  STORAGE   signalProcessingThread  Timestamp monitor shutting down	11
1173	<*> <*> <*> I  STORAGE   signalProcessingThread  WiredTigerKVEngine shutting down	11
1174	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down session sweeper thread	11
1175	Parsed value for ClusterType  k8s bgp kubeadm kdd  from datastore  global  	22
1176	<*> <*> <*> I  STORAGE   signalProcessingThread  Finished shutting down session sweeper thread	11
1177	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down journal flusher thread	11
1178	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region germanynorth 	11
1179	<*> <*> <*> I  STORAGE   signalProcessingThread  Finished shutting down journal flusher thread	11
1180	error building openapi models for <*>  ERROR <*> has invalid property  anyOf	7
1181	ERROR <*> has invalid property  anyOf	6
1182	<*> <*> <*> I  STORAGE   signalProcessingThread  Shutting down checkpoint thread	11
1183	<*> <*> <*> I  STORAGE   signalProcessingThread  Finished shutting down checkpoint thread	11
1184	<*> <*> <*> I  STORAGE   signalProcessingThread  shutdown  removing fs lock...	11
1185	<*> <*> <*> I  <*>         signalProcessingThread  Dropping the scope cache for shutdown	11
1186	<*> <*> <*> I  CONTROL   signalProcessingThread  now exiting	11
1187	<*> <*> <*> I  CONTROL   signalProcessingThread  shutting down with code 0	11
1188	MongoDB init process complete  ready for start up.	11
1189	Parsing value for CalicoVersion  <*>  from datastore  global  	22
1190	Parsed value for CalicoVersion  <*>  from datastore  global  	22
1191	Parsing value for LogSeverityScreen  Info  from datastore  global  	22
1192	Parsed value for LogSeverityScreen  INFO  from datastore  global  	22
1193	Skipping config value for LogSeverityScreen from datastore  global   already have a value from environment variable	22
1194	Parsing value for IpInIpEnabled  true  from datastore  global  	22
1195	Parsed value for IpInIpEnabled  true  from datastore  global  	22
1196	Parsing value for ReportingIntervalSecs  0  from datastore  global  	22
1197	Parsed value for ReportingIntervalSecs  0s  from datastore  global  	22
1198	Parsing value for ClusterGUID  <*>  from datastore  global  	22
1199	Parsed value for ClusterGUID  <*>  from datastore  global  	22
1200	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region koreacentral 	11
1201	<*> <*> <*> I  CONTROL   initandlisten  options    net    bindIp   <*>     replication    replSet   rs0     security    authorization   enabled   keyFile   <*>     storage    dbPath   <*>     	11
1202	<*> <*> <*> I  STORAGE   initandlisten  Detected data files in <*> created by the  wiredTiger  storage engine  so setting the active storage engine to  wiredTiger .	11
1203	<*> <*> <*> I  STORAGE   initandlisten  WiredTiger message  <*> <*>  <*> <*>   txn-recover  Recovering log <*> through <*>	44
1204	<*> <*> <*> I  STORAGE   initandlisten  WiredTiger message  <*> <*>  <*> <*>   txn-recover  Main recovery loop  starting at <*> to <*>	11
1205	<*> <*> <*> I  STORAGE   initandlisten  Modifying the table logging settings for all existing WiredTiger tables. Logging enabled? 0	11
1206	Merging in config from datastore  per-host   map IpInIpTunnelAddr <*> 	11
1207	<*> <*> <*> I  STORAGE   initandlisten  createCollection  <*> with generated UUID  <*> and options    	44
1208	<*> <*> <*> I  INDEX     initandlisten  index build  done building index _id_ on ns <*>	44
1209	<*> <*> <*> I  REPL      initandlisten  Did not find local initialized voted for document at startup.	11
1210	<*> <*> <*> I  REPL      initandlisten  Did not find local Rollback ID document at startup. Creating one.	11
1211	<*> <*> <*> I  REPL      initandlisten  Initialized the rollback ID to <*>	11
1212	<*> <*> <*> I  REPL      initandlisten  Did not find local replica set configuration document at startup   NoMatchingDocument  Did not find replica set configuration document in <*>	11
1213	<*> <*> <*> I  CONTROL   LogicalSessionCacheRefresh  Sessions collection is not set up  waiting until next sessions refresh interval  Replication has not yet been configured	11
1214	<*> <*> <*> I  CONTROL   LogicalSessionCacheReap  Failed to reap transaction table  NotYetInitialized  Replication has not yet been configured	11
1215	Parsing value for IpInIpTunnelAddr  <*>  from datastore  per-host  	11
1216	Parsed value for IpInIpTunnelAddr  <*>  from datastore  per-host  	11
1217	     END logs for container manager of pod <*>     	2
1218	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region koreacentral 	11
1219	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region germanywestcentral 	11
1220	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region koreasouth 	22
1221	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region norwaywest 	11
1222	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region francecentral 	22
1223	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region francesouth 	22
1224	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAzureStorageTypeMeta   Storing azure Storage type meta for region norwayeast 	11
1225	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region australiacentral 	22
1226	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateGcpStorageTypeMeta   Storing gcp Storage type meta for region <*> 	264
1227	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region australiacentral2 	22
1228	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateGcpStorageTypeMeta   Storing gcp Storage type meta for region asia-east2 	11
1229	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region uaecentral 	11
1230	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region uaecentral 	11
1231	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region uaenorth 	22
1232	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateGcpStorageTypeMeta   Storing gcp Storage type meta for region asia-northeast2 	11
1233	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region southafricanorth 	22
1234	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateGcpStorageTypeMeta   Storing gcp Storage type meta for region asia-northeast3 	11
1235	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region southafricawest 	22
1236	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region switzerlandnorth 	22
1237	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateGcpStorageTypeMeta   Storing gcp Storage type meta for region asia-southeast2 	11
1238	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region switzerlandwest 	22
1239	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region germanynorth 	22
1240	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region germanywestcentral 	22
1241	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region norwaywest 	11
1242	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Updating azure instance type meta for region norwaywest 	11
1243	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateAzureInstanceTypeMeta   Storing azure instance type meta for region norwayeast 	22
1244	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Storing gcp instance type meta for region <*> 	407
1245	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Updating gcp instance type meta for region <*> 	110
1246	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Storing gcp instance type meta for region asia-east2 	22
1247	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Storing gcp instance type meta for region asia-northeast2 	22
1248	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Storing gcp instance type meta for region asia-northeast3 	11
1249	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Updating gcp instance type meta for region asia-northeast3 	11
1250	Oct <*> <*> <*> <*> INFO  cloudinstancetypemeta_repo.go <*>  MongoCloudInstanceTypeMetaRepo .UpdateGcpInstanceTypeMeta   Storing gcp instance type meta for region asia-southeast2 	22
1251	Recompute BGP peerings  HostBGPConfig node <*>  name ip_addr_v4  updated  HostBGPConfig node <*>  name ip_addr_v6  updated  HostBGPConfig node <*>  name network_v4  updated  HostBGPConfig node <*>  name rr_cluster_id  updated  BlockAffinityKey cidr <*>  host <*>  updated  <*> updated	11
1252	Using Kubernetes datastore driver  sharing Kubernetes client with datastore driver.	11
1253	Server Version  version.Info Major  <*>   Minor  <*>   GitVersion  <*>   GitCommit  <*>   GitTreeState  clean   BuildDate  <*> <*> <*>   GoVersion  <*>   Compiler  gc   Platform  linux/amd64  	11
1254	Health of component changed lastReport health.HealthReport Live true  Ready false  name  felix-startup  newReport &health.HealthReport Live true  Ready true 	11
1255	Health enabled.  Server is already running. host  localhost  port <*>	11
1256	Successfully loaded configuration. GOMAXPROCS 4 builddate  <*>  config &config.Config UseInternalDataplaneDriver true  DataplaneDriver  <*>   WireguardEnabled false  WireguardListeningPort <*>  WireguardRoutingRulePriority <*>  WireguardInterfaceName  wireguard.cali   WireguardMTU <*>  BPFEnabled false  BPFDisableUnprivileged true  BPFLogLevel  off   BPFDataIfacePattern   regexp.Regexp  <*>   BPFConnectTimeLoadBalancingEnabled true  BPFExternalServiceMode  tunnel   BPFKubeProxyIptablesCleanupEnabled true  BPFKubeProxyMinSyncPeriod <*>  BPFKubeProxyEndpointSlicesEnabled false  DebugBPFCgroupV2     DebugBPFMapRepinEnabled true  DatastoreType  kubernetes   FelixHostname  <*>   EtcdAddr  <*> <*>   EtcdScheme  http   EtcdKeyFile     EtcdCertFile     EtcdCaFile     EtcdEndpoints   string nil   TyphaAddr     TyphaK8sServiceName     TyphaK8sNamespace  <*>   TyphaReadTimeout <*>  TyphaWriteTimeout <*>  TyphaKeyFile     TyphaCertFile     TyphaCAFile     TyphaCN     TyphaURISAN     Ipv6Support false  IptablesBackend  legacy   RouteRefreshInterval <*>  InterfaceRefreshInterval <*>  DeviceRouteSourceAddress net.IP nil   DeviceRouteProtocol <*>  RemoveExternalRoutes true  IptablesRefreshInterval <*>  IptablesPostWriteCheckIntervalSecs <*>  IptablesLockFilePath  <*>   IptablesLockTimeoutSecs 0  IptablesLockProbeIntervalMillis <*>  FeatureDetectOverride map string string nil   IpsetsRefreshInterval <*>  MaxIpsetSize <*>  XDPRefreshInterval <*>  PolicySyncPathPrefix     NetlinkTimeoutSecs <*>  MetadataAddr     MetadataPort <*>  OpenstackRegion     InterfacePrefix  cali   InterfaceExclude    regexp.Regexp   regexp.Regexp  <*>    ChainInsertMode  insert   DefaultEndpointToHostAction  ACCEPT   IptablesFilterAllowAction  ACCEPT   IptablesMangleAllowAction  ACCEPT   LogPrefix  <*>   LogFilePath     LogSeverityFile     LogSeverityScreen  INFO   LogSeveritySys     VXLANEnabled false  VXLANPort <*>  VXLANVNI <*>  VXLANMTU <*>  IPv4VXLANTunnelAddr net.IP nil   VXLANTunnelMACAddr     IpInIpEnabled true  IpInIpMtu <*>  IpInIpTunnelAddr net.IP 0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0xff  0xff  0xc0  <*>  0x0  <*>   AllowVXLANPacketsFromWorkloads false  AllowIPIPPacketsFromWorkloads false  AWSSrcDstCheck  DoNothing   ReportingIntervalSecs 0  ReportingTTLSecs <*>  EndpointReportingEnabled false  EndpointReportingDelaySecs <*>  IptablesMarkMask <*>  DisableConntrackInvalidCheck false  HealthEnabled true  HealthPort <*>  HealthHost  localhost   PrometheusMetricsEnabled false  PrometheusMetricsHost     PrometheusMetricsPort <*>  PrometheusGoMetricsEnabled true  PrometheusProcessMetricsEnabled true  FailsafeInboundHostPorts   config.ProtoPort config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>    FailsafeOutboundHostPorts   config.ProtoPort config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>    KubeNodePortRanges   numorstring.Port numorstring.Port MinPort <*>  MaxPort <*>  PortName       NATPortRange numorstring.Port MinPort 0x0  MaxPort 0x0  PortName      NATOutgoingAddress net.IP nil   UsageReportingEnabled true  UsageReportingInitialDelaySecs <*>  UsageReportingIntervalSecs <*>  ClusterGUID  <*>   ClusterType  k8s bgp kubeadm kdd   CalicoVersion  <*>   ExternalNodesCIDRList   string nil   DebugMemoryProfilePath     DebugCPUProfilePath  <*>   DebugDisableLogDropping false  DebugSimulateCalcGraphHangAfter 0  DebugSimulateDataplaneHangAfter 0  DebugPanicAfter 0  DebugSimulateDataRace false  RouteSource  CalicoIPAM   RouteTableRange idalloc.IndexRange Min <*>  Max <*>   IptablesNATOutgoingInterfaceFilter     SidecarAccelerationEnabled false  XDPEnabled true  GenericXDPEnabled false  Variant  Calico   internalOverrides map string string    sourceToRawConfig map config.Source map string string <*> map string string  CalicoVersion   <*>    ClusterGUID   <*>    ClusterType   k8s bgp kubeadm kdd    IpInIpEnabled   true    LogSeverityScreen   Info    ReportingIntervalSecs   0    <*> map string string  IpInIpTunnelAddr   <*>    <*> map string string  LogFilePath   None    LogSeverityFile   None    LogSeveritySys   None    MetadataAddr   None    <*> map string string  datastoretype   kubernetes    defaultendpointtohostaction   ACCEPT    felixhostname   <*>    healthenabled   true    ipinipmtu   <*>    ipv6support   false    logseverityscreen   info    vxlanmtu   <*>    wireguardmtu   <*>     rawValues map string string  CalicoVersion   <*>    ClusterGUID   <*>    ClusterType   k8s bgp kubeadm kdd    DatastoreType   kubernetes    DefaultEndpointToHostAction   ACCEPT    FelixHostname   <*>    HealthEnabled   true    IpInIpEnabled   true    IpInIpMtu   <*>    IpInIpTunnelAddr   <*>    Ipv6Support   false    LogFilePath   None    LogSeverityFile   None    LogSeverityScreen   info    LogSeveritySys   None    MetadataAddr   None    ReportingIntervalSecs   0    VXLANMTU   <*>    WireguardMTU   <*>    Err error nil   loadClientConfigFromEnvironment  func     apiconfig.CalicoAPIConfig  error   <*>   useNodeResourceUpdates false  gitcommit  <*> <*> <*>  version  <*> 	11
1257	<*> <*> <*> I  NETWORK   conn1  received client metadata from <*> <*> conn1    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1258	<*> <*> <*> I  ACCESS    conn1  Successfully authenticated as principal root on admin from client <*> <*>	11
1259	 updating Ingress status  namespace  <*>  ingress  <*>  currentValue    newValue   IP <*> Hostname  Ports     	99
1260	 updating Ingress status  namespace  <*>  ingress  optic-ingress-resource  currentValue    newValue   IP <*> Hostname  Ports     	11
1261	Throttling request took <*>  request  PUT https <*> <*>	11
1262	Using internal  linux  dataplane driver.	11
1263	Calculated iptables mark bits acceptMark <*> endpointMark <*> endpointMarkNonCali <*> passMark <*> scratch0Mark <*> scratch1Mark <*>	11
1264	Creating internal dataplane driver. config intdataplane.Config Hostname  <*>   IPv6Enabled false  RuleRendererOverride rules.RuleRenderer nil   IPIPMTU <*>  VXLANMTU <*>  MaxIPSetSize <*>  IptablesBackend  legacy   IPSetsRefreshInterval <*>  RouteRefreshInterval <*>  DeviceRouteSourceAddress net.IP nil   DeviceRouteProtocol <*>  RemoveExternalRoutes true  IptablesRefreshInterval <*>  IptablesPostWriteCheckInterval <*>  IptablesInsertMode  insert   IptablesLockFilePath  <*>   IptablesLockTimeout 0  IptablesLockProbeInterval <*>  XDPRefreshInterval <*>  Wireguard wireguard.Config Enabled false  ListeningPort <*>  FirewallMark 0  RoutingRulePriority <*>  RoutingTableIndex <*>  InterfaceName  wireguard.cali   MTU <*>   NetlinkTimeout <*>  RulesConfig rules.Config IPSetConfigV4   ipsets.IPVersionConfig  <*>   IPSetConfigV6   ipsets.IPVersionConfig  <*>   WorkloadIfacePrefixes   string  cali    IptablesMarkAccept <*>  IptablesMarkPass <*>  IptablesMarkScratch0 <*>  IptablesMarkScratch1 <*>  IptablesMarkEndpoint <*>  IptablesMarkNonCaliEndpoint <*>  KubeNodePortRanges   numorstring.Port numorstring.Port MinPort <*>  MaxPort <*>  PortName       KubeIPVSSupportEnabled false  OpenStackMetadataIP net.IP nil   OpenStackMetadataPort <*>  OpenStackSpecialCasesEnabled false  VXLANEnabled false  VXLANPort <*>  VXLANVNI <*>  IPIPEnabled true  IPIPTunnelAddress net.IP 0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0xff  0xff  0xc0  <*>  0x0  <*>   VXLANTunnelAddress net.IP nil   AllowVXLANPacketsFromWorkloads false  AllowIPIPPacketsFromWorkloads false  WireguardEnabled false  WireguardInterfaceName  wireguard.cali   IptablesLogPrefix  <*>   EndpointToHostAction  ACCEPT   IptablesFilterAllowAction  ACCEPT   IptablesMangleAllowAction  ACCEPT   FailsafeInboundHostPorts   config.ProtoPort config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>    FailsafeOutboundHostPorts   config.ProtoPort config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>    DisableConntrackInvalid false  NATPortRange numorstring.Port MinPort 0x0  MaxPort 0x0  PortName      IptablesNATOutgoingInterfaceFilter     NATOutgoingAddress net.IP nil   BPFEnabled false   IfaceMonitorConfig ifacemonitor.Config InterfaceExcludes    regexp.Regexp   regexp.Regexp  <*>    ResyncInterval <*>   StatusReportingInterval 0  ConfigChangedRestartCallback  func    <*>   PostInSyncCallback  func    <*>   HealthAggregator   <*>  <*>   RouteTableManager   <*>  <*>   DebugSimulateDataplaneHangAfter 0  ExternalNodesCidrs   string nil   BPFEnabled false  BPFDisableUnprivileged true  BPFKubeProxyIptablesCleanupEnabled true  BPFLogLevel  off   BPFDataIfacePattern   regexp.Regexp  <*>   XDPEnabled true  XDPAllowGeneric false  BPFConntrackTimeouts conntrack.Timeouts CreationGracePeriod <*>  TCPPreEstablished <*>  TCPEstablished <*>  TCPFinsSeen <*>  TCPResetSeen <*>  UDPLastSeen <*>  ICMPLastSeen <*>   BPFCgroupV2     BPFConnTimeLBEnabled true  BPFMapRepin true  BPFNodePortDSREnabled false  KubeProxyMinSyncPeriod <*>  KubeProxyEndpointSlicesEnabled false  SidecarAccelerationEnabled false  LookPathOverride  func string   string  error   nil   KubeClientSet   <*>  <*>   FeatureDetectOverrides map string string nil  	11
1265	Oct <*> <*> <*> <*> INFO  retry.go <*>   retrying  <*>   checking for the wait state  	11
1266	Creating rule renderer. config rules.Config IPSetConfigV4   ipsets.IPVersionConfig  <*>   IPSetConfigV6   ipsets.IPVersionConfig  <*>   WorkloadIfacePrefixes   string  cali    IptablesMarkAccept <*>  IptablesMarkPass <*>  IptablesMarkScratch0 <*>  IptablesMarkScratch1 <*>  IptablesMarkEndpoint <*>  IptablesMarkNonCaliEndpoint <*>  KubeNodePortRanges   numorstring.Port numorstring.Port MinPort <*>  MaxPort <*>  PortName       KubeIPVSSupportEnabled false  OpenStackMetadataIP net.IP nil   OpenStackMetadataPort <*>  OpenStackSpecialCasesEnabled false  VXLANEnabled false  VXLANPort <*>  VXLANVNI <*>  IPIPEnabled true  IPIPTunnelAddress net.IP 0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0x0  0xff  0xff  0xc0  <*>  0x0  <*>   VXLANTunnelAddress net.IP nil   AllowVXLANPacketsFromWorkloads false  AllowIPIPPacketsFromWorkloads false  WireguardEnabled false  WireguardInterfaceName  wireguard.cali   IptablesLogPrefix  <*>   EndpointToHostAction  ACCEPT   IptablesFilterAllowAction  ACCEPT   IptablesMangleAllowAction  ACCEPT   FailsafeInboundHostPorts   config.ProtoPort config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>    FailsafeOutboundHostPorts   config.ProtoPort config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  udp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>   config.ProtoPort Protocol  tcp   Port <*>    DisableConntrackInvalid false  NATPortRange numorstring.Port MinPort 0x0  MaxPort 0x0  PortName      IptablesNATOutgoingInterfaceFilter     NATOutgoingAddress net.IP nil   BPFEnabled false 	11
1267	Workload to host packets will be accepted.	11
1268	filter table allowed packets will be accepted immediately.	11
1269	mangle table allowed packets will be accepted immediately.	11
1270	configured to periodically rescan interfaces. interval <*>	11
1271	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*>	220
1272	Looked up iptables command backendMode  legacy  candidates   string  <*>    <*>   command  <*>  ipVersion <*> <*>  <*> 	61
1273	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> <*>	22
1274	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> <*>	550
1275	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*>	33
1276	Updating detected iptables features features iptables.Features SNATFullyRandom true  MASQFullyRandom true  RestoreSupportsLock true  iptablesVersion <*> kernelVersion <*>	11
1277	Calculated old-insert detection regex. pattern   ? <*>  cali-| ? <*>  <*> ? <*>  <*> ? <*>  <*> ? <*>  <*> ? <*>  <*> ? <*>  calipo-| ? <*>  felix- 	31
1278	Looked up iptables command backendMode  legacy  candidates   string  <*>    iptables-restore   command  <*>  ipVersion <*> <*>  restore 	41
1279	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #2  <*> connections now open 	11
1280	<*> <*> <*> I  NETWORK   conn2  received client metadata from <*> <*> conn2    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1281	Calculated old-insert detection regex. pattern   ? <*>  cali-| ? <*>  <*> ? <*>  <*> ? <*>  <*> ? <*>  <*> ? <*>  <*> ? <*>  calipo-| ? <*>  felix-|-A POSTROUTING .  <*> . <*> POSTROUTING <*> tunl0 <*> addrtype ! <*> LOCAL <*> <*> addrtype <*> LOCAL <*> MASQUERADE 	11
1282	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAwsStorageTypeMeta   Storing aws Storage type meta for region <*> 	213
1283	<*> <*> <*> I  ACCESS    conn2  Successfully authenticated as principal root on admin from client <*> <*>	11
1284	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #3  <*> connections now open 	11
1285	<*> <*> <*> I  NETWORK   conn3  received client metadata from <*> <*> conn3    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1286	<*> <*> <*> I  NETWORK   conn3  end connection <*> <*>  <*> connections now open 	11
1287	<*> <*> <*> I  COMMAND   conn2  initiate   no configuration specified. Using a default configuration for the set	11
1288	<*> <*> <*> I  COMMAND   conn2  created this configuration for initiation     _id   rs0   version  <*>  members      _id  0  host   <*> <*>       	11
1289	<*> <*> <*> I  REPL      conn2  replSetInitiate admin command received from client	11
1290	<*> <*> <*> I  REPL      conn2  replSetInitiate config object with <*> members parses ok	11
1291	<*> <*> <*> I  REPL      conn2        	22
1292	<*> <*> <*> I  REPL      conn2  creating replication oplog of size  <*>	11
1293	<*> <*> <*> I  STORAGE   conn2  createCollection  <*> with generated UUID  <*> and options    capped  true  size  <*>  autoIndexId  false  	11
1294	<*> <*> <*> I  STORAGE   conn2  Starting OplogTruncaterThread <*>	11
1295	<*> <*> <*> I  STORAGE   conn2  The size storer reports that the oplog contains 0 records totaling to 0 bytes	11
1296	<*> <*> <*> I  STORAGE   conn2  Scanning the oplog to determine where to place markers for truncation	11
1297	<*> <*> <*> I  STORAGE   conn2  WiredTiger record store oplog processing took <*>	11
1298	Oct <*> <*> <*> <*> INFO  logrus.go <*>  logrusLogger <*>   Storing gcp instance type meta for region <*> 	11
1299	Can t enable XDP acceleration. error kernel is too old  have  <*> but want at least  <*> 	6
1300	Oct <*> <*> <*> <*> INFO  cloudstoragetypemeta_repo.go <*>  MongoCloudStorageTypeMetaRepo .UpdateAwsStorageTypeMeta   Updating aws Storage type meta for region <*> 	211
1301	Oct <*> <*> <*> <*> WARNING  cache_service.go <*>  CacheService .ResourceEvict   unable to resource cache evict due to empty kind for the uid  <*>  	208
1302	<*> <*> <*> I  STORAGE   conn2  createCollection  <*> with generated UUID  <*> and options    	11
1303	<*> <*> <*> I  INDEX     conn2  index build  done building index _id_ on ns <*>	11
1304	<*> <*> <*> I  REPL      conn2  New replica set config in use    _id   rs0   version  <*>  protocolVersion  <*>  writeConcernMajorityJournalDefault  true  members      _id  0  host   <*> <*>   arbiterOnly  false  buildIndexes  true  hidden  false  priority  <*>  tags      slaveDelay  0  votes  <*>      settings    chainingAllowed  true  heartbeatIntervalMillis  <*>  heartbeatTimeoutSecs  <*>  electionTimeoutMillis  <*>  catchUpTimeoutMillis  <*>  catchUpTakeoverDelayMillis  <*>  getLastErrorModes      getLastErrorDefaults    w  <*>  wtimeout  0    replicaSetId  ObjectId  <*>      	22
1305	<*> <*> <*> I  REPL      conn2  This node is <*> <*> in the config	22
1306	<*> <*> <*> I  REPL      conn2  transition to STARTUP2 from STARTUP	11
1307	<*> <*> <*> I  REPL      conn2  Starting replication storage threads	11
1308	<*> <*> <*> I  REPL      conn2  transition to RECOVERING from STARTUP2	11
1309	<*> <*> <*> I  REPL      conn2  Starting replication fetcher thread	11
1310	Source SourceSyncer readiness changed  ready true	1
1311	Oct <*> <*> <*> <*> INFO  vsphere_service.go <*>  VSphereService .ValidateVsphereAccount   reqId <*>   validating vsphere account service   orgId  projectUid  sessionId <*> tenantUid  userUid sysadmin  	3
1312	     END logs for container cloud of pod <*>     	1
1313	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	22
1314	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*>	22
1315	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> profile%2Csystem&resolvePackValues true <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*>	22
1316	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*> <*>  <*> 0  <*>   <*>  <*> <*>  <*>     <*> <*> 0 <*> <*> <*> <*> jet	623
1317	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> profile%2Csystem&resolvePackValues true <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> ally	227
1318	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> ally	378
1319	     END logs for container cert-manager of pod <*>     	1
1320	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> ally	33
1321	<*> <*> <*>  <*> <*> <*> <*> <*>   PATCH <*> <*>  <*> 0  <*>   <*>  <*> <*>  <*>     <*> <*> 0 <*> <*> <*> <*> ally	44
1322	<*> <*> <*>  <*> <*> <*> <*> <*>   PUT <*> <*>  <*> 0  <*>   <*>  <*> <*>  <*>     <*> <*> 0 <*> <*> <*> <*> ally	44
1323	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> ally	63
1324	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	11
1325	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	11
1326	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*> <*> false   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> <*>	44
1327	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*> <*> false   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	22
1328	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> ftp <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> <*>	22
1329	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> s3 <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> <*>	11
1330	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> false <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	11
1331	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> sysadmin	490
1332	<*> <*> <*>  <*> <*> <*> <*> <*>   PATCH <*> <*>  <*> 0  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> 0 <*> <*> <*> <*> sysadmin	11
1333	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> ftp <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> sysadmin	22
1334	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> s3 <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> sysadmin	11
1335	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> false <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	22
1336	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> jet	22
1337	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> <*>  <*> 0  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> 0 <*> <*> <*> <*> sysadmin	11
1338	<*> <*> <*>  <*> <*> <*> <*> <*>   POST <*> <*>  <*> <*>  https <*>   <*>  Windows NT <*>  <*>  <*>  <*>  KHTML  like Gecko  <*> <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> sysadmin	66
1339	Oct <*> <*> <*> <*> ERROR  hongo_meta_read_repo.go <*>  HongoMetaReadRepo .GetMetaArchive   Unable to get the metadata from the archive  metadata.name   due to  ResourceNotFound  Metadata Archive    $and     metadata.uid    $eq   sysadmin      kind    $eq   user       is not found	6
1340	data    identifier      $and       metadata.uid      $eq     sysadmin        kind      $eq     user         kind   Metadata Archive    	5
1341	Oct <*> <*> <*> <*> INFO  userasset_read_repo.go <*>  MongoUserAssetReadRepo .findAllAssets   Getting the assets of type clusterrbac 	2
1342	<*> <*> <*>  <*> <*> <*> <*> <*>   GET <*> true <*>  <*> <*>  <*>   <*>  <*> <*>  <*>     <*> <*> <*> <*> <*> <*> <*> requester missing	22
1343	<*> <*> <*> I  REPL      conn2  Starting replication applier thread	11
1344	<*> <*> <*> I  REPL      conn2  Starting replication reporter thread	11
1345	<*> <*> <*> I  REPL      <*>  Starting oplog application	11
1346	<*> <*> <*> I  COMMAND   conn2  command <*> command  replSetInitiate   replSetInitiate      $db   admin    numYields 0 reslen <*> locks   ParallelBatchWriterMode    acquireCount    r  <*>      ReplicationStateTransition    acquireCount    w  <*>      Global    acquireCount    r  <*>  w  <*>  W  <*>    acquireWaitCount    W  <*>    timeAcquiringMicros    W  <*>      Database    acquireCount    r  <*>  w  <*>  W  <*>      Collection    acquireCount    r  <*>  w  <*>      Mutex    acquireCount    r  <*>      oplog    acquireCount    r  <*>  w  <*>       flowControl   acquireCount  <*>   storage    protocol op_query <*>	11
1347	<*> <*> <*> I  REPL      <*>  transition to SECONDARY from RECOVERING	11
1348	<*> <*> <*> I  ELECTION  <*>  conducting a dry run election to see if we could be <*> current term  0	11
1349	<*> <*> <*> I  ELECTION  <*>  dry election run succeeded  running for election in term <*>	11
1350	<*> <*> <*> I  ELECTION  <*>  election succeeded  assuming primary role in term <*>	11
1351	<*> <*> <*> I  REPL      <*>  transition to PRIMARY from SECONDARY	11
1352	<*> <*> <*> I  REPL      <*>  Resetting sync source to empty  which was  <*>	11
1353	<*> <*> <*> I  REPL      <*>  Entering primary <*> mode.	11
1354	<*> <*> <*> I  REPL      <*>  Exited primary <*> mode.	11
1355	<*> <*> <*> I  REPL      <*>  Stopping replication producer	11
1356	<*> <*> <*> I  REPL      ReplBatcher  Oplog buffer has been drained in term <*>	11
1357	<*> <*> <*> I  REPL      RstlKillOpThread  Starting to kill user operations	11
1358	<*> <*> <*> I  REPL      RstlKillOpThread  Stopped killing user operations	11
1359	<*> <*> <*> I  REPL      RstlKillOpThread  State transition ops metrics    lastStateTransition   stepUp   userOpsKilled  0  userOpsRunning  0  	11
1360	<*> <*> <*> I  SHARDING  <*>  Marking collection config.transactions as collection version  <unsharded>	11
1361	<*> <*> <*> I  STORAGE   <*>  createCollection  config.transactions with generated UUID  <*> and options    	11
1362	<*> <*> <*> I  REPL      conn2  replSetReconfig admin command received from client  new config    _id   rs0   version  <*>  protocolVersion  <*>  writeConcernMajorityJournalDefault  true  members      _id  0  host   <*> <*>   arbiterOnly  false  buildIndexes  true  hidden  false  priority  <*>  tags      slaveDelay  0  votes  <*>      settings    chainingAllowed  true  heartbeatIntervalMillis  <*>  heartbeatTimeoutSecs  <*>  electionTimeoutMillis  <*>  catchUpTimeoutMillis  <*>  catchUpTakeoverDelayMillis  <*>  getLastErrorModes      getLastErrorDefaults    w  <*>  wtimeout  0    replicaSetId  ObjectId  <*>      configsvr  false  	11
1363	<*> <*> <*> I  REPL      conn2  replSetReconfig config object with <*> members parses ok	11
1364	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns config.transactions	11
1365	<*> <*> <*> I  REPL      <*>  transition to primary complete  database writes are now permitted	11
1366	<*> <*> <*> I  SHARDING  <*>  Marking collection admin.system.keys as collection version  <unsharded>	11
1367	     END logs for container controller of pod <*>     	1
1368	<*> <*> <*> I  STORAGE   <*>  createCollection  admin.system.keys with generated UUID  <*> and options    	11
1369	<*> <*> <*> I  NETWORK   conn2  end connection <*> <*>  <*> connection now open 	11
1370	<*> <*> <*> I  STORAGE   WTJournalFlusher  Triggering the first stable checkpoint. Initial Data  Timestamp <*>  <*>  PrevStable  Timestamp 0  0  CurrStable  Timestamp <*>  <*> 	11
1371	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns admin.system.keys	11
1372	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #4  <*> connections now open 	11
1373	<*> <*> <*> I  NETWORK   conn4  received client metadata from <*> <*> conn4    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1374	<*> <*> <*> I  ACCESS    conn4  Successfully authenticated as principal root on admin from client <*> <*>	11
1375	<*> <*> <*> I  NETWORK   conn4  end connection <*> <*>  <*> connection now open 	11
1376	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> <*>  <*> connections now open 	8184
1377	<*> <*> <*> I  NETWORK   conn5  received client metadata from <*> <*> conn5    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1378	<*> <*> <*> I  ACCESS    conn5  Successfully authenticated as principal root on admin from client <*> <*>	11
1379	<*> <*> <*> I  NETWORK   conn5  end connection <*> <*>  <*> connection now open 	11
1380	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #6  <*> connections now open 	11
1381	<*> <*> <*> I  NETWORK   conn6  received client metadata from <*> <*> conn6    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1382	<*> <*> <*> I  ACCESS    conn6  Successfully authenticated as principal root on admin from client <*> <*>	11
1383	<*> <*> <*> I  NETWORK   conn6  end connection <*> <*>  <*> connection now open 	11
1384	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #8  <*> connections now open 	11
1385	<*> <*> <*> I  NETWORK   conn8  received client metadata from <*> <*> conn8    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1386	<*> <*> <*> I  NETWORK   conn7  received client metadata from <*> <*> conn7    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1387	<*> <*> <*> I  NETWORK   conn8  end connection <*> <*>  <*> connections now open 	11
1388	<*> <*> <*> I  NETWORK   conn7  end connection <*> <*>  <*> connection now open 	11
1389	<*> <*> <*> I  NETWORK   conn9  received client metadata from <*> <*> conn9    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1390	<*> <*> <*> I  NETWORK   conn10  received client metadata from <*> <*> conn10    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1391	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> <*>  4 connections now open 	11
1392	<*> <*> <*> I  NETWORK   <*>  received client metadata from <*> <*> <*>    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	4158
1393	<*> <*> <*> I  ACCESS    <*>  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	847
1394	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> <*>  6 connections now open 	22
1395	<*> <*> <*> I  NETWORK   <*>  end connection <*> <*>  <*> connections now open 	4554
1396	<*> <*> <*> I  NETWORK   <*>  end connection <*> <*>  4 connections now open 	11
1397	<*> <*> <*> I  NETWORK   conn14  received client metadata from <*> <*> conn14    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1398	<*> <*> <*> I  NETWORK   conn16  received client metadata from <*> <*> conn16    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1399	<*> <*> <*> I  ACCESS    conn16  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1400	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #20  <*> connections now open 	11
1401	<*> <*> <*> I  NETWORK   conn20  received client metadata from <*> <*> conn20    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1402	<*> <*> <*> I  ACCESS    <*>  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	187
1403	<*> <*> <*> I  SHARDING  <*>  Marking collection <*> as collection version  <unsharded>	110
1404	<*> <*> <*> I  STORAGE   <*>  createCollection  <*> with generated UUID  <*> and options    	33
1405	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns <*>	55
1406	<*> <*> <*> I  NETWORK   conn22  received client metadata from <*> <*> conn22    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1407	<*> <*> <*> I  NETWORK   conn22  end connection <*> <*>  <*> connections now open 	11
1408	<*> <*> <*> I  NETWORK   conn24  received client metadata from <*> <*> conn24    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1409	<*> <*> <*> I  NETWORK   conn25  received client metadata from <*> <*> conn25    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1410	<*> <*> <*> I  NETWORK   conn26  received client metadata from <*> <*> conn26    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1411	<*> <*> <*> I  ACCESS    conn26  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1412	<*> <*> <*> I  NETWORK   conn27  received client metadata from <*> <*> conn27    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1413	<*> <*> <*> I  ACCESS    conn27  Successfully authenticated as principal root on admin from client <*> <*>	11
1414	<*> <*> <*> I  NETWORK   conn27  end connection <*> <*>  <*> connections now open 	11
1415	<*> <*> <*> I  NETWORK   conn28  received client metadata from <*> <*> conn28    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1416	<*> <*> <*> I  ACCESS    conn28  Successfully authenticated as principal root on admin from client <*> <*>	11
1417	<*> <*> <*> I  NETWORK   conn28  end connection <*> <*>  <*> connections now open 	11
1418	<*> <*> <*> I  NETWORK   <*>  received client metadata from <*> <*> <*>    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	2563
1419	<*> <*> <*> I  ACCESS    <*>  Successfully authenticated as principal root on admin from client <*> <*>	2563
1420	<*> <*> <*> I  NETWORK   listener  connection accepted from <*> <*> #30  <*> connections now open 	11
1421	<*> <*> <*> I  NETWORK   conn30  received client metadata from <*> <*> conn30    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1422	<*> <*> <*> I  ACCESS    conn30  Successfully authenticated as principal root on admin from client <*> <*>	11
1423	<*> <*> <*> I  NETWORK   conn30  end connection <*> <*>  <*> connections now open 	11
1424	<*> <*> <*> I  NETWORK   conn31  received client metadata from <*> <*> conn31    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1425	<*> <*> <*> I  NETWORK   conn32  received client metadata from <*> <*> conn32    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1426	<*> <*> <*> I  NETWORK   conn31  end connection <*> <*>  <*> connections now open 	11
1427	<*> <*> <*> I  NETWORK   conn32  end connection <*> <*>  <*> connections now open 	11
1428	<*> <*> <*> I  NETWORK   conn33  received client metadata from <*> <*> conn33    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1429	<*> <*> <*> I  NETWORK   conn34  received client metadata from <*> <*> conn34    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1430	<*> <*> <*> I  NETWORK   conn35  received client metadata from <*> <*> conn35    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1431	<*> <*> <*> I  ACCESS    conn35  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1432	<*> <*> <*> I  NETWORK   conn36  received client metadata from <*> <*> conn36    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1433	<*> <*> <*> I  NETWORK   conn37  received client metadata from <*> <*> conn37    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1434	<*> <*> <*> I  NETWORK   conn37  end connection <*> <*>  <*> connections now open 	11
1435	<*> <*> <*> I  NETWORK   conn36  end connection <*> <*>  <*> connections now open 	11
1436	<*> <*> <*> I  NETWORK   conn38  received client metadata from <*> <*> conn38    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1437	<*> <*> <*> I  NETWORK   conn39  received client metadata from <*> <*> conn39    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1438	<*> <*> <*> I  NETWORK   conn40  received client metadata from <*> <*> conn40    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1439	<*> <*> <*> I  ACCESS    conn40  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1440	<*> <*> <*> I  SHARDING  conn35  Marking collection <*> as collection version  <unsharded>	11
1441	<*> <*> <*> I  STORAGE   conn35  createCollection  <*> with generated UUID  <*> and options    	11
1442	<*> <*> <*> I  INDEX     conn35  index build  done building index _id_ on ns <*>	11
1443	<*> <*> <*> I  NETWORK   conn41  received client metadata from <*> <*> conn41    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1444	<*> <*> <*> I  NETWORK   conn42  received client metadata from <*> <*> conn42    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1445	<*> <*> <*> I  NETWORK   conn42  end connection <*> <*>  <*> connections now open 	11
1446	<*> <*> <*> I  NETWORK   conn41  end connection <*> <*>  <*> connections now open 	11
1447	<*> <*> <*> I  NETWORK   conn43  received client metadata from <*> <*> conn43    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1448	<*> <*> <*> I  NETWORK   conn44  received client metadata from <*> <*> conn44    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1449	<*> <*> <*> I  NETWORK   conn45  received client metadata from <*> <*> conn45    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1450	<*> <*> <*> I  ACCESS    conn45  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1451	<*> <*> <*> I  SHARDING  conn45  Marking collection <*> as collection version  <unsharded>	11
1452	<*> <*> <*> I  NETWORK   conn46  received client metadata from <*> <*> conn46    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1453	<*> <*> <*> I  NETWORK   conn47  received client metadata from <*> <*> conn47    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1454	<*> <*> <*> I  NETWORK   conn47  end connection <*> <*>  <*> connections now open 	11
1455	<*> <*> <*> I  NETWORK   conn46  end connection <*> <*>  <*> connections now open 	11
1456	<*> <*> <*> I  NETWORK   conn48  received client metadata from <*> <*> conn48    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1457	<*> <*> <*> I  NETWORK   conn49  received client metadata from <*> <*> conn49    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1458	<*> <*> <*> I  NETWORK   conn50  received client metadata from <*> <*> conn50    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1459	<*> <*> <*> I  ACCESS    conn50  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1460	<*> <*> <*> I  SHARDING  conn50  Marking collection <*> as collection version  <unsharded>	11
1461	<*> <*> <*> I  STORAGE   conn45  createCollection  <*> with generated UUID  <*> and options    	11
1462	<*> <*> <*> I  INDEX     conn45  index build  done building index _id_ on ns <*>	11
1463	<*> <*> <*> I  NETWORK   conn52  received client metadata from <*> <*> conn52    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1464	<*> <*> <*> I  NETWORK   conn52  end connection <*> <*>  <*> connections now open 	11
1465	<*> <*> <*> I  NETWORK   conn53  received client metadata from <*> <*> conn53    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1466	<*> <*> <*> I  NETWORK   conn54  received client metadata from <*> <*> conn54    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1467	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.tenants as collection version  <unsharded>	11
1468	<*> <*> <*> I  NETWORK   conn57  received client metadata from <*> <*> conn57    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1469	<*> <*> <*> I  NETWORK   conn56  received client metadata from <*> <*> conn56    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1470	<*> <*> <*> I  NETWORK   conn57  end connection <*> <*>  <*> connections now open 	11
1471	<*> <*> <*> I  NETWORK   conn56  end connection <*> <*>  <*> connections now open 	11
1472	<*> <*> <*> I  NETWORK   conn58  received client metadata from <*> <*> conn58    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1473	<*> <*> <*> I  NETWORK   conn60  received client metadata from <*> <*> conn60    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1474	<*> <*> <*> I  ACCESS    conn60  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1475	<*> <*> <*> I  NETWORK   conn62  received client metadata from <*> <*> conn62    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1476	<*> <*> <*> I  NETWORK   conn62  end connection <*> <*>  <*> connections now open 	11
1477	<*> <*> <*> I  NETWORK   conn63  received client metadata from <*> <*> conn63    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1478	<*> <*> <*> I  NETWORK   conn64  received client metadata from <*> <*> conn64    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1479	<*> <*> <*> I  NETWORK   conn65  received client metadata from <*> <*> conn65    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1480	<*> <*> <*> I  ACCESS    conn65  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1481	<*> <*> <*> I  NETWORK   conn66  received client metadata from <*> <*> conn66    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1482	<*> <*> <*> I  NETWORK   conn67  received client metadata from <*> <*> conn67    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1483	<*> <*> <*> I  NETWORK   conn67  end connection <*> <*>  <*> connections now open 	11
1484	<*> <*> <*> I  NETWORK   conn66  end connection <*> <*>  <*> connections now open 	11
1485	<*> <*> <*> I  NETWORK   conn68  received client metadata from <*> <*> conn68    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1486	<*> <*> <*> I  NETWORK   conn70  received client metadata from <*> <*> conn70    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1487	<*> <*> <*> I  ACCESS    conn70  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1488	<*> <*> <*> I  STORAGE   conn70  createCollection  <*> with generated UUID  <*> and options    	110
1489	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns <*>	110
1490	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.machines with generated UUID  <*> and options    	11
1491	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.machines	11
1492	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.packtags with generated UUID  <*> and options    	11
1493	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.packtags	11
1494	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.clusterprofiles with generated UUID  <*> and options    	11
1495	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.clusterprofiles	11
1496	<*> <*> <*> I  NETWORK   conn72  received client metadata from <*> <*> conn72    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1497	<*> <*> <*> I  ACCESS    conn72  Successfully authenticated as principal root on admin from client <*> <*>	11
1498	<*> <*> <*> I  NETWORK   conn72  end connection <*> <*>  <*> connections now open 	11
1499	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.users with generated UUID  <*> and options    	11
1500	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.users	11
1501	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.roles with generated UUID  <*> and options    	11
1502	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.roles	11
1503	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.tenants with generated UUID  <*> and options    	11
1504	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.tenants	11
1505	<*> <*> <*> I  NETWORK   conn73  received client metadata from <*> <*> conn73    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1506	<*> <*> <*> I  ACCESS    conn73  Successfully authenticated as principal root on admin from client <*> <*>	11
1507	<*> <*> <*> I  NETWORK   conn73  end connection <*> <*>  <*> connections now open 	11
1508	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.notifications with generated UUID  <*> and options    	11
1509	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.notifications	11
1510	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.notification_events with generated UUID  <*> and options    	11
1511	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.notification_events	11
1512	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.cloudmeta_instancetypes with generated UUID  <*> and options    	11
1513	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.cloudmeta_instancetypes	11
1514	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.plans with generated UUID  <*> and options    	11
1515	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.plans	11
1516	<*> <*> <*> I  NETWORK   conn74  received client metadata from <*> <*> conn74    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1517	<*> <*> <*> I  ACCESS    conn74  Successfully authenticated as principal root on admin from client <*> <*>	11
1518	<*> <*> <*> I  NETWORK   conn74  end connection <*> <*>  <*> connections now open 	11
1519	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.plan_usages with generated UUID  <*> and options    	11
1520	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.plan_usages	11
1521	<*> <*> <*> I  STORAGE   conn70  createCollection  hubbledb.tokens with generated UUID  <*> and options    	11
1522	<*> <*> <*> I  INDEX     conn70  index build  done building index _id_ on ns hubbledb.tokens	11
1523	<*> <*> <*> I  NETWORK   conn75  received client metadata from <*> <*> conn75    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1524	<*> <*> <*> I  ACCESS    conn75  Successfully authenticated as principal root on admin from client <*> <*>	11
1525	<*> <*> <*> I  NETWORK   conn75  end connection <*> <*>  <*> connections now open 	11
1526	<*> <*> <*> I  NETWORK   conn77  received client metadata from <*> <*> conn77    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1527	<*> <*> <*> I  NETWORK   conn76  received client metadata from <*> <*> conn76    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1528	<*> <*> <*> I  NETWORK   conn76  end connection <*> <*>  <*> connections now open 	11
1529	<*> <*> <*> I  NETWORK   conn77  end connection <*> <*>  <*> connections now open 	11
1530	<*> <*> <*> I  NETWORK   conn78  received client metadata from <*> <*> conn78    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1531	<*> <*> <*> I  NETWORK   conn80  received client metadata from <*> <*> conn80    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1532	<*> <*> <*> I  ACCESS    conn80  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1533	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    aclmeta.scope  <*>    name   aclmeta.scope_1   ns   <*>    using method  Hybrid	66
1534	<*> <*> <*> I  INDEX     conn80  build may temporarily use up to <*> megabytes of RAM	396
1535	<*> <*> <*> I  INDEX     conn80  index build  collection scan done. scanned 0 total records in 0 seconds	396
1536	<*> <*> <*> I  INDEX     conn80  index build  inserted 0 keys from external sorter into index in 0 seconds	396
1537	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.scope_1 on ns <*>	66
1538	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   <*>    using method  Hybrid	55
1539	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.tenantUid_1 on ns <*>	55
1540	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    kind  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1541	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns <*>	55
1542	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    aclmeta.scope  <*>    name   aclmeta.scope_1   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1543	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.scope_1 on ns hubbledb.clusterprofiles	11
1544	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1545	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.clusterprofiles	11
1546	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    status.isPublished  <*>    name   <*>   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1547	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.clusterprofiles	11
1548	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.machines properties    v  <*>  key    aclmeta.scope  <*>    name   aclmeta.scope_1   ns   hubbledb.machines    using method  Hybrid	11
1549	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.scope_1 on ns hubbledb.machines	11
1550	<*> <*> <*> I  NETWORK   conn82  received client metadata from <*> <*> conn82    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1551	<*> <*> <*> I  ACCESS    conn82  Successfully authenticated as principal root on admin from client <*> <*>	11
1552	<*> <*> <*> I  NETWORK   conn82  end connection <*> <*>  <*> connections now open 	11
1553	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.machines properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.machines    using method  Hybrid	11
1554	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.machines	11
1555	<*> <*> <*> I  NETWORK   conn83  received client metadata from <*> <*> conn83    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1556	<*> <*> <*> I  ACCESS    conn83  Successfully authenticated as principal root on admin from client <*> <*>	11
1557	<*> <*> <*> I  NETWORK   conn83  end connection <*> <*>  <*> connections now open 	11
1558	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    metadata.name  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1559	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.roles properties    v  <*>  key    aclmeta.scope  <*>    name   aclmeta.scope_1   ns   hubbledb.roles    using method  Hybrid	11
1560	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.scope_1 on ns hubbledb.roles	11
1561	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.roles properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.roles    using method  Hybrid	11
1562	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.roles	11
1563	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.roles properties    v  <*>  key    metadata.name  <*>    name   <*>   ns   hubbledb.roles    using method  Hybrid	11
1564	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.roles	11
1565	<*> <*> <*> I  NETWORK   conn84  received client metadata from <*> <*> conn84    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1566	<*> <*> <*> I  ACCESS    conn84  Successfully authenticated as principal root on admin from client <*> <*>	11
1567	<*> <*> <*> I  NETWORK   conn84  end connection <*> <*>  <*> connections now open 	11
1568	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    spec.cloudType  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1569	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    <*>  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1570	<*> <*> <*> I  NETWORK   conn85  received client metadata from <*> <*> conn85    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1571	<*> <*> <*> I  ACCESS    conn85  Successfully authenticated as principal root on admin from client <*> <*>	11
1572	<*> <*> <*> I  NETWORK   conn85  end connection <*> <*>  <*> connections now open 	11
1573	<*> <*> <*> I  INDEX     conn80  index build  starting on <*> properties    v  <*>  key    spec.name  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1574	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.packtags properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubbledb.packtags    using method  Hybrid	11
1575	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.packtags	22
1576	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.packtags properties    v  <*>  key    spec.tag  <*>    name   <*>   ns   hubbledb.packtags    using method  Hybrid	11
1577	<*> <*> <*> I  STORAGE   conn80  createCollection  hubbledb.blob with provided UUID  <*> and options    uuid  UUID  <*>    	11
1578	<*> <*> <*> I  INDEX     conn80  index build  done building index _id_ on ns hubbledb.blob	11
1579	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.blob properties    v  <*>  key    status.associatedObject  <*>    name   <*>   ns   hubbledb.blob    using method  Hybrid	11
1580	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.blob	11
1581	<*> <*> <*> I  NETWORK   conn86  received client metadata from <*> <*> conn86    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1582	<*> <*> <*> I  ACCESS    conn86  Successfully authenticated as principal root on admin from client <*> <*>	11
1583	<*> <*> <*> I  NETWORK   conn86  end connection <*> <*>  <*> connections now open 	11
1584	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.tokens properties    v  <*>  key    spec.value  <*>    name   <*>   ns   hubbledb.tokens    using method  Hybrid	11
1585	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.tokens	11
1586	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.users properties    v  <*>  key    aclmeta.scope  <*>    name   aclmeta.scope_1   ns   hubbledb.users    using method  Hybrid	11
1587	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.scope_1 on ns hubbledb.users	11
1588	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.users properties    v  <*>  key    spec.emailIdHash  <*>    name   <*>   ns   hubbledb.users    using method  Hybrid	11
1589	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.users	11
1590	<*> <*> <*> I  NETWORK   conn87  received client metadata from <*> <*> conn87    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1591	<*> <*> <*> I  ACCESS    conn87  Successfully authenticated as principal root on admin from client <*> <*>	11
1592	<*> <*> <*> I  NETWORK   conn87  end connection <*> <*>  <*> connections now open 	11
1593	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.users properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.users    using method  Hybrid	11
1594	<*> <*> <*> I  INDEX     conn80  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.users	11
1595	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.tenants properties    v  <*>  key    spec.orgName  <*>    name   <*>   ns   hubbledb.tenants    using method  Hybrid	11
1596	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.tenants	11
1597	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.cloudmeta_instancetypes properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubbledb.cloudmeta_instancetypes    using method  Hybrid	11
1598	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.cloudmeta_instancetypes	11
1599	<*> <*> <*> I  STORAGE   conn80  createCollection  hubbledb.componentevents with provided UUID  <*> and options    uuid  UUID  <*>    	11
1600	<*> <*> <*> I  INDEX     conn80  index build  done building index _id_ on ns hubbledb.componentevents	11
1601	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.componentevents properties    v  <*>  key    relatedObject.uid  <*>    name   <*>   ns   hubbledb.componentevents    using method  Hybrid	11
1602	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.componentevents	11
1603	<*> <*> <*> I  NETWORK   conn88  received client metadata from <*> <*> conn88    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1604	<*> <*> <*> I  ACCESS    conn88  Successfully authenticated as principal root on admin from client <*> <*>	11
1605	<*> <*> <*> I  NETWORK   conn88  end connection <*> <*>  <*> connections now open 	11
1606	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.notifications properties    v  <*>  key    relatedObject.uid  <*>    name   <*>   ns   hubbledb.notifications    using method  Hybrid	11
1607	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.notifications	11
1608	<*> <*> <*> I  INDEX     conn80  index build  starting on hubbledb.notification_events properties    v  <*>  key    relatedObject.uid  <*>    name   <*>   ns   hubbledb.notification_events    using method  Hybrid	11
1609	<*> <*> <*> I  INDEX     conn80  index build  done building index <*> on ns hubbledb.notification_events	11
1610	<*> <*> <*> I  NETWORK   conn90  received client metadata from <*> <*> conn90    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1611	<*> <*> <*> I  NETWORK   conn90  end connection <*> <*>  <*> connections now open 	11
1612	<*> <*> <*> I  NETWORK   conn92  received client metadata from <*> <*> conn92    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1613	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.componentevents properties    v  <*>  key    severity  <*>    name   severity_1   ns   hubbledb.componentevents    using method  Hybrid	11
1614	<*> <*> <*> I  INDEX     <*>  build may temporarily use up to <*> megabytes of RAM	660
1615	<*> <*> <*> I  INDEX     <*>  index build  collection scan done. scanned 0 total records in 0 seconds	660
1616	<*> <*> <*> I  INDEX     <*>  index build  inserted 0 keys from external sorter into index in 0 seconds	660
1617	<*> <*> <*> I  INDEX     <*>  index build  done building index severity_1 on ns hubbledb.componentevents	11
1618	<*> <*> <*> I  NETWORK   conn94  received client metadata from <*> <*> conn94    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1619	<*> <*> <*> I  NETWORK   conn94  end connection <*> <*>  <*> connections now open 	11
1620	<*> <*> <*> I  NETWORK   conn96  received client metadata from <*> <*> conn96    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1621	<*> <*> <*> I  NETWORK   conn98  received client metadata from <*> <*> conn98    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1622	<*> <*> <*> I  ACCESS    conn98  Successfully authenticated as principal hubble on hubble_timeseriesdb from client <*> <*>	11
1623	<*> <*> <*> I  STORAGE   conn98  createCollection  hubble_timeseriesdb.machinemetrics with provided UUID  <*> and options    uuid  UUID  <*>    	11
1624	<*> <*> <*> I  INDEX     conn98  index build  done building index _id_ on ns hubble_timeseriesdb.machinemetrics	11
1625	<*> <*> <*> I  INDEX     conn98  index build  starting on hubble_timeseriesdb.machinemetrics properties    v  <*>  key    values.timestamp  <*>    name   <*>   ns   hubble_timeseriesdb.machinemetrics    using method  Hybrid	11
1626	<*> <*> <*> I  INDEX     conn98  build may temporarily use up to <*> megabytes of RAM	66
1627	<*> <*> <*> I  INDEX     conn98  index build  collection scan done. scanned 0 total records in 0 seconds	66
1628	<*> <*> <*> I  INDEX     conn98  index build  inserted 0 keys from external sorter into index in 0 seconds	66
1629	<*> <*> <*> I  INDEX     conn98  index build  done building index <*> on ns hubble_timeseriesdb.machinemetrics	55
1630	<*> <*> <*> I  INDEX     conn98  index build  starting on hubble_timeseriesdb.machinemetrics properties    v  <*>  key    metadata.kind  <*>    name   <*>   ns   hubble_timeseriesdb.machinemetrics    using method  Hybrid	11
1631	<*> <*> <*> I  INDEX     conn98  index build  starting on hubble_timeseriesdb.machinemetrics properties    v  <*>  key    metadata.relatedObject.uid  <*>    name   <*>   ns   hubble_timeseriesdb.machinemetrics    using method  Hybrid	11
1632	<*> <*> <*> I  INDEX     conn98  index build  starting on hubble_timeseriesdb.machinemetrics properties    v  <*>  key    metadata.relatedObject.kind  <*>    name   <*>   ns   hubble_timeseriesdb.machinemetrics    using method  Hybrid	11
1633	<*> <*> <*> I  INDEX     conn98  index build  starting on hubble_timeseriesdb.machinemetrics properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubble_timeseriesdb.machinemetrics    using method  Hybrid	11
1634	<*> <*> <*> I  INDEX     conn98  index build  starting on hubble_timeseriesdb.machinemetrics properties    v  <*>  key    metadata.startTimestamp  <*>    name   metadata.startTimestamp_1   ns   hubble_timeseriesdb.machinemetrics    using method  Hybrid	11
1635	<*> <*> <*> I  INDEX     conn98  index build  done building index metadata.startTimestamp_1 on ns hubble_timeseriesdb.machinemetrics	11
1636	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    spec.actionType  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1637	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns <*>	220
1638	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    metadata.creationTimestamp  <*>    name   metadata.creationTimestamp_1   ns   <*>    using method  Hybrid	22
1639	<*> <*> <*> I  INDEX     <*>  index build  done building index metadata.creationTimestamp_1 on ns <*>	22
1640	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    <*>  <*>    name   <*>   ns   <*>    using method  Hybrid	44
1641	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    spec.actor.tenantUid  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1642	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    spec.actor.userUid  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1643	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.metadata_archives with provided UUID  <*> and options    uuid  UUID  <*>    	11
1644	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.metadata_archives	11
1645	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.metadata_archives properties    v  <*>  key    metadata.uid  <*>    name   <*>   ns   hubbledb.metadata_archives    using method  Hybrid	11
1646	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.metadata_archives	22
1647	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.metadata_archives properties    v  <*>  key    kind  <*>    name   <*>   ns   hubbledb.metadata_archives    using method  Hybrid	11
1648	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.notification_events properties    v  <*>  key    spec.type  <*>  state.isProcessed  <*>    name   <*>   ns   hubbledb.notification_events    using method  Hybrid	11
1649	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.notification_events	11
1650	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.clusterprofile_archives with provided UUID  <*> and options    uuid  UUID  <*>    	11
1651	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.clusterprofile_archives	11
1652	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofile_archives properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.clusterprofile_archives    using method  Hybrid	11
1653	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.clusterprofile_archives	11
1654	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.user_archives with provided UUID  <*> and options    uuid  UUID  <*>    	11
1655	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.user_archives	11
1656	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.user_archives properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.user_archives    using method  Hybrid	11
1657	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.user_archives	11
1658	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.locks with provided UUID  <*> and options    uuid  UUID  <*>    	11
1659	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.locks	11
1660	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.locks properties    v  <*>  key    spec.key  <*>    name   <*>   ns   hubbledb.locks    using method  Hybrid	11
1661	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.locks	11
1662	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    status.state  <*>    name   <*>   ns   <*>    using method  Hybrid	22
1663	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.plan_usages properties    v  <*>  key    spec.userUid  <*>    name   <*>   ns   hubbledb.plan_usages    using method  Hybrid	11
1664	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.plan_usages	33
1665	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.plan_usages properties    v  <*>  key    spec.planUid  <*>    name   <*>   ns   hubbledb.plan_usages    using method  Hybrid	11
1666	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.plan_usages properties    v  <*>  key    spec.month  <*>    name   <*>   ns   hubbledb.plan_usages    using method  Hybrid	11
1667	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1668	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.clusterprofiles	55
1669	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    kind  <*>    name   <*>   ns   <*>    using method  Hybrid	22
1670	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    aclmeta.projectUid  <*>    name   <*>   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1671	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.overlords with provided UUID  <*> and options    uuid  UUID  <*>    	11
1672	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.overlords	11
1673	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.overlords properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.overlords    using method  Hybrid	11
1674	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.overlords	11
1675	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.notifications properties    v  <*>  key    relatedObject.kind  <*>    name   <*>   ns   hubbledb.notifications    using method  Hybrid	11
1676	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.notifications	11
1677	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.nats_accounts with provided UUID  <*> and options    uuid  UUID  <*>    	11
1678	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.nats_accounts	11
1679	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.nats_accounts properties    v  <*>  key    spec.name  <*>    name   <*>   ns   hubbledb.nats_accounts    using method  Hybrid	11
1680	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.nats_accounts	11
1681	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.nats_users with provided UUID  <*> and options    uuid  UUID  <*>    	11
1682	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.nats_users	11
1683	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.nats_users properties    v  <*>  key    spec.account  <*>  spec.name  <*>    name   <*>   ns   hubbledb.nats_users    using method  Hybrid	11
1684	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.nats_users	11
1685	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.ipam_networks with generated UUID  <*> and options    	11
1686	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.ipam_networks	11
1687	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.ipam_pools with generated UUID  <*> and options    	11
1688	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.ipam_pools	11
1689	<*> <*> <*> I  STORAGE   <*>  createCollection  <*> with provided UUID  <*> and options    uuid  UUID  <*>    	22
1690	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  unique  true  key    metadata.name  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1691	<*> <*> <*> I  COMMAND   <*>  command <*> command  createIndexes   createIndexes   systems   indexes      key    metadata.name  <*>    name   <*>   unique  true      lsid    id  UUID  <*>      $clusterTime    clusterTime  Timestamp <*>  <*>   signature    hash  BinData 0  <*>   keyId  <*>      $db   hubbledb    numYields 0 reslen <*> locks   ParallelBatchWriterMode    acquireCount    r  <*>      ReplicationStateTransition    acquireCount    w  <*>      Global    acquireCount    r  <*>  w  <*>      Database    acquireCount    r  <*>  w  <*>      Collection    acquireCount    r  4  w  <*>  R  <*>  W  <*>      Mutex    acquireCount    r  4       flowControl   acquireCount  <*>   storage    protocol op_msg <*>	11
1692	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.users as collection version  <unsharded>	11
1693	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.users properties    v  <*>  unique  true  key    spec.emailId  <*>    name   <*>   ns   hubbledb.users    using method  Hybrid	11
1694	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.users	11
1695	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.user_assets with provided UUID  <*> and options    uuid  UUID  <*>    	11
1696	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.user_assets	11
1697	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.user_assets properties    v  <*>  key    kind  <*>    name   <*>   ns   hubbledb.user_assets    using method  Hybrid	11
1698	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.user_assets	11
1699	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.user_assets properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.user_assets    using method  Hybrid	11
1700	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.user_assets	11
1701	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.cloudmeta_instancetypes properties    v  <*>  key    kind  <*>    name   <*>   ns   hubbledb.cloudmeta_instancetypes    using method  Hybrid	11
1702	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.cloudmeta_instancetypes	11
1703	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    metadata.name  <*>    name   <*>   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1704	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    metadata.creationTimestamp  <*>    name   metadata.creationTimestamp_1   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1705	<*> <*> <*> I  INDEX     <*>  index build  done building index metadata.creationTimestamp_1 on ns hubbledb.clusterprofiles	11
1706	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    spec.published.type  <*>    name   <*>   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1707	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.ipam_networks properties    v  <*>  key    spec.privateGatewayUid  <*>    name   <*>   ns   hubbledb.ipam_networks    using method  Hybrid	11
1708	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.ipam_networks	11
1709	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.ipam_pools properties    v  <*>  key    spec.ipamNetworkUid  <*>    name   <*>   ns   hubbledb.ipam_pools    using method  Hybrid	11
1710	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.ipam_pools	11
1711	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    metadata.annotations.spectroClusterUid  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1712	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.machines properties    v  <*>  key    metadata.annotations.spectroClusterUid  <*>    name   <*>   ns   hubbledb.machines    using method  Hybrid	11
1713	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.machines	11
1714	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    aclmeta.scope  <*>    name   aclmeta.scope_1   ns   <*>    using method  Hybrid	11
1715	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.scope_1 on ns <*>	11
1716	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   <*>    using method  Hybrid	11
1717	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.tenantUid_1 on ns <*>	11
1718	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    aclemta.projectUid  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1719	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    metadata.name  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1720	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    aclmeta.projectUid  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1721	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.serviceversions with provided UUID  <*> and options    uuid  UUID  <*>    	11
1722	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.serviceversions	11
1723	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.serviceversions properties    v  <*>  key    spec.name  <*>    name   <*>   ns   hubbledb.serviceversions    using method  Hybrid	11
1724	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.serviceversions	11
1725	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.invoices with provided UUID  <*> and options    uuid  UUID  <*>    	11
1726	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.invoices	11
1727	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.invoices properties    v  <*>  key    aclmeta.tenantUid  <*>    name   aclmeta.tenantUid_1   ns   hubbledb.invoices    using method  Hybrid	11
1728	<*> <*> <*> I  INDEX     <*>  index build  done building index aclmeta.tenantUid_1 on ns hubbledb.invoices	11
1729	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.manifests with generated UUID  <*> and options    	11
1730	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.manifests	11
1731	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    spec.addonType  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1732	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    spec.type  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1733	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    spec.cloudTypes  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1734	<*> <*> <*> I  COMMAND   <*>  CMD  drop hubbledb.packvalues	55
1735	<*> <*> <*> I  NETWORK   conn200  received client metadata from <*> <*> conn200    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1736	<*> <*> <*> I  NETWORK   conn200  end connection <*> <*>  <*> connections now open 	11
1737	<*> <*> <*> I  NETWORK   conn202  received client metadata from <*> <*> conn202    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1738	<*> <*> <*> I  COMMAND   <*>  CMD  drop hubbledb.serviceversions	11
1739	<*> <*> <*> I  STORAGE   <*>  dropCollection  hubbledb.serviceversions  <*>  <*> storage engine will take ownership of <*> collection with optime   ts  Timestamp 0  0   t  <*>   and commit timestamp Timestamp 0  0 	11
1740	<*> <*> <*> I  STORAGE   <*>  Finishing collection drop for hubbledb.serviceversions  <*> .	11
1741	<*> <*> <*> I  STORAGE   <*>  Deferring table drop for index  _id_  on collection  <*>  <*>  . Ident   <*>   commit timestamp   Timestamp <*>  <*>  	11
1742	<*> <*> <*> I  STORAGE   <*>  Deferring table drop for index  <*>  on collection  <*>  <*>  . Ident   <*>   commit timestamp   Timestamp <*>  <*>  	11
1743	<*> <*> <*> I  STORAGE   <*>  Deferring table drop for collection  hubbledb.serviceversions   <*> . Ident  <*>  commit timestamp  Timestamp <*>  <*> 	11
1744	<*> <*> <*> I  NETWORK   conn204  received client metadata from <*> <*> conn204    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1745	<*> <*> <*> I  NETWORK   conn204  end connection <*> <*>  <*> connections now open 	11
1746	<*> <*> <*> I  NETWORK   conn206  received client metadata from <*> <*> conn206    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1747	<*> <*> <*> I  NETWORK   conn208  received client metadata from <*> <*> conn208    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1748	<*> <*> <*> I  ACCESS    conn208  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1749	<*> <*> <*> I  INDEX     conn208  index build  starting on hubbledb.componentevents properties    v  <*>  key    metadata.creationTimestamp  <*>    name   <*>   ns   hubbledb.componentevents    using method  Hybrid	11
1750	<*> <*> <*> I  INDEX     conn208  build may temporarily use up to <*> megabytes of RAM	11
1751	<*> <*> <*> I  INDEX     conn208  index build  collection scan done. scanned 0 total records in 0 seconds	11
1752	<*> <*> <*> I  INDEX     conn208  index build  inserted 0 keys from external sorter into index in 0 seconds	11
1753	<*> <*> <*> I  INDEX     conn208  index build  done building index <*> on ns hubbledb.componentevents	11
1754	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.spectrocluster_features with provided UUID  <*> and options    uuid  UUID  <*>    	11
1755	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.spectrocluster_features	11
1756	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.spectrocluster_features properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubbledb.spectrocluster_features    using method  Hybrid	11
1757	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.spectrocluster_features	11
1758	<*> <*> <*> I  INDEX     <*>  index build  starting on <*> properties    v  <*>  key    metadata.tags  <*>    name   <*>   ns   <*>    using method  Hybrid	11
1759	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.clusterprofiles properties    v  <*>  key    metadata.tags  <*>    name   <*>   ns   hubbledb.clusterprofiles    using method  Hybrid	11
1760	<*> <*> <*> I  NETWORK   conn220  received client metadata from <*> <*> conn220    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1761	<*> <*> <*> I  NETWORK   conn220  end connection <*> <*>  <*> connections now open 	11
1762	<*> <*> <*> I  STORAGE   TimestampMonitor  Removing <*> idents with drop timestamps before timestamp Timestamp <*>  <*> 	11
1763	<*> <*> <*> I  STORAGE   TimestampMonitor  Completing drop for ident <*>  ns  <*>  with drop timestamp Timestamp <*>  <*> 	22
1764	<*> <*> <*> I  STORAGE   TimestampMonitor  Completing drop for ident <*>  ns  hubbledb.serviceversions  with drop timestamp Timestamp <*>  <*> 	11
1765	<*> <*> <*> I  NETWORK   conn230  received client metadata from <*> <*> conn230    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1766	<*> <*> <*> I  ACCESS    conn230  Successfully authenticated as principal root on admin from client <*> <*>	11
1767	<*> <*> <*> I  NETWORK   conn230  end connection <*> <*>  <*> connections now open 	11
1768	<*> <*> <*> I  ACCESS    <*>  Successfully authenticated as principal hubble on hubble_timeseriesdb from client <*> <*>	66
1769	<*> <*> <*> I  STORAGE   <*>  createCollection  hubble_timeseriesdb.podmetrics with provided UUID  <*> and options    uuid  UUID  <*>    	11
1770	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubble_timeseriesdb.podmetrics	11
1771	<*> <*> <*> I  INDEX     <*>  index build  starting on hubble_timeseriesdb.podmetrics properties    v  <*>  key    metadata.kind  <*>    name   <*>   ns   hubble_timeseriesdb.podmetrics    using method  Hybrid	11
1772	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubble_timeseriesdb.podmetrics	33
1773	<*> <*> <*> I  INDEX     <*>  index build  starting on hubble_timeseriesdb.podmetrics properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubble_timeseriesdb.podmetrics    using method  Hybrid	11
1774	<*> <*> <*> I  NETWORK   conn240  received client metadata from <*> <*> conn240    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1775	<*> <*> <*> I  ACCESS    conn240  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1776	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.spectrocluster_usages with provided UUID  <*> and options    uuid  UUID  <*>    	11
1777	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.spectrocluster_usages	11
1778	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.spectrocluster_usages properties    v  <*>  key    <*>  <*>    name   <*>   ns   hubbledb.spectrocluster_usages    using method  Hybrid	11
1779	<*> <*> <*> I  INDEX     <*>  index build  done building index <*> on ns hubbledb.spectrocluster_usages	33
1780	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.spectrocluster_usages properties    v  <*>  key    spec.year  <*>    name   <*>   ns   hubbledb.spectrocluster_usages    using method  Hybrid	11
1781	<*> <*> <*> I  INDEX     <*>  index build  starting on hubbledb.spectrocluster_usages properties    v  <*>  key    aclmeta.projectUid  <*>    name   <*>   ns   hubbledb.spectrocluster_usages    using method  Hybrid	11
1782	<*> <*> <*> I  INDEX     <*>  index build  starting on hubble_timeseriesdb.podmetrics properties    v  <*>  key    spec.month  <*>    name   <*>   ns   hubble_timeseriesdb.podmetrics    using method  Hybrid	11
1783	<*> <*> <*> I  NETWORK   conn256  received client metadata from <*> <*> conn256    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1784	<*> <*> <*> I  NETWORK   conn256  end connection <*> <*>  <*> connections now open 	11
1785	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.nats as collection version  <unsharded>	11
1786	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.nats with generated UUID  <*> and options    	11
1787	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.nats	11
1788	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.nats_accounts as collection version  <unsharded>	11
1789	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.locks as collection version  <unsharded>	11
1790	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.nats_users as collection version  <unsharded>	11
1791	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.scheduler as collection version  <unsharded>	11
1792	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.scheduler with generated UUID  <*> and options    	11
1793	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.scheduler	11
1794	<*> <*> <*> I  NETWORK   conn280  received client metadata from <*> <*> conn280    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1795	<*> <*> <*> I  NETWORK   conn300  received client metadata from <*> <*> conn300    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1796	<*> <*> <*> I  NETWORK   conn302  received client metadata from <*> <*> conn302    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1797	<*> <*> <*> I  NETWORK   conn303  received client metadata from <*> <*> conn303    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1798	<*> <*> <*> I  NETWORK   conn303  end connection <*> <*>  <*> connections now open 	11
1799	<*> <*> <*> I  NETWORK   conn302  end connection <*> <*>  <*> connections now open 	11
1800	<*> <*> <*> I  NETWORK   conn304  received client metadata from <*> <*> conn304    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1801	<*> <*> <*> I  NETWORK   conn306  received client metadata from <*> <*> conn306    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1802	<*> <*> <*> I  NETWORK   conn307  received client metadata from <*> <*> conn307    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1803	<*> <*> <*> I  NETWORK   conn308  received client metadata from <*> <*> conn308    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1804	<*> <*> <*> I  NETWORK   conn307  end connection <*> <*>  <*> connections now open 	11
1805	<*> <*> <*> I  NETWORK   conn308  end connection <*> <*>  <*> connections now open 	11
1806	<*> <*> <*> I  ACCESS    conn306  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1807	<*> <*> <*> I  NETWORK   conn280  end connection <*> <*>  <*> connections now open 	11
1808	<*> <*> <*> I  NETWORK   conn300  end connection <*> <*>  <*> connections now open 	11
1809	<*> <*> <*> I  NETWORK   conn304  end connection <*> <*>  <*> connections now open 	11
1810	<*> <*> <*> I  NETWORK   conn306  end connection <*> <*>  <*> connections now open 	11
1811	<*> <*> <*> I  NETWORK   conn320  received client metadata from <*> <*> conn320    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1812	<*> <*> <*> I  ACCESS    conn320  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1813	<*> <*> <*> I  NETWORK   conn324  received client metadata from <*> <*> conn324    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1814	<*> <*> <*> I  NETWORK   conn326  received client metadata from <*> <*> conn326    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1815	<*> <*> <*> I  ACCESS    conn326  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1816	<*> <*> <*> I  NETWORK   conn326  end connection <*> <*>  <*> connections now open 	11
1817	<*> <*> <*> I  NETWORK   conn330  received client metadata from <*> <*> conn330    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1818	<*> <*> <*> I  NETWORK   conn324  end connection <*> <*>  <*> connections now open 	11
1819	<*> <*> <*> I  NETWORK   conn320  end connection <*> <*>  <*> connections now open 	11
1820	<*> <*> <*> I  NETWORK   conn332  received client metadata from <*> <*> conn332    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1821	<*> <*> <*> I  NETWORK   conn332  end connection <*> <*>  <*> connections now open 	11
1822	<*> <*> <*> I  NETWORK   conn336  received client metadata from <*> <*> conn336    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1823	<*> <*> <*> I  ACCESS    conn336  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1824	<*> <*> <*> I  NETWORK   conn336  end connection <*> <*>  <*> connections now open 	11
1825	<*> <*> <*> I  NETWORK   conn330  end connection <*> <*>  <*> connections now open 	11
1826	<*> <*> <*> I  NETWORK   conn340  received client metadata from <*> <*> conn340    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1827	<*> <*> <*> I  NETWORK   conn340  end connection <*> <*>  <*> connections now open 	11
1828	<*> <*> <*> I  NETWORK   conn346  received client metadata from <*> <*> conn346    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1829	<*> <*> <*> I  ACCESS    conn346  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1830	<*> <*> <*> I  NETWORK   conn356  received client metadata from <*> <*> conn356    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1831	<*> <*> <*> I  ACCESS    conn356  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1832	<*> <*> <*> I  NETWORK   conn346  end connection <*> <*>  <*> connections now open 	11
1833	<*> <*> <*> I  NETWORK   conn356  end connection <*> <*>  <*> connections now open 	11
1834	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.backupregistries as collection version  <unsharded>	11
1835	<*> <*> <*> I  NETWORK   conn360  received client metadata from <*> <*> conn360    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1836	<*> <*> <*> I  NETWORK   conn360  end connection <*> <*>  <*> connections now open 	11
1837	<*> <*> <*> I  NETWORK   conn364  received client metadata from <*> <*> conn364    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1838	<*> <*> <*> I  NETWORK   conn364  end connection <*> <*>  <*> connections now open 	11
1839	<*> <*> <*> I  NETWORK   conn366  received client metadata from <*> <*> conn366    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1840	<*> <*> <*> I  NETWORK   conn370  received client metadata from <*> <*> conn370    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1841	<*> <*> <*> I  ACCESS    conn370  Successfully authenticated as principal root on admin from client <*> <*>	11
1842	<*> <*> <*> I  NETWORK   conn370  end connection <*> <*>  <*> connections now open 	11
1843	<*> <*> <*> I  NETWORK   conn380  received client metadata from <*> <*> conn380    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1844	<*> <*> <*> I  ACCESS    conn380  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1845	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.cloudmeta_instancetypes as collection version  <unsharded>	11
1846	<*> <*> <*> I  NETWORK   conn400  received client metadata from <*> <*> conn400    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1847	<*> <*> <*> I  NETWORK   conn400  end connection <*> <*>  <*> connections now open 	11
1848	<*> <*> <*> I  NETWORK   conn404  received client metadata from <*> <*> conn404    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1849	<*> <*> <*> I  ACCESS    conn404  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1850	<*> <*> <*> I  NETWORK   conn406  received client metadata from <*> <*> conn406    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1851	<*> <*> <*> I  NETWORK   conn406  end connection <*> <*>  <*> connections now open 	11
1852	<*> <*> <*> I  SHARDING  conn404  Marking collection <*> as collection version  <unsharded>	22
1853	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.cloudmeta_storagetypes as collection version  <unsharded>	11
1854	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.cloudmeta_storagetypes with generated UUID  <*> and options    	11
1855	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.cloudmeta_storagetypes	11
1856	<*> <*> <*> I  NETWORK   conn420  received client metadata from <*> <*> conn420    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1857	<*> <*> <*> I  NETWORK   conn430  received client metadata from <*> <*> conn430    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1858	<*> <*> <*> I  NETWORK   conn432  received client metadata from <*> <*> conn432    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1859	<*> <*> <*> I  ACCESS    conn432  Successfully authenticated as principal hubble on hubble_timeseriesdb from client <*> <*>	11
1860	<*> <*> <*> I  NETWORK   conn436  received client metadata from <*> <*> conn436    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1861	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.metadata_archives as collection version  <unsharded>	11
1862	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.roles as collection version  <unsharded>	11
1863	<*> <*> <*> I  NETWORK   conn440  received client metadata from <*> <*> conn440    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1864	<*> <*> <*> I  NETWORK   conn446  received client metadata from <*> <*> conn446    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1865	<*> <*> <*> I  ACCESS    conn446  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1866	<*> <*> <*> I  SHARDING  <*>  Marking collection hubble_timeseriesdb.machinemetrics as collection version  <unsharded>	11
1867	<*> <*> <*> I  SHARDING  conn404  Marking collection hubbledb.spectrocluster_features as collection version  <unsharded>	11
1868	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.installers as collection version  <unsharded>	11
1869	<*> <*> <*> I  ACCESS    conn440  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1870	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.spectrocluster_usages as collection version  <unsharded>	11
1871	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.machines as collection version  <unsharded>	11
1872	<*> <*> <*> I  NETWORK   conn456  received client metadata from <*> <*> conn456    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1873	<*> <*> <*> I  NETWORK   conn456  end connection <*> <*>  <*> connections now open 	11
1874	<*> <*> <*> I  NETWORK   conn460  received client metadata from <*> <*> conn460    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1875	<*> <*> <*> I  ACCESS    conn460  Successfully authenticated as principal hubble on hubble_archivedb from client <*> <*>	11
1876	<*> <*> <*> I  NETWORK   conn464  received client metadata from <*> <*> conn464    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1877	<*> <*> <*> I  NETWORK   conn466  received client metadata from <*> <*> conn466    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1878	<*> <*> <*> I  ACCESS    conn466  Successfully authenticated as principal hubble on hubble_timeseriesdb from client <*> <*>	11
1879	<*> <*> <*> I  NETWORK   conn470  received client metadata from <*> <*> conn470    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1880	<*> <*> <*> I  NETWORK   conn476  received client metadata from <*> <*> conn476    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1881	<*> <*> <*> I  NETWORK   conn473  received client metadata from <*> <*> conn473    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1882	<*> <*> <*> I  ACCESS    conn473  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1883	<*> <*> <*> I  ACCESS    conn476  Successfully authenticated as principal hubble on hubbledb from client <*> <*>	11
1884	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.tenant_assets as collection version  <unsharded>	11
1885	<*> <*> <*> I  NETWORK   conn480  received client metadata from <*> <*> conn480    driver    name   <*>   version   <*>     os    type   linux   architecture   amd64     platform   <*>   	11
1886	<*> <*> <*> I  NETWORK   conn520  received client metadata from <*> <*> conn520    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1887	<*> <*> <*> I  ACCESS    conn520  Successfully authenticated as principal root on admin from client <*> <*>	11
1888	<*> <*> <*> I  NETWORK   conn520  end connection <*> <*>  <*> connections now open 	11
1889	<*> <*> <*> I  SHARDING  <*>  Marking collection hubble_timeseriesdb.podmetrics as collection version  <unsharded>	11
1890	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.packtags as collection version  <unsharded>	11
1891	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.clusterprofiles as collection version  <unsharded>	11
1892	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.notifications as collection version  <unsharded>	11
1893	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.tokens as collection version  <unsharded>	11
1894	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.notification_events as collection version  <unsharded>	11
1895	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.error_logs as collection version  <unsharded>	11
1896	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.blob as collection version  <unsharded>	11
1897	<*> <*> <*> I  TXN       <*>  transaction parameters   lsid    id  UUID  <*>    uid  BinData 0  <*>     txnNumber  <*>  autocommit  false  readConcern    level   snapshot       readTimestamp Timestamp 0  0   ninserted <*> keysInserted <*> terminationCause committed timeActiveMicros <*> timeInactiveMicros <*> numYields 0 locks   ReplicationStateTransition    acquireCount    w  <*>      Global    acquireCount    w  <*>      Database    acquireCount    w  <*>      Collection    acquireCount    w  <*>      Mutex    acquireCount    r  <*>       storage    wasPrepared 0  <*>	11
1898	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.componentevents as collection version  <unsharded>	11
1899	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.user_assets as collection version  <unsharded>	11
1900	<*> <*> <*> I  NETWORK   conn530  received client metadata from <*> <*> conn530    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1901	<*> <*> <*> I  ACCESS    conn530  Successfully authenticated as principal root on admin from client <*> <*>	11
1902	<*> <*> <*> I  NETWORK   conn530  end connection <*> <*>  <*> connections now open 	11
1903	<*> <*> <*> I  SHARDING  <*>  Marking collection hubbledb.tenant_preferences as collection version  <unsharded>	11
1904	<*> <*> <*> I  STORAGE   <*>  createCollection  hubbledb.error_logs with generated UUID  <*> and options    	11
1905	<*> <*> <*> I  INDEX     <*>  index build  done building index _id_ on ns hubbledb.error_logs	11
1906	<*> <*> <*> I  NETWORK   conn600  received client metadata from <*> <*> conn600    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1907	<*> <*> <*> I  ACCESS    conn600  Successfully authenticated as principal root on admin from client <*> <*>	11
1908	<*> <*> <*> I  NETWORK   conn600  end connection <*> <*>  <*> connections now open 	11
1909	<*> <*> <*> I  NETWORK   conn604  received client metadata from <*> <*> conn604    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1910	<*> <*> <*> I  ACCESS    conn604  Successfully authenticated as principal root on admin from client <*> <*>	11
1911	<*> <*> <*> I  NETWORK   conn604  end connection <*> <*>  <*> connections now open 	11
1912	<*> <*> <*> I  NETWORK   conn606  received client metadata from <*> <*> conn606    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1913	<*> <*> <*> I  ACCESS    conn606  Successfully authenticated as principal root on admin from client <*> <*>	11
1914	<*> <*> <*> I  NETWORK   conn606  end connection <*> <*>  <*> connections now open 	11
1915	<*> <*> <*> I  SHARDING  conn440  Marking collection hubbledb.overlords as collection version  <unsharded>	11
1916	<*> <*> <*> I  NETWORK   conn620  received client metadata from <*> <*> conn620    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1917	<*> <*> <*> I  ACCESS    conn620  Successfully authenticated as principal root on admin from client <*> <*>	11
1918	<*> <*> <*> I  NETWORK   conn620  end connection <*> <*>  <*> connections now open 	11
1919	<*> <*> <*> I  NETWORK   conn630  received client metadata from <*> <*> conn630    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1920	<*> <*> <*> I  ACCESS    conn630  Successfully authenticated as principal root on admin from client <*> <*>	11
1921	<*> <*> <*> I  NETWORK   conn630  end connection <*> <*>  <*> connections now open 	11
1922	<*> <*> <*> I  NETWORK   conn632  received client metadata from <*> <*> conn632    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1923	<*> <*> <*> I  ACCESS    conn632  Successfully authenticated as principal root on admin from client <*> <*>	11
1924	<*> <*> <*> I  NETWORK   conn632  end connection <*> <*>  <*> connections now open 	11
1925	<*> <*> <*> I  NETWORK   conn636  received client metadata from <*> <*> conn636    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1926	<*> <*> <*> I  ACCESS    conn636  Successfully authenticated as principal root on admin from client <*> <*>	11
1927	<*> <*> <*> I  NETWORK   conn636  end connection <*> <*>  <*> connections now open 	11
1928	<*> <*> <*> I  NETWORK   conn640  received client metadata from <*> <*> conn640    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1929	<*> <*> <*> I  ACCESS    conn640  Successfully authenticated as principal root on admin from client <*> <*>	11
1930	<*> <*> <*> I  NETWORK   conn640  end connection <*> <*>  <*> connections now open 	11
1931	<*> <*> <*> I  NETWORK   conn646  received client metadata from <*> <*> conn646    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1932	<*> <*> <*> I  ACCESS    conn646  Successfully authenticated as principal root on admin from client <*> <*>	11
1933	<*> <*> <*> I  NETWORK   conn646  end connection <*> <*>  <*> connections now open 	11
1934	<*> <*> <*> I  NETWORK   conn660  received client metadata from <*> <*> conn660    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1935	<*> <*> <*> I  ACCESS    conn660  Successfully authenticated as principal root on admin from client <*> <*>	11
1936	<*> <*> <*> I  NETWORK   conn660  end connection <*> <*>  <*> connections now open 	11
1937	<*> <*> <*> I  NETWORK   conn680  received client metadata from <*> <*> conn680    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1938	<*> <*> <*> I  ACCESS    conn680  Successfully authenticated as principal root on admin from client <*> <*>	11
1939	<*> <*> <*> I  NETWORK   conn680  end connection <*> <*>  <*> connections now open 	11
1940	<*> <*> <*> I  NETWORK   conn700  received client metadata from <*> <*> conn700    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1941	<*> <*> <*> I  ACCESS    conn700  Successfully authenticated as principal root on admin from client <*> <*>	11
1942	<*> <*> <*> I  NETWORK   conn700  end connection <*> <*>  <*> connections now open 	11
1943	<*> <*> <*> I  NETWORK   conn704  received client metadata from <*> <*> conn704    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1944	<*> <*> <*> I  ACCESS    conn704  Successfully authenticated as principal root on admin from client <*> <*>	11
1945	<*> <*> <*> I  NETWORK   conn704  end connection <*> <*>  <*> connections now open 	11
1946	<*> <*> <*> I  NETWORK   conn706  received client metadata from <*> <*> conn706    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1947	<*> <*> <*> I  ACCESS    conn706  Successfully authenticated as principal root on admin from client <*> <*>	11
1948	<*> <*> <*> I  NETWORK   conn706  end connection <*> <*>  <*> connections now open 	11
1949	<*> <*> <*> I  NETWORK   conn720  received client metadata from <*> <*> conn720    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1950	<*> <*> <*> I  ACCESS    conn720  Successfully authenticated as principal root on admin from client <*> <*>	11
1951	<*> <*> <*> I  NETWORK   conn720  end connection <*> <*>  <*> connections now open 	11
1952	<*> <*> <*> I  NETWORK   conn730  received client metadata from <*> <*> conn730    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1953	<*> <*> <*> I  ACCESS    conn730  Successfully authenticated as principal root on admin from client <*> <*>	11
1954	<*> <*> <*> I  NETWORK   conn730  end connection <*> <*>  <*> connections now open 	11
1955	<*> <*> <*> I  NETWORK   conn732  received client metadata from <*> <*> conn732    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1956	<*> <*> <*> I  ACCESS    conn732  Successfully authenticated as principal root on admin from client <*> <*>	11
1957	<*> <*> <*> I  NETWORK   conn732  end connection <*> <*>  <*> connections now open 	11
1958	<*> <*> <*> I  NETWORK   conn736  received client metadata from <*> <*> conn736    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1959	<*> <*> <*> I  ACCESS    conn736  Successfully authenticated as principal root on admin from client <*> <*>	11
1960	<*> <*> <*> I  NETWORK   conn736  end connection <*> <*>  <*> connections now open 	11
1961	<*> <*> <*> I  NETWORK   conn740  received client metadata from <*> <*> conn740    driver    name   nodejs   version   <*>     os    type   Linux   name   linux   architecture   <*>   version   <*>     platform   <*> <*>  LE  <*>  <*>   	11
1962	<*> <*> <*> I  ACCESS    conn740  Successfully authenticated as principal root on admin from client <*> <*>	11
1963	<*> <*> <*> I  NETWORK   conn740  end connection <*> <*>  <*> connections now open 	11
1964	     END logs for container mongo of pod <*>     	11
1965	> <*> start <*>	11
1966	> forever <*>	11
1967	 <*> <*>     <*> not set. Defaulting to  <*>	11
1968	 <*> <*>     <*> not set. Your script will exit if it does not stay up for at least <*>	11
1969	Using mongo port  <*>	11
1970	Starting up <*>	11
1971	The cluster domain  cluster.local  was successfully verified.	11
1972	<*> method will no longer be available in the next major release <*> as MongoDB <*> will only allow auth against users in the admin db and will no longer allow multiple credentials on a socket. Please authenticate using MongoClient.connect with auth credentials.	2991
1973	Error in workloop No pods are currently running  probably just give them some time.	11
1974	Pod has been <*> for replica set initialization	11
1975	initReplSet <*> <*>	11
1976	initial rsConfig is   _id   rs0  	11
1977	version  <*> 	22
1978	protocolVersion  <*> 	22
1979	writeConcernMajorityJournalDefault  true 	22
1980	members 	22
1981	    _id  0 	22
1982	host   <*> <*>  	22
1983	arbiterOnly  false 	22
1984	buildIndexes  true 	22
1985	hidden  false 	22
1986	priority  <*> 	22
1987	tags     	22
1988	slaveDelay  0 	22
1989	votes  <*>     	22
1990	settings 	22
1991	  chainingAllowed  true 	22
1992	heartbeatIntervalMillis  <*> 	22
1993	heartbeatTimeoutSecs  <*> 	22
1994	electionTimeoutMillis  <*> 	22
1995	catchUpTimeoutMillis  <*> 	22
1996	catchUpTakeoverDelayMillis  <*> 	22
1997	getLastErrorModes     	22
1998	getLastErrorDefaults    w  <*>  wtimeout  0   	22
1999	replicaSetId  <*>    	11
2000	replSetReconfig   _id   rs0  	11
2001	replicaSetId  <*>   	11
2002	configsvr  false  	11
