0	Cluster Name <*>	240
1	<*>	480
2	Pod Name <*>	80
3	Namespace <*>	120
4	Node Name <*>	160
5	Pod Labels component <*> tier control-plane	45
6	Cluster Uid <*>	280
7	Tenant Uid <*>	320
8	Project Uid <*>	360
9	Cloud Type vsphere	400
10	Cluster Env cleanup	440
11	Time <*> <*> <*> <*> <*> UTC m <*>	440
12	...........................................	1531
13	Container Name <*>	396
14	Image Id <*> <*>	583
15		2438
16	Pod Events	183
17	Pod Labels k8s-app <*> <*> <*>	15
18	Watch channel closed by remote <*> recreate watcher ListRoot <*>	9251
19	Terminating main client watcher loop	132
20	Main client watcher loop	132
21	Container Name coredns	22
22	Pod Labels <*> <*> k8s-app <*> <*> <*>	35
23	Pod Labels <*> <*> <*> <*> k8s-app <*>	5
24	Pod Labels k8s-app <*> <*> <*> <*> <*>	5
25	Summarising <*> dataplane reconciliation loops over <*> avg <*> longest <*> <*>	17292
26	Using autodetected IPv4 address on interface eth0 <*>	18733
27	Pod Labels component etcd tier control-plane	15
28	Container Name etcd	33
29	<*> <*> <*> <*> <*> <*> <*> I | etcdserver/api/etcdhttp <*> OK status code <*>	112475
30	<*> <*> <*> <*> <*> <*> <*> I | etcdserver start to snapshot applied <*> lastsnap <*>	1782
31	<*> <*> <*> <*> <*> <*> <*> I | etcdserver saved snapshot at index <*>	1782
32	<*> <*> <*> <*> <*> <*> <*> I | etcdserver compacted raft log at <*>	1782
33	<*> <*> <*> <*> <*> <*> <*> I | pkg/fileutil purged file <*> successfully	2013
34	<*> <*> <*> <*> <*> <*> <*> I | mvcc store.index compact <*>	3894
35	<*> <*> <*> <*> <*> <*> <*> I | mvcc finished scheduled compaction at <*> took <*>	3894
36	Discovered VM using normal UUID format	3696
37	<*> <*> <*> <*> <*> <*> <*> W | rafthttp the clock difference against peer <*> is too high <*> > <*>	34815
38	parsed scheme <*>	12513
39	ccResolverWrapper sending update to cc https <*> <*> <nil> 0 <nil> <nil> <nil>	12510
40	ClientConn switching balancer to pick_first	12507
41	Error proxying data from backend to client tls use of closed connection	1443
42	Pod Labels app <*> <*> <*> <*> <*> role <*>	5
43	Throttling request took <*> request GET https <*> <*> <*>	12452
44	Error while processing Node <*> failed to allocate cidr from cluster cidr at idx 0 CIDR allocation failed there are no remaining CIDRs left to allocate in the accepted range	2574
45	Event occurred object <*> kind Node apiVersion <*> type Normal reason CIDRNotAvailable message Node <*> status is now CIDRNotAvailable	2574
46	Pod Labels role <*> app <*> <*> <*> <*> <*>	5
47	Summarising <*> dataplane reconciliation loops over <*> avg <*> longest <*> <*> <*> <*> <*>	352
48	Auditing failed of request encoding failed <*> Kind DeleteOptions is unstructured and is not suitable for converting to <*>	2640
49	Watch error received from Upstream ListRoot <*> error too old resource version <*> <*>	121
50	Full resync is required ListRoot <*>	121
51	Pod Labels app <*> <*> <*> role <*>	5
52	forcing resync	1254
53	Watch close <*> <*> total 0 items received	2420
54	Connecting to unix <*>	186402
55	Pod Labels <*> <*> <*> <*> role <*> app <*>	5
56	Pod Labels control-plane <*> <*> <*>	5
57	Pod Labels <*> <*> control-plane controller-manager	5
58	Pod Labels <*> infrastructure-vsphere control-plane controller-manager <*> <*>	5
59	Pod Labels app spectro component cluster-management-agent log-regex logrus-text module ally <*> <*> <*> proxy	5
60	Pod Labels control-plane controller-manager <*> <*> <*> <*>	5
61	Pod Labels <*> cluster-api control-plane controller-manager <*> <*>	5
62	Pod Labels <*> <*> <*> <*> <*> infrastructure-metal3 control-plane controller-manager	5
63	Pod Labels <*> <*> broadcastjob-name <*> <*> proxy <*> host <*> skip	10
64	Pod Labels <*> <*> <*> bootstrap-kubeadm control-plane controller-manager	5
65	Pod Labels <*> skip <*> <*> broadcastjob-name <*> <*> proxy <*> host	5
66	Container Name cluster-management-agent	11
67	Pod Labels control-plane controller-manager <*> <*> <*> proxy	5
68	Container Name manager	88
69	Container Name node-agent	33
70	Cached <*> logs as events cluster <*>	5302
71	Pushing <*> cached event s to hubble cluster <*>	3839
72	Starting node agent controller	33
73	Pushed >>> <*> event s cluster <*>	3839
74	Found <*> spectro node tasks to reconcile	33
75	upgrade agent No change in version is required as current version and newVersion is <*> same cluster <*>	594
76	Reconciling <*> spectro node task	33
77	<*> <*> <*> register aws provider <*>	11
78	Running cmd exec task <*>	33
79	<*> <*> <*> register azure provider <*>	11
80	Creating <*> file	33
81	Executing command <*> +x print-logs.sh	33
82	<*> <*> <*> register maas provider <*>	11
83	Executing command <*> print-logs.sh	33
84	>>>>>>> Watching SpectroCluster CRD in <*> namespace cluster <*>	143
85	<*> <*> <*> register vsphere provider	11
86	<*> <*> <*> register gcp provider <*>	11
87	<*> <*> <*> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	77
88	>>>>>>>>>>>>>>>>> Event type ADDED SpectroClusterStatus conditions type ProviderReady status True lastProbeTime <*> <*> <*> lastTransitionTime <*> <*> <*> reason CRDInstalled message cluster-api providers crd installed and controllers deployed cluster <*>	286
89	Skipping to post <*> conditions to hubble as there is no difference in cached conditions cluster <*>	286
90	<*> <*> <*> register generic provider	11
91	<*> <*> <*> <*>	220
92	metrics server is starting to listen addr <*> <*>	11
93	register field index	11
94	<*> <*> <*> register openstack provider	11
95	new clientset registry	11
96	watch in a single namespace namespace <*>	11
97	ExecSync for <*> with command <*> <*> <*> and timeout <*> s	18194
98	setup webhook	11
99	Exec process <*> exits with exit code 0 and error <nil>	22121
100	resync period every <*>	11
101	Finish piping stdout of container exec <*>	22121
102	registering webhook path <*>	176
103	ExecSync for <*> returns with exit code 0	22132
104	Finish piping stderr of container exec <*>	22132
105	metrics server is starting to listen addr <*>	11
106	Registered webhook handler <*>	154
107	Portforward for <*> port	4180
108	starting manager	22
109	Portforward for <*> returns URL http <*> <*>	4180
110	attempting to acquire leader lease <*>	22
111	Executing port forwarding command <*> <*> TCP4 localhost <*> in network namespace host	4180
112	starting palette metrics server at <*>	11
113	starting metrics server path <*>	22
114	successfully acquired lease <*>	22
115	Starting EventSource controller spectrocluster source Type metadata creationTimestamp null spec clusterProfileTemplate clusterConfig machineManagementConfig machineHealthConfig status	11
116	Applying rbac rules cluster <*>	660
117	error forwarding port <*> to pod <*> uid failed to execute portforward in network namespace host socat command returns error EOF stderr	2728
118	Starting EventSource controller pack source Type metadata creationTimestamp null spec packConfigSpec layer packRef layer server name tag status	11
119	<*> items n cluster <*>	660
120	error forwarding port <*> to pod <*> uid failed to execute portforward in network namespace host socat command returns error exit status <*> stderr <*> <*> <*> <*> socat <*> E write <*> <*> <*> Broken pipe n	418
121	Starting Controller controller pack	11
122	Starting workers controller pack worker count <*>	11
123	Starting EventSource controller spectrocluster source Type metadata creationTimestamp null spec clusterConfig region endpointAccess status	11
124	Starting EventSource controller spectrocluster source Type metadata creationTimestamp null spec cloudAccountRef null clusterConfig network networkName ipPool nameserver controlPlaneEndpoint placement network networkName ipPool nameserver machinePoolConfig null status nodeImage	11
125	Starting EventSource controller spectrocluster source Type metadata creationTimestamp null spec clusterConfig subscriptionId resourceGroup location sshKey controlPlaneSubnet workerSubnet status vhdImage images	11
126	Starting Controller controller spectrocluster	11
127	ExecSync for <*> with command <*> <*> and timeout <*> s	3916
128	Unable to retrieve pull secret <*> for <*> due to secret <*> not found. The image pull may not succeed.	1606
129	Starting workers controller spectrocluster worker count <*>	11
130	>>>>>>>>>>>>>>>>> Event type MODIFIED SpectroClusterStatus conditions type ProviderReady status True lastProbeTime <*> <*> <*> lastTransitionTime <*> <*> <*> reason CRDInstalled message cluster-api providers crd installed and controllers deployed cluster <*>	220
131	Summarising 6 dataplane reconciliation loops over <*> avg <*> longest <*> <*>	165
132	detecting env spectrocluster Namespace <*> Name <*>	742
133	Updated lastProbeTime <*> <*> <*> lastTransitionTime <*> <*> <*> message cluster-api providers crd installed and controllers deployed reason CRDInstalled status True type ProviderReady conditions for spectro cluster cluster <*>	220
134	Finish port forwarding for <*> port <*>	814
135	detect env done spectrocluster Namespace <*> Name <*> env cleanup	741
136	reconcile delete target spectrocluster Namespace <*> Name <*>	740
137	reconcileCAPI spectrocluster Namespace <*> Name <*>	739
138	createCAPISecret spectrocluster Namespace <*> Name <*>	738
139	deployment ready spectrocluster Namespace <*> Name <*> name <*>	5896
140	deployment ready spectrocluster Namespace <*> Name <*> name cert-manager	737
141	>>>>>>> Watching event s in <*> namespace cluster <*>	132
142	initialize webhook	11
143	Starting <*>	11
144	reconcileCAPI done spectrocluster Namespace <*> Name <*>	737
145	Waiting for caches to sync for <*>	11
146	pivot resource back to prepare cleanup spectrocluster Namespace <*> Name <*>	737
147	Starting reflector <*> 0s from <*> <*>	44
148	getForcePivotStatus	737
149	Listing and watching <*> from <*> <*>	110
150	reconcileTargetKubeClient spectrocluster Namespace <*> Name <*>	737
151	fetching kubeconfig with key spectrocluster Namespace <*> Name <*> key Namespace <*> Name <*>	737
152	reconcileTargetKubeClient done spectrocluster Namespace <*> Name <*>	737
153	target namespace already marked for deletion	737
154	reconcilePivotBack spectrocluster Namespace <*> Name <*>	737
155	start to scale down controller deployments spectrocluster Namespace <*> Name <*>	737
156	scale down controller success spectrocluster Namespace <*> Name <*> <*> <*>	4419
157	Secret <*> added	11
158	ValidatingWebhookConfiguration <*> added	11
159	MutatingWebhookConfiguration <*> added	11
160	CustomResourceDefinition <*> added	22
161	scale down controller deployments done spectrocluster Namespace <*> Name <*>	734
162	caches populated	110
163	pivot move resource spectrocluster Namespace <*> Name <*>	733
164	Caches are synced for <*>	11
165	Reconciler error error failed to pivot move resource back failed to pause capi cluster error updating the pause status for <*> Kind <*> Internal error occurred failed calling webhook <*> Post https <*> <*> <*> dial tcp <*> <*> connect connection refused controller spectrocluster name <*> namespace <*>	732
166	Started <*>	11
167	Starting to sync webhook certs and configurations	33
168	cert is invalid or expiring regenerating a new one	11
169	Secret <*> updated	11
170	Cert writer update secret <*> resourceVersion from <*> to <*>	11
171	cert directory doesn t exist <*>	11
172	cloud setup already done skipping spectrocluster Namespace <*> Name <*>	726
173	performed write of new data to ts data directoryts <*>	11
174	Find path <*> not in handlers map <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*> <*>	33
175	ValidatingWebhookConfiguration <*> updated	22
176	Finished to sync webhook certs and configurations	33
177	MutatingWebhookConfiguration <*> update	22
178	wait webhook ready	11
179	Starting reflector <*> <*> from <*> <*>	66
180	>>>>>>>>> Watching Namespaces cluster <*>	143
181	Error proxying data from backend to client write tcp <*> <*> <*> write broken pipe	132
182	current pathstarget <*> <*> <*> cert.pem key.pem tls.crt tls.key	22
183	new pathstarget <*> <*> <*> cert.pem key.pem tls.crt tls.key	22
184	paths to removetarget <*>	22
185	Error proxying data from backend to client write tcp <*> <*> <*> write connection reset by peer	22
186	>>>>>>> Watching <*> in <*> namespace cluster <*>	264
187	no update required for target <*>	22
188	Error proxying data from client to backend tls use of closed connection	22
189	Normal message <*> became leader object kind ConfigMap namespace <*> name <*> uid <*> apiVersion <*> resourceVersion <*> reason LeaderElection	11
190	<*> <*> <*> <*> <*> <*> <*> I | wal segmented wal file <*> is created	231
191	starting webhook server	11
192	Updated current TLS certificate	11
193	serving webhook server host <*> port <*>	11
194	Starting certificate watcher	11
195	setup controllers	11
196	Starting AdvancedCronJob Controller	11
197	Starting EventSource controller <*> source Type metadata creationTimestamp null spec schedule template status	11
198	Starting EventSource controller <*> source Type metadata creationTimestamp null spec template metadata creationTimestamp null spec containers null completionPolicy failurePolicy status active 0 succeeded 0 failed 0 desired 0 phase	22
199	Starting EventSource controller <*> source Type metadata creationTimestamp null spec containers null status	11
200	Starting EventSource controller <*> source Type metadata creationTimestamp null spec status daemonEndpoints kubeletEndpoint Port 0 nodeInfo machineID systemUUID bootID kernelVersion osImage containerRuntimeVersion kubeletVersion kubeProxyVersion operatingSystem architecture	11
201	<*> <*> <*> <*> <*> <*> <*> W | etcdserver read-only range request key <*> with result range_response_count <*> size <*> took too long <*> to execute	308
202	custom resource gate not found groupVersionKind <*> Kind StatefulSet in discovery the server could not find the requested resource	11
203	Starting EventSource controller <*> source Type metadata creationTimestamp null spec template metadata creationTimestamp null spec containers null status	11
204	Starting Controller controller <*>	22
205	Starting workers controller <*> worker count <*>	22
206	received request webhook <*> UID <*> kind group <*> version <*> kind BroadcastJob resource group <*> version <*> resource broadcastjobs	22
207	wrote response webhook <*> UID <*> allowed true result metadata code <*>	11
208	wrote response webhook <*> UID <*> allowed true result metadata reason allowed to be admitted code <*>	11
209	<*> has <*> nodes remaining to schedule pods	121
210	Before broadcastjob reconcile <*> desired <*> active 0 failed 0	22
211	creating pod on node <*>	33
212	Controller <*> created pod <*>	33
213	Normal message Created pod <*> object kind BroadcastJob namespace <*> name <*> uid <*> apiVersion <*> resourceVersion <*> reason SuccessfulCreate	33
214	After broadcastjob reconcile <*> desired <*> active <*> failed 0	110
215	Updating job <*> status <*> Conditions <*> nil StartTime <*> <*> CompletionTime <*> nil Active <*> Succeeded 0 Failed 0 Desired <*> Phase running	66
216	Successfully Reconciled controller <*> name <*> namespace <*>	110
217	Before broadcastjob reconcile <*> desired <*> active <*> failed 0	99
218	Updating job <*> status <*> Conditions <*> nil StartTime <*> <*> CompletionTime <*> nil Active <*> Succeeded <*> Failed 0 Desired <*> Phase running	44
219	job <*> is Complete Job completed <*> pods succeeded 0 pods failed	11
220	Job <*> is Complete will be deleted after <*> seconds	11
221	After broadcastjob reconcile <*> desired <*> active 0 failed 0	11
222	Updating job <*> status <*> Conditions <*> <*> Type Complete Status True LastProbeTime <*> Time time.Time wall <*> ext <*> loc time.Location <*> LastTransitionTime <*> Time time.Time wall <*> ext <*> loc time.Location <*> Reason Complete Message Job completed <*> pods succeeded 0 pods failed StartTime <*> <*> CompletionTime <*> <*> Active 0 Succeeded <*> Failed 0 Desired <*> Phase completed	11
223	Normal message Job <*> is completed <*> pods succeeded 0 pods failed object kind BroadcastJob namespace <*> name <*> uid <*> apiVersion <*> resourceVersion <*> reason JobComplete	11
224	<*> <*> <*> <*> Successfully assigned <*> to <*>	9
225	<*> <*> <*> <*> Pulling image <*> <*>	29
226	<*> <*> <*> <*> Successfully pulled image <*> <*> in <*>	25
227	<*> <*> <*> <*> Created container manager	6
228	<*> <*> <*> <*> Started container manager	5
229	<*> <*> <*> <*> Readiness probe failed Get http <*> <*> dial tcp <*> <*> connect connection refused	4
230	<*> <*> <*> <*> <*> <*> <*> W | etcdserver read-only range request key <*> range_end <*> count_only true with result range_response_count 0 size <*> took too long <*> to execute	99
231	<*> <*> <*> <*> <*> <*> <*> W | etcdserver failed to send out heartbeat on time exceeded the <*> timeout for <*> to <*>	22
232	<*> <*> <*> <*> <*> <*> <*> W | etcdserver server is likely overloaded	22
233	Error proxying data from backend to client read tcp <*> <*> <*> read connection reset by peer	11
234	error forwarding port <*> to pod <*> uid failed to execute portforward in network namespace host socat command returns error write tcp <*> <*> <*> use of closed network connection stderr	110
235	error forwarding port <*> to pod <*> uid failed to execute portforward in network namespace host socat command returns error write tcp <*> <*> <*> write broken pipe stderr	88
236	Local endpoint updated id WorkloadEndpoint node <*> orchestrator k8s workload <*> name eth0	165
237	id <orchestrator_id k8s workload_id <*> endpoint_id eth0 > endpoint <state active name <*> profile_ids <*> profile_ids <*> ipv4_nets <*> >	165
238	Updating per-endpoint chains. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	165
239	Queueing update of chain. chainName <*> ipVersion <*> table filter	594
240	Updating endpoint routes. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	165
241	Chain became referenced marking it for programming chainName <*>	176
242	Skipping configuration of interface because it is oper down. ifaceName <*>	55
243	Re-evaluated workload endpoint status adminUp true failed false known true operUp false status down workloadEndpointID proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	55
244	Storing endpoint status update ipVersion <*> status down workload proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	55
245	Trying to connect to netlink	55
246	Endpoint down for at least one IP version id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0 ipVersion <*> status down	55
247	Reporting combined status. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0 status down	55
248	Linux interface addrs changed. addrs set.mapSet ifaceName <*>	99
249	Linux interface state changed. ifIndex <*> ifaceName <*> state up	55
250	&intdataplane.ifaceAddrsUpdate Name <*> Addrs set.mapSet	99
251	Interface addrs changed. update &intdataplane.ifaceAddrsUpdate Name <*> Addrs set.mapSet	99
252	&intdataplane.ifaceUpdate Name <*> State up Index <*>	55
253	Workload interface came up marking for reconfiguration. ifaceName <*>	55
254	Workload interface state changed marking for status update. ifaceName <*>	55
255	Applying <*> configuration to interface. ifaceName <*>	165
256	Re-evaluated workload endpoint status adminUp true failed false known true operUp true status up workloadEndpointID proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	165
257	Storing endpoint status update ipVersion <*> status up workload proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	165
258	Endpoint up for at least one IP version id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0 ipVersion <*> status up	165
259	Reporting combined status. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0 status up	165
260	Netlink address update. addr fe80 ecee eeff feee eeee exists true ifIndex <*>	55
261	Linux interface addrs changed. addrs set.mapSet fe80 ecee eeff feee eeee set.empty ifaceName <*>	55
262	&intdataplane.ifaceAddrsUpdate Name <*> Addrs set.mapSet fe80 ecee eeff feee eeee set.empty	55
263	Interface addrs changed. update &intdataplane.ifaceAddrsUpdate Name <*> Addrs set.mapSet fe80 ecee eeff feee eeee set.empty	55
264	Local endpoint deleted id WorkloadEndpoint node <*> orchestrator k8s workload <*> name eth0	44
265	id <orchestrator_id k8s workload_id <*> endpoint_id eth0 >	44
266	Workload removed deleting its chains. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	44
267	Queuing deletion of chain. chainName <*> ipVersion <*> table filter	132
268	Workload removed deleting old state. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	44
269	Chain no longer referenced marking it for removal chainName <*>	132
270	Re-evaluated workload endpoint status adminUp false failed false known false operUp false status workloadEndpointID proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	44
271	Storing endpoint status update ipVersion <*> status workload proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	44
272	Removing conntrack flows ip <*>	44
273	Reporting endpoint removed. id proto.WorkloadEndpointID OrchestratorId k8s WorkloadId <*> EndpointId eth0	44
274	<*> <*> <*> <*> <*> <*> <*> W | etcdserver read-only range request key <*> with result range_response_count 0 size <*> took too long <*> to execute	22
275	<*> <*> <*> bird KIF Received address message for unknown interface <*>	33
276	Linux interface state changed. ifIndex <*> ifaceName <*> state down	44
277	&intdataplane.ifaceUpdate Name <*> State down Index <*>	44
278	Netlink address update. addr fe80 ecee eeff feee eeee exists false ifIndex <*>	44
279	Linux interface addrs changed. addrs <nil> ifaceName <*>	44
280	&intdataplane.ifaceAddrsUpdate Name <*> Addrs set.Set nil	44
281	Interface addrs changed. update &intdataplane.ifaceAddrsUpdate Name <*> Addrs set.Set nil	44
282	Summarising <*> dataplane reconciliation loops over <*> avg <*> longest <*> <*> <*>	22
283	Summarising 6 dataplane reconciliation loops over <*> avg <*> longest <*> <*> <*> <*> <*>	11
284	id <name <*> > profile <inbound_rules <action allow rule_id RlLgQRgidk_vSbTC > outbound_rules <action allow rule_id <*> > >	11
285	Queueing update of chain. chainName <*> ipVersion <*> table mangle	22
286	id <name <*> > profile <>	11
287	id <name <*> >	22
288	Queuing deletion of chain. chainName <*> ipVersion <*> table mangle	22
289	Summarising <*> dataplane reconciliation loops over <*> avg <*> longest <*> <*> <*> <*>	11
290	Upgrading palette as current version is <*> and new version is <*> cluster <*>	11
291	Successfully audited via rest Upgrading palette as current version is <*> and new version is <*> Normal cluster <*>	11
292	Pushing 4 cached event s to hubble cluster <*>	33
293	Pushed >>> 4 event s cluster <*>	33
294	Cached 6 logs as events cluster <*>	11
295	Event occurred object <*> kind Deployment apiVersion <*> type Normal reason <*> message Scaled up replica set <*> to <*>	33
296	Event occurred object <*> kind ReplicaSet apiVersion <*> type Normal reason SuccessfulCreate message Created pod <*>	33
297	Event occurred object <*> kind Deployment apiVersion <*> type Normal reason <*> message Scaled down replica set <*> to 0	22
298	Event occurred object <*> kind ReplicaSet apiVersion <*> type Normal reason SuccessfulDelete message Deleted pod <*>	22
299	<*> <*> <*> <*> <*> <*> <*> Handling the message <*>	11
300	NATS Received message with key <*> cluster <*>	11
301	Successfully audited Received message with key <*> nats Normal cluster <*>	11
302	Collecting logs on demand cluster <*>	11
303	Cluster Feature Log Fetcher Request noOfLines <*> duration <*> k8sRequest namespace <*> <*> nodeRequest nodeLog <*> <*> cluster <*>	11
304	Successfully audited Started processing log collection request at <*> <*> <*> <*> <*> UTC m <*> log Normal cluster <*>	11
305	Applying crony task controller by getting it from scar cluster <*>	11
306	Creating crony task controller deployment with version <*> cluster <*>	11
307	Fetching manifest of service crony and version <*> for action apply cluster <*>	11
308	Applying manifest with file name <*> cluster <*>	11
309	Applying on demand log fetch spectro node task cluster <*>	11
310	Fetching manifest from service crony and version <*> for action resources with file name <*> cluster <*>	22
311	Applying spectro node task <*> cluster <*>	11
312	Finding spectro node task <*> cluster <*>	11
313	spectro node task <*> is not found cluster <*>	11
314	Creating spectro node task <*> cluster <*>	11
315	Created spectro node task <*> cluster <*>	11
316	Applied spectro node task <*> cluster <*>	11
317	Applied on demand log fetch spectro node task cluster <*>	11
318	Checking for replica to be in running state for deployment <*> in iteration 0 cluster <*>	11
319	Waiting for pod to be in running state for deployment <*> cluster <*>	66
320	Checking for replica to be in running state for deployment <*> in iteration <*> cluster <*>	44
321	Checking for replica to be in running state for deployment <*> in iteration 4 cluster <*>	11
322	Checking for replica to be in running state for deployment <*> in iteration 6 cluster <*>	11
323	Deployment <*> is in running state with <*> replica cluster <*>	11
324	Applying on demand log fetch broadcast job cluster <*>	11
325	Deleting <*> broadcast job cluster <*>	11
326	Deleted <*> broadcast job cluster <*>	11
327	Checking for deletion of all log grep pods in iteration 0 cluster <*>	11
328	Old Log grep pods on all nodes have been deleted successfully in iteration 0 cluster <*>	11
329	Applying <*> broadcast job cluster <*>	11
330	Finding broadcast job <*> cluster <*>	55
331	broadcast job <*> is not found cluster <*>	11
332	Creating <*> broadcast job cluster <*>	11
333	Created <*> broadcast job cluster <*>	11
334	Applied <*> broadcast job cluster <*>	11
335	Applied on demand log fetch broadcast job cluster <*>	11
336	Checking for completed log grep pods in iteration 0 cluster <*>	11
337	Found broadcast job <*> cluster <*>	44
338	Waiting for pod containing node logs to get completed... cluster <*>	33
339	Checking for completed log grep pods in iteration <*> cluster <*>	33
340	Considering pod <*> with phase Running cluster <*>	440
341	Pod <*> doesn t have any label. Thus skipping to parse logs. cluster <*>	33
342	Considering pod <*> with phase Succeeded cluster <*>	33
343	error forwarding port <*> to pod <*> uid failed to execute portforward in network namespace host socat command returns error write tcp <*> <*> <*> write connection reset by peer stderr	22
344	topologymanager Topology Admit Handler	66
345	operationExecutor.VerifyControllerAttachedVolume started for volume <*> UniqueName <*> pod <*> UID <*>	66
346	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd <*> Started Kubernetes transient mount for <*>	66
347	RunPodsandbox for &PodSandboxMetadata Name <*> Uid <*> Namespace <*> Attempt 0	66
348	Calico CNI found existing endpoint & WorkloadEndpoint <*> <*> <*> <*> <*> <*> 0 <*> <*> <*> <*> <*> UTC <nil> <nil> map control-plane <*> <*> <*> <*> <*> <*> k8s <*> default map k8s <*> <*> eth0 <*> <*> <*> <*> TCP <*> metrics TCP <*> health TCP <*> ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*>	22
349	Extracted identifiers for CmdAddK8s ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*>	22
350	Calico CNI IPAM request count IPv4 <*> IPv6 0 ContainerID <*> HandleID <*> Workload <*>	22
351	Auto assigning IP ContainerID <*> HandleID <*> Workload <*> assignArgs ipam.AutoAssignArgs Num4 <*> Num6 0 HandleID string <*> Attrs map string string namespace <*> node <*> pod <*> timestamp <*> <*> <*> <*> <*> UTC Hostname <*> IPv4Pools <*> IPv6Pools <*> MaxBlocksPerHost 0 HostReservedAttrIPv4s ipam.HostReservedAttr nil HostReservedAttrIPv6s ipam.HostReservedAttr nil	22
352	About to acquire host-wide IPAM lock. source ipam_plugin.go <*>	22
353	Acquired host-wide IPAM lock. source ipam_plugin.go <*>	22
354	Auto-assign <*> ipv4 0 ipv6 addrs for host <*>	22
355	Looking up existing affinities for host handle <*> host <*>	22
356	Looking up existing affinities for host host <*>	22
357	Trying affinity for <*> host <*>	22
358	Attempting to load block cidr <*> host <*>	22
359	Affinity is confirmed and block has been loaded cidr <*> host <*>	22
360	Attempting to assign <*> addresses from block block <*> handle <*> host <*>	22
361	Creating new handle <*>	22
362	Writing block in order to claim IPs block <*> handle <*> host <*>	22
363	Successfully claimed IPs <*> block <*> handle <*> host <*>	22
364	Auto-assigned <*> out of <*> IPv4s <*> handle <*> host <*>	22
365	Released host-wide IPAM lock. source ipam_plugin.go <*>	22
366	Calico CNI IPAM assigned addresses IPv4 <*> IPv6 ContainerID <*> HandleID <*> Workload <*>	22
367	Populated endpoint ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*> endpoint <*> TypeMeta <*> Kind WorkloadEndpoint APIVersion <*> ObjectMeta <*> Name <*> GenerateName <*> Namespace <*> SelfLink UID <*> ResourceVersion <*> Generation 0 CreationTimestamp <*> Time time.Time wall 0x0 ext <*> loc time.Location <*> DeletionTimestamp <*> nil DeletionGracePeriodSeconds <*> nil Labels map string string control-plane <*> <*> <*> <*> <*> <*> k8s <*> default Annotations map string string nil OwnerReferences <*> nil Finalizers string nil ClusterName ManagedFields <*> nil Spec v3.WorkloadEndpointSpec Orchestrator k8s Workload Node <*> ContainerID Pod <*> Endpoint eth0 IPNetworks string <*> IPNATs <*> nil IPv4Gateway IPv6Gateway Profiles string <*> <*> InterfaceName <*> MAC Ports <*> <*> Name <*> Protocol numorstring.Protocol Type <*> NumVal 0x0 StrVal TCP Port <*> <*> Name metrics Protocol numorstring.Protocol Type <*> NumVal 0x0 StrVal TCP Port <*> <*> Name health Protocol numorstring.Protocol Type <*> NumVal 0x0 StrVal TCP Port <*>	22
368	Calico CNI using IPs <*> ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*>	22
369	Setting the host side veth name to <*> ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*>	22
370	Disabling IPv4 forwarding ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*>	22
371	<*> <*> <*> Oct <*> <*> <*> <*> <*> kernel <*> IPv6 ADDRCONF NETDEV_UP <*> link is not ready	22
372	<*> <*> <*> Oct <*> <*> <*> <*> <*> kernel <*> IPv6 ADDRCONF NETDEV_UP eth0 link is not ready	22
373	<*> <*> <*> Oct <*> <*> <*> <*> <*> kernel <*> IPv6 ADDRCONF NETDEV_CHANGE eth0 link becomes ready	22
374	<*> <*> <*> Oct <*> <*> <*> <*> <*> kernel <*> IPv6 ADDRCONF NETDEV_CHANGE <*> link becomes ready	22
375	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd-udevd <*> link_config autonegotiation is unset or enabled the speed and duplex are not writable.	22
376	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd-networkd <*> <*> Link UP	22
377	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd-networkd <*> <*> Gained carrier	22
378	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd-timesyncd <*> Network configuration changed trying to establish connection.	44
379	<*> <*> <*> Oct <*> <*> <*> <*> <*> <*> <*> WARNING Unknown index <*> <*> <*> interface list	22
380	Added Mac interface name and active container ID to endpoint ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*> endpoint <*> TypeMeta <*> Kind WorkloadEndpoint APIVersion <*> ObjectMeta <*> Name <*> GenerateName <*> Namespace <*> SelfLink UID <*> ResourceVersion <*> Generation 0 CreationTimestamp <*> Time time.Time wall 0x0 ext <*> loc time.Location <*> DeletionTimestamp <*> nil DeletionGracePeriodSeconds <*> nil Labels map string string control-plane <*> <*> <*> <*> <*> <*> k8s <*> default Annotations map string string nil OwnerReferences <*> nil Finalizers string nil ClusterName ManagedFields <*> nil Spec v3.WorkloadEndpointSpec Orchestrator k8s Workload Node <*> ContainerID <*> Pod <*> Endpoint eth0 IPNetworks string <*> IPNATs <*> nil IPv4Gateway IPv6Gateway Profiles string <*> <*> InterfaceName <*> MAC a2 <*> <*> f4 <*> 3a Ports <*> <*> Name <*> Protocol numorstring.Protocol Type <*> NumVal 0x0 StrVal TCP Port <*> <*> Name metrics Protocol numorstring.Protocol Type <*> NumVal 0x0 StrVal TCP Port <*> <*> Name health Protocol numorstring.Protocol Type <*> NumVal 0x0 StrVal TCP Port <*>	22
381	Wrote updated endpoint to datastore ContainerID <*> Namespace <*> Pod <*> WorkloadEndpoint <*>	22
382	starting signal loop namespace <*> path <*> pid <*>	132
383	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd-timesyncd <*> Synchronized to time server <*> <*> <*> .	44
384	RunPodSandbox for &PodSandboxMetadata Name <*> Uid <*> Namespace <*> Attempt 0 returns sandbox id <*>	66
385	PullImage <*> <*>	66
386	ImageUpdate event &ImageUpdate Name <*> <*> Labels map string string <*> managed XXX_unrecognized	198
387	ImageUpdate event &ImageUpdate Name sha256 <*> Labels map string string <*> managed XXX_unrecognized	66
388	PullImage <*> <*> returns image reference sha256 <*>	66
389	CreateContainer within sandbox <*> for container &ContainerMetadata Name manager Attempt 0	22
390	CreateContainer within sandbox <*> for container &ContainerMetadata Name node-agent Attempt 0	44
391	CreateContainer within sandbox <*> for &ContainerMetadata Name node-agent Attempt 0 returns container id <*>	44
392	StartContainer for <*>	66
393	StartContainer for <*> returns successfully	66
394	CreateContainer within sandbox <*> for &ContainerMetadata Name manager Attempt 0 returns container id <*>	22
395	<*> <*> <*> Oct <*> <*> <*> <*> <*> systemd-networkd <*> <*> Gained IPv6LL	22
396	<*> <*> <*> tail can t open <*> No such file or directory	33
397	<*> <*> <*> tail no files	33
398	Failed to run command <*> print-logs.sh with output ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	33
399	<*> <*> <*> <*> <*> <*> <*> W | etcdserver read-only range request key <*> range_end <*> with result range_response_count 0 size <*> took too long <*> to execute	88
400	<*> <*> <*> <*> <*> <*> <*> W | etcdserver read-only range request key <*> range_end <*> with result range_response_count <*> size <*> took too long <*> to execute	11
401	<*> <*> <*> <*> <*> <*> <*> W | etcdserver request header <ID <*> username <*> auth_revision <*> > txn <compare <target MOD key <*> mod_revision <*> > success <request_put <key <*> value_size <*> >> failure <>> with result size <*> took too long <*> to execute	11
402	<*> <*> <*> and error exit status <*>	33
403	Failed to run command <*> print-logs.sh . exit status <*>	33
404	Execution failed. Failed to execute command <*> print-logs.sh . exit status <*>	33
405	exit status 1Failed to run ansible task	33
406	Failed to execute task exit status 1Failed to run cmd exec task <*>	33
407	Reconciled successfully	33
408	<*> <*> <*> <*> Created container node-agent	15
409	<*> <*> <*> <*> Started container node-agent	12
